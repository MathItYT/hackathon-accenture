Machine Translated by Google

Una descripción general completa de los modelos de lenguaje grandes
Humza Naveeda , Asad Ullah Khana,

, Shi Qiub,

, Saeed Anware, f, Muhammad Usmane, f, Naveed Akhtarg, i
Nick Barnesh , Ajmal Miani

, Muhammad Saqibc,d,

,

aUniversidad de Ingeniería y Tecnología (UET), Lahore, Pakistán bUniversidad China de
Hong Kong (CUHK), RAEHK, China cUniversidad de Tecnología de Sídney (UTS),
Sídney, Australia dOrganización de Investigación Científica e Industrial de la
Commonwealth (CSIRO), Sídney, Australia eKing Universidad Fahd de Petróleo y Minerales (KFUPM), Dhahran, Arabia
Saudita fSDAIA­KFUPM Centro Conjunto de Investigación para la Inteligencia Artificial (JRCAI), Dhahran,
Arabia Saudita gLa Universidad de Melbourne (UoM), Melbourne, Australia hUniversidad Nacional de Australia (ANU), Canberra ,
Australia iLa Universidad de Australia Occidental (UWA), Perth, Australia

Abstracto
Los modelos de lenguaje grande (LLM) han demostrado recientemente capacidades notables en tareas de procesamiento del lenguaje natural y
más. Este éxito de los LLM ha dado lugar a una gran afluencia de contribuciones de investigación en esta dirección. Estos trabajos abarcan diversos
temas, como innovaciones arquitectónicas, mejores estrategias de capacitación, mejoras en la duración del contexto, ajuste, LLM multimodales,
robótica, conjuntos de datos, evaluación comparativa, eficiencia y más. Con el rápido desarrollo de las técnicas y los avances constantes en la
investigación de LLM, se ha vuelto considerablemente difícil percibir el panorama más amplio de los avances en esta dirección. Teniendo en cuenta
la gran cantidad de literatura que emerge rápidamente sobre LLM, es imperativo que la comunidad de investigación pueda beneficiarse de una
descripción general concisa pero completa de los desarrollos recientes en este campo. Este artículo proporciona una descripción general de la
literatura existente sobre una amplia gama de conceptos relacionados con el LLM. Nuestra descripción general completa e independiente de los LLM
analiza conceptos básicos relevantes además de cubrir los temas avanzados en la frontera de la investigación en los LLM. Este artículo de revisión
2024
abril
de
9
[cs.CL]
arXiv:2307.06435v9

pretende no solo proporcionar una encuesta sistemática, sino también una referencia rápida e integral para que los investigadores y profesionales
obtengan conocimientos de extensos resúmenes informativos de los trabajos existentes para avanzar en la investigación del LLM.
Palabras clave:

Modelos de lenguajes grandes, LLM, chatGPT, LLM aumentados, LLM multimodales, formación LLM, evaluación comparativa de LLM

1. Introducción
El lenguaje juega un papel fundamental a la hora de facilitar la
comunicación y la autoexpresión de los humanos y su interacción con
las máquinas. La necesidad de modelos generalizados surge de la
creciente demanda de máquinas que manejen tareas lingüísticas
complejas, incluidas la traducción, el resumen, la recuperación de
información, las interacciones conversacionales, etc. Recientemente, se
han observado avances significativos en los modelos lingüísticos,
atribuidos principalmente a los transformadores. [1], mayores
capacidades computacionales y disponibilidad de datos de entrenamiento a gran escala.
Estos desarrollos han provocado una transformación revolucionaria al
permitir la creación de LLM que pueden aproximarse al desempeño a
nivel humano en diversas tareas [2, 3]. Grande

Contribución igual
Direcciones de correo electrónico: humza_naveed@yahoo.com (Humza Naveed),
aukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi Qiu),
muhammad.saqib@data61.csiro.au (Muhammad Saqib), saeed.anwar@kfupm.edu.sa
(Saeed Anwar), muhammad.usman@kfupm.edu.sa

Figura 1: La tendencia de los artículos publicados a lo largo de los años que contienen las palabras

(Muhammad Usman), naveed.akhtar1@unimelb.edu.au (Naveed Akhtar),

clave "Modelo de lenguaje grande", "Modelo de lenguaje grande + Ajuste fino" y "Modelo de lenguaje

nick.barnes @anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au (Ajmal

grande + Alineación".

Mian)

Preimpresión enviada a Elsevier

11 de abril de 2024

Machine Translated by Google

CodeGen (marzo)
GPT­NeoX­20B (abril)
UL2 (mayo)

TK­Instruct (mayo)
GLM (octubre)
mT0 (diciembre)

2019

OPT­IML
Galáctica (noviembre)

CPM­2 (junio)

2021

2020

2022

WebGPT (diciembre)

Xuan Yuan 2.0 (mayo)

Vicuña

MPT (junio)

Koala (mayo)

CódigoT5+

Asistente­LM

codificador estrella

Asistente­Codificador (junio)

Llama 2 (julio)

Cabra

Código Llama (agosto)

2023

2024
PanGu­Σ (marzo)

MT­NLG (ene)

Códice (julio)
GPT­3 (mayo)

LLaMA (febrero)

HuaTuo (abril)

OPTAR

PanGu­α (abril)
T0 (octubre)

mT5 (octubre)

T5 (octubre)

Alpaca (marzo)

ERNIE 3.0

Gorrión (septiembre)

Código Alfa (febrero)

Jurásico­1 (agosto)

FLAN­U­PaLM (octubre)

Chinchilla (Mar)

GPT­4

HyperCLOVA (septiembre)

ChatGPT (noviembre)

Bardo (octubre)

BloombergGPT

Palma (abril)

claudio

Yuan 1,0 (octubre)

AlexaTM (agosto)

PaLM2 (mayo)

Tuza (diciembre)

U­PALM (octubre)

Géminis (diciembre)

ERNIE 3.0 Titán

FLORACIÓN (noviembre)

GLAM
LaMDA

Figura 2: Visualización cronológica de los lanzamientos de LLM: las tarjetas azules representan modelos "preentrenados", mientras que las tarjetas naranjas corresponden a modelos "adaptados a las instrucciones". Modelos
en la mitad superior significan disponibilidad de código abierto, mientras que los de la mitad inferior son de código cerrado. El gráfico ilustra la tendencia creciente hacia modelos optimizados para la instrucción y modelos
de código abierto, destacando el panorama en evolución y las tendencias en la investigación del procesamiento del lenguaje natural.

Los modelos de lenguaje (LLM) han surgido como sistemas de inteligencia artificial

adoptado en diversos entornos, incluidos multimodales, robótica,

de vanguardia que pueden procesar y generar texto.

manipulación de herramientas, respuesta a preguntas, agentes autónomos, etc.

con comunicación coherente [4], y generalizar a múltiples

También se han sugerido varias mejoras en estas áreas.

tareas [5, 6].

ya sea mediante capacitación específica para la tarea [25, 26, 27, 28, 29, 30, 31] o

El progreso histórico en el procesamiento del lenguaje natural (PNL)

mejores indicaciones [32].

evolucionó del modelado estadístico al lenguaje neuronal y luego

Las habilidades del LLM para resolver diversas tareas a nivel humano.

desde modelos de lenguaje previamente entrenados (PLM) hasta LLM. Mientras

El rendimiento tiene el costo de un entrenamiento e inferencia lentos,

El modelado de lenguaje convencional (LM) entrena modelos de tareas específicas

amplios requisitos de hardware y mayores costos de funcionamiento.

en entornos supervisados, los PLM se entrenan en un entorno autosupervisado.

Esos requisitos han limitado su adopción y han abierto

establecer en un gran corpus de texto [7, 8, 9] con el objetivo de aprender

oportunidades para diseñar mejores arquitecturas [15, 33, 34, 35]

una representación genérica que se puede compartir entre varios PNL

y estrategias de formación [36, 37, 21, 38, 39, 40, 41]. El ajuste eficiente de

tareas. Después de realizar ajustes para las tareas posteriores, los PLM superan

parámetros [38, 41, 40], la poda [42, 43], la cuantificación [44, 45], la destilación

las ganancias de rendimiento del modelado de lenguaje tradicional (LM).

del conocimiento y la interpolación de la longitud del contexto [46, 47, 48, 49] , entre

Los PLM más grandes aportan más ganancias de rendimiento, lo que ha llevado

otros, son algunos de los métodos

a la transición de PLM a LLM mediante un aumento significativo de los parámetros

ampliamente estudiado para la utilización eficiente de LLM.

del modelo (de decenas a cientos de miles de millones) [10] y

Debido al éxito de los LLM en una amplia variedad de tareas, el

Conjunto de datos de entrenamiento (muchos GB y TB) [10, 11]. Siguiendo esto

La literatura de investigación ha experimentado recientemente una gran afluencia de

En desarrollo, se han propuesto numerosos LLM en la literatura [10, 11, 12, 6, 13,

Contribuciones relacionadas con LLM. Los investigadores han organizado el

14, 15]. Una tendencia creciente en el

Literatura de LLM en encuestas [50, 51, 52, 53] y temas específicos

número de LLM publicados y nombres de algunos LLM importantes

encuestas en [54, 55, 56, 57, 58]. En contraste con estas encuestas, nuestra

propuestos a lo largo de los años se muestran en la Fig. 1 y Fig. 2, respectivamente.

La contribución se centra en proporcionar una información completa pero concisa.

Los primeros trabajos en LLM, como T5 [10] y mT5 [11] emplearon el aprendizaje

los detalles arquitectónicos y de capacitación de los estudiantes previamente capacitados.

por transferencia hasta que GPT­3 [6] mostró que los LLM son

LLM y profundiza en los detalles de conceptos como ajuste fino, LLM multimodales,

Descripción general de la dirección general de la investigación LLM. Este artículo resume

Transferible desde cero a tareas posteriores sin necesidad de realizar ajustes.

LLM aumentados, conjuntos de datos, evaluación, aplicaciones, desafíos y otros

Los LLM responden con precisión a las consultas de tareas cuando se les solicita

para proporcionar una descripción general integral e independiente. Nuestras

descripciones de tareas y ejemplos. Sin embargo, los LLM previamente capacitados

contribuciones clave son

no sigue la intención del usuario y se desempeña peor en configuraciones de

resumido de la siguiente manera.

disparo cero que en configuraciones de pocos disparos. Ajustarlos con datos de
• Presentamos una encuesta sobre los avances en la investigación LLM.

instrucciones de tareas [16, 17, 18, 19] y alinearlos con las preferencias humanas

proporcionando una visión general concisa y completa de la dirección.

[20, 21] mejora la generalización a tareas invisibles, mejorando significativamente
el rendimiento de tiro cero y reduciendo los errores desalineados. comportamiento.

• Presentamos resúmenes extensos de modelos previamente entrenados que

Incluya detalles detallados de arquitectura y detalles de capacitación.

Además de una mejor generalización y adaptación del dominio,
Los LLM parecen tener habilidades emergentes, como razonamiento,

• Resumimos los principales hallazgos de las contribuciones populares.

planificación, toma de decisiones, aprendizaje en contexto, respuesta en

y proporcionar una discusión detallada sobre el diseño clave y

configuraciones de disparo cero, etc. Se sabe que estas habilidades las adquieren

aspectos de desarrollo de los LLM para ayudar a los profesionales a

debido a su escala gigantesca, incluso cuando los LLM previamente entrenados no

aprovechar eficazmente esta tecnología.

están entrenados específicamente para poseer estos atributos [22, 23, 24]. Tales

• En este artículo independiente, cubrimos una variedad de conceptos para

habilidades han llevado a los LLM a ser ampliamente

presentar la dirección general de los LLM integralmente.
2

Machine Translated by Google

Figura 3: Una descripción general más amplia de los LLM, dividiendo los LLM en siete ramas: 1. Capacitación previa 2. Ajuste fino 3. Eficiente 4. Inferencia 5. Evaluación 6. Aplicaciones
7. Desafíos

exhaustivamente, incluyendo antecedentes, capacitación previa, ajuste,

arquitecturas, canales de capacitación y estrategias, ajuste y

LLM multimodales, LLM aumentados, LLM impulsados

utilización en diferentes dominios. La sección 4 destaca la configuración y los parámetros

agentes, conjuntos de datos, evaluación, etc.

que juegan un papel crucial en el funcionamiento de estos modelos. Se presentan
resúmenes y discusiones.

Seguimos libremente la terminología existente para garantizar una perspectiva

en la sección 3.8. La formación y evaluación del LLM, los conjuntos de datos y

estandarizada de esta dirección de investigación. Por ejemplo, a continuación [50], nuestra

Los puntos de referencia se analizan en la sección 5, seguidos de los desafíos.

encuesta analiza los LLM previamente capacitados con 10 mil millones

y direcciones futuras, y conclusión en las secciones 7 y 8, respectivamente.

parámetros o más. Referimos a los lectores interesados en los más pequeños
modelos previamente entrenados para [51, 52, 53].

La organización de este trabajo es la siguiente. La sección 2 analiza
Los antecedentes de los LLM. La sección 3 se centra en la descripción general de los LLM,
3

Machine Translated by Google

2.4. Funciones de activación

2. Antecedentes

Las funciones de activación desempeñan un papel crucial en la capacidad

Proporcionamos los antecedentes relevantes para comprender los fundamentos
relacionados con los LLM en esta sección. discutimos brevemente

de ajuste de curvas de las redes neuronales [69]. Discutimos la activación.

componentes necesarios en LLM y recomendar a los lectores interesados

funciones utilizadas en LLM en esta sección.

en detalle a las obras originales.

ReLU [70]: La unidad lineal rectificada (ReLU) se define como:

2.1. Tokenización

ReLU(x) = máx(0, x)

(1)

La tokenización [59] es un paso de preprocesamiento esencial en
GeLU [71]: La unidad lineal de error gaussiano (GeLU) es la

Formación LLM que analiza el texto en unidades que no se descomponen.
llamados fichas. Los tokens pueden ser caracteres, subpalabras [60], símbolos

combinación de ReLU, abandono [72] y abandono de zona [73].

[61] o palabras, según el proceso de tokenización.

Variantes de GLU [74]: La unidad lineal cerrada [75] es una

Algunos de los esquemas de tokenización comúnmente utilizados en LLM

capa de red que es un producto por elementos (

incluya pieza de palabra [62], codificación de par de bytes (BPE) [61] y un­igramLM

transformación y una proyección lineal transformada sigmoidea (σ)

) de una línea

[60]. Se anima a los lectores a consultar [63] para obtener más información.

de la entrada dada como:

encuesta detallada.
GLU(x, W, V, b, c) = (xW + b)

σ(xV + c),

(2)

2.2. Posiciones de codificación
El transformador procesa secuencias de entrada en paralelo y

donde X es la entrada de la capa y se aprenden l, W, b, V y c

independientemente unos de otros. Además, el módulo de atención del

parámetros. Otras variantes de GLU [74] utilizadas en LLM son:

transformador no captura información de posición.
Como resultado, se introdujeron codificaciones posicionales en el transformador
[64], donde se agrega un vector de incrustación posicional a
la incrustación del token. Las variantes de incrustación posicional incluyen

ReGLU(x, W, V, b, c) = máx(0, xW + b)

,

GEGLU(x, W, V, b, c) = GELU(xW + b)

(xV + c),

S wiGLU(x, W, V, b, c, β) = S deseoβ(xW + b)

(xV + c).

Codificaciones posicionales absolutas, relativas o aprendidas. Dentro de la
codificación relativa, Alibi y RoPE son dos posicionales ampliamente utilizados.

2.5. Normalización de capas

incrustaciones en LLM.
Coartada [65]: Resta un sesgo escalar de la puntuación de atención

La normalización de capas conduce a una convergencia más rápida y es un

que aumenta con la distancia entre las posiciones simbólicas. Este

componente integrado de los transformadores [64]. Además de Layer­Norm [76] y

favorece el uso de tokens recientes para llamar la atención.

RMSNorm [77], los LLM utilizan la normalización previa a la capa [78], aplicándola

RoPE [66]: rota las representaciones de consultas y claves en un ángulo

antes de la atención de múltiples cabezales (MHA).

proporcional a la posición absoluta del token en la entrada

Se ha demostrado que la prenorma proporciona estabilidad de entrenamiento en

secuencia, lo que resulta en un esquema de codificación posicional relativo

LLM. Otra variante de normalización, DeepNorm [79] soluciona el problema con

que decae con la distancia entre las fichas.

gradientes más grandes en prenormalidad.

2.3. Atención en LLM
2.6. Formación distribuida de LLM

Atención asigna pesos a los tokens de entrada según su importancia, de modo

Esta sección describe enfoques de capacitación distribuidos de LLM.

que el modelo dé más énfasis a los tokens relevantes.
La atención en transformadores [64] calcula la consulta, la clave y el valor.

brevemente. Más detalles están disponibles en [13, 37, 80, 81].

asignaciones para secuencias de entrada, donde la puntuación de atención es

Paralelismo de datos: el paralelismo de datos replica el modelo en

obtenido multiplicando la consulta y la clave, y luego utilizado para

múltiples dispositivos donde los datos de un lote se dividen entre

valores de peso. Discutimos diferentes estrategias de atención utilizadas en

dispositivos. Al final de cada iteración de entrenamiento, los pesos se
sincronizan en todos los dispositivos.

LLM a continuación.

Autoatención [64]: Calcula la atención mediante consultas, claves,

Paralelismo tensorial: el paralelismo tensorial fragmenta un cálculo tensorial entre

y valores del mismo bloque (codificador o decodificador).

dispositivos. También se le conoce como paralelismo horizontal.

Atención cruzada: se utiliza en arquitecturas de codificador­decodificador,

o paralelismo del modelo intracapa.

donde las salidas del codificador son las consultas y los pares clave­valor

Paralelismo de canalización: capas de modelo de fragmentos de paralelismo de canalización

provienen del decodificador.

a través de diferentes dispositivos. Esto también se conoce como paralelismo
vertical.

Atención escasa [67]: la autoatención tiene O (n

2

) tiempo complejo­

Paralelismo de modelos: una combinación de paralelismo de tensor y canalización

idad que se vuelve inviable para secuencias grandes. acelerar
el cálculo, la escasa atención [67] calcula iterativamente

se conoce como paralelismo de modelos.

Atención en ventanas correderas para ganar velocidad.

Paralelismo 3D: una combinación de paralelismo de datos, tensor y modelo se

Atención Flash [68]: El acceso a la memoria es el principal cuello de botella

conoce como paralelismo 3D.

en el cálculo de la atención utilizando GPU. Para acelerar, parpadea

Paralelismo del optimizador: el paralelismo del optimizador también conocido como

La atención emplea mosaicos de entrada para minimizar las lecturas de memoria.

El optimizador de redundancia cero [37] implementa el estado del optimizador.

y escribe entre la memoria de alto ancho de banda (HBM) de la GPU

partición, partición de gradiente y partición de parámetros

y la SRAM en chip.

entre dispositivos para reducir el consumo de memoria manteniendo
los costes de comunicación lo más bajos posible.
4

Machine Translated by Google

2.7. Bibliotecas
Algunas bibliotecas comúnmente utilizadas para la capacitación de LLM son:
Transformers [82]: la biblioteca proporciona acceso a varios modelos de
transformadores previamente entrenados con API para entrenar, ajustar, inferir y
desarrollar modelos personalizados.
DeepSpeed [36]: una biblioteca para entrenamiento distribuido escalable e inferencia
de modelos de aprendizaje profundo.
Megatron­LM [80]: proporciona técnicas optimizadas para GPU para la formación a
Figura 4: Un ejemplo de patrones de atención en modelos de lenguaje, imagen tomada de
[93].

gran escala de LLM.

JAX [83]: una biblioteca de Python para computación numérica de alto
rendimiento y aprendizaje automático escalable. Puede diferenciar funciones
nativas de Python y NumPy y ejecutarlas en GPU.
Colossal­AI [84]: una colección de componentes para escribir modelos distribuidos
de aprendizaje profundo.
BMTrain [81]: una biblioteca para escribir código de formación de LLM independiente
y eficiente.
FastMoE [85]: proporciona API para crear un modelo de combinación de expertos

Figura 5: Un ejemplo de objetivos de entrenamiento del modelo de lenguaje, imagen de [93].

(MoE) en PyTorch.
MindSpore [86]: un marco de inferencia y capacitación en aprendizaje profundo
extensible a la computación móvil, perimetral y en la nube.

la atención y conexión de bloques transformadores. En la Figura 4 se muestra una

PyTorch [87]: un marco desarrollado por el laboratorio de investigación de

ilustración de los patrones de atención de estas arquitecturas .

inteligencia artificial de Facebook (FAIR) para construir modelos de aprendizaje
profundo. Las características principales de PyTorch incluyen un gráfico de cálculo

Codificador Decodificador: esta arquitectura procesa entradas a través del codificador

dinámico y un estilo de codificación pitónica.

y pasa la representación intermedia al decodificador para generar la salida. Aquí, el

Tensorflow [88]: un marco de aprendizaje profundo escrito por Google. Las

codificador ve la secuencia completa utilizando la atención propia, mientras que el

características clave de TensorFlow son la computación basada en gráficos, la

decodificador procesa la secuencia una tras otra implementando la atención cruzada.

ejecución entusiasta, la escalabilidad, etc.
MXNet [89]: Apache MXNet es un marco de aprendizaje profundo con soporte para
escribir programas en múltiples lenguajes, incluidos Python, C++, Scala, R, etc.

Decodificador causal: un tipo de arquitectura que no tiene un codificador y procesa

También proporciona soporte para gráficos de cálculo dinámicos y estáticos.

y genera resultados utilizando un decodificador, donde el token predicho depende
solo de los pasos de tiempo anteriores.

2.8. Preprocesamiento de datos
Decodificador de prefijo: también conocido como decodificador no causal, donde el

Esta sección resume brevemente la tecnología de preprocesamiento de datos.

cálculo de la atención no depende estrictamente de la información pasada y la

Técnicas utilizadas en la formación de LLM.

atención es bidireccional. En la Figura 4 se muestra un ejemplo de una máscara de

Filtrado de calidad: para obtener mejores resultados, la calidad de los datos de

atención no causal .

entrenamiento es esencial. Algunos enfoques para filtrar datos son: 1) basado en

Mezcla de expertos: es una variante de la arquitectura transformadora con expertos

clasificadores y 2) basado en heurísticas. Los enfoques basados en clasificadores

independientes paralelos y un enrutador para enrutar tokens a los expertos. Estos

entrenan a un clasificador con datos de alta calidad y predicen la calidad del texto

expertos son capas de retroalimentación después del bloque de atención [90]. La

para el filtrado, mientras que los basados en heurística emplean algunas reglas

mezcla de expertos (MoE) es una arquitectura dispersa eficiente que ofrece un

para el filtrado como lenguaje, métricas, estadísticas y palabras clave.

rendimiento comparable a los modelos densos y permite aumentar el tamaño del

Deduplicación de datos: los datos duplicados pueden afectar el rendimiento del

modelo sin aumentar el costo computacional activando solo unos pocos expertos

modelo y aumentar la memorización de datos; por lo tanto, para capacitar a los

a la vez [91, 92].

LLM, la deduplicación de datos es uno de los pasos de preprocesamiento.
Esto se puede realizar en múltiples niveles, como oraciones, documentos y
conjuntos de datos.

2.10. Objetivos previos al entrenamiento

Reducción de la privacidad: la mayoría de los datos de capacitación para LLM se

Esta sección describe los objetivos previos a la capacitación de LLM. Para

recopilan a través de fuentes web. Estos datos contienen información privada; por
lo tanto, muchos LLM emplean métodos basados en heurísticas para filtrar

obtener más detalles, consulte el artículo [93].

información como nombres, direcciones y números de teléfono para evitar aprender

Modelado de lenguaje completo: un objetivo de modelado de lenguaje autorregresivo

información personal.

en el que se le pide al modelo que prediga tokens futuros dados los tokens
anteriores; se muestra un ejemplo en la Figura 5.
Modelado de lenguaje de prefijos: un objetivo de entrenamiento no causal, donde

2.9. Arquitecturas

se elige un prefijo al azar y solo se utilizan los tokens de destino restantes para

Aquí discutimos las variantes de las arquitecturas de transformadores.

calcular la pérdida. Un ejemplo se muestra en la Figura 5.

utilizado en LLM. La diferencia surge debido a la aplicación de
5

Machine Translated by Google

Figura 6: Un diagrama de flujo básico que representa varias etapas de los LLM, desde la capacitación previa hasta la indicación/utilización. Es posible incitar a los LLM a generar respuestas en
diferentes etapas de capacitación, como la capacitación previa, el ajuste de instrucciones o el ajuste de alineación. "RL" significa aprendizaje por refuerzo, "RM" representa modelado de
recompensa y "RLHF" representa aprendizaje por refuerzo con retroalimentación humana.

2.12. Etapas de adaptación de los LLM

Modelado de lenguaje enmascarado: en este objetivo de capacitación, los
tokens o tramos (una secuencia de tokens) se enmascaran aleatoriamente y se

Esta sección analiza los fundamentos de las etapas de adaptación de los

le pide al modelo que prediga los tokens enmascarados dado el contexto pasado

LLM, desde la capacitación previa hasta el ajuste para las tareas posteriores y

y futuro. Un ejemplo se muestra en la Figura 5.

su utilización. En la Figura 6 se muestra un ejemplo de diferentes etapas de

Modelado de lenguaje unificado: El modelado de lenguaje unificado [94] es una

capacitación e inferencia en LLM. En este artículo, nos referimos al ajuste de

combinación de objetivos de entrenamiento de lenguaje causales, no causales

alineación como alineación con las preferencias humanas, mientras que

y enmascarados. Aquí, en el modelado del lenguaje enmascarado, la atención

ocasionalmente la literatura usa el término alineación para diferentes

no es bidireccional sino unidireccional, y atiende al contexto de izquierda a

propósitos.

derecha o de derecha a izquierda.

2.12.1. Pre­entrenamiento
En la primera etapa, el modelo se entrena de forma autosupervisada en un
corpus grande para predecir los siguientes tokens dada la entrada. Las

2.11. Leyes de escala de LLM

opciones de diseño de los LLM varían desde arquitecturas de codificador­
decodificador hasta arquitecturas de solo decodificador con diferentes bloques

Las leyes de escala estudian la combinación óptima de parámetros del

de construcción y funciones de pérdida en las secciones 2.5, 2.4, 2.10.

modelo, tamaño del conjunto de datos y recursos computacionales que predicen
la mejora en el rendimiento del modelo. Se ha demostrado que la pérdida

2.12.2. Sintonia FINA

aumenta de acuerdo con la ley de potencias con el tamaño del modelo, el

Existen diferentes estilos para perfeccionar un LLM. Esta sección

tamaño del conjunto de datos y los recursos informáticos [95]. Este estudio
sugiere que los modelos más grandes son más importantes que los big data

analiza brevemente los enfoques de ajuste.

para lograr un mejor rendimiento. Otra variante de la ley de escalamiento [96]

Transferir aprendizaje: los LLM previamente capacitados se desempeñan bien

en diversas
tareas [6, 15]. Sin embargo, para mejorar el rendimiento de
sugiere que el tamaño del modelo y el número de tokens de entrenamiento deben escalarse
por igual.
6

Machine Translated by Google

una tarea posterior, los modelos previamente entrenados se ajustan con el

Considerando que para mejorar aún más los LLM en tareas de razonamiento, muchos

datos específicos de la tarea [10, 11], conocidos como aprendizaje por transferencia.

Los métodos [16, 97] los entrenan en conjuntos de datos de razonamiento. discutimos

Ajuste de instrucciones: para permitir que un modelo responda al usuario

A continuación se presentan varias técnicas de estímulo para el razonamiento.

consultas de manera efectiva, el modelo previamente entrenado se ajusta con datos

Cadena de pensamiento (CoT): un caso especial de indicaciones donde

formateados en instrucciones, es decir, instrucciones y una entrada­salida.

Las demostraciones contienen información de razonamiento agregada con

par. Las instrucciones generalmente comprenden datos de tareas múltiples en formato simple.

entradas y salidas para que el modelo genere resultados con

lenguaje natural, guiando al modelo a responder de acuerdo con el

razonamiento paso a paso. Más detalles sobre las indicaciones de CoT están

mensaje y la entrada. Este tipo de ajuste mejora la generalización inmediata y el

disponibles en [55, 103, 101].

rendimiento de las tareas posteriores. Detalles

Autoconsistencia: mejora el rendimiento de CoT al generar múltiples respuestas y

sobre el formato de datos de instrucciones y sus diversos estilos están disponibles en

seleccionar la respuesta más frecuente [104].

[16, 50, 97].
Ajuste de alineación: los LLM son propensos a generar información falsa, sesgada y

Árbol del pensamiento (ToT): explora múltiples caminos de razonamiento

y texto dañino. Para que sean útiles, honestos e inofensivos,

con posibilidades de mirar hacia adelante y hacia atrás para resolver problemas [105].

Los modelos se alinean utilizando la retroalimentación humana. La alineación implica
pedir a los LLM que generen respuestas inesperadas y luego actualizar sus parámetros

Instrucciones de un solo turno: en esta configuración de indicaciones, los LLM son

para evitar tales respuestas [20, 21, 98].

consultado una sola vez con toda la información relevante en el

Garantiza que los LLM funcionen de acuerdo con las intenciones humanas y

inmediato. Los LLM generan respuestas al comprender el contexto, ya sea en un

valores. Un modelo se define como un modelo "alineado" si el

entorno de pocas posibilidades o de cero posibilidades.

El modelo cumple tres criterios: útil, honesto e inofensivo.

Instrucciones de varios turnos: resolver una tarea compleja requiere múltiples

“HHHH” [99].

interacciones con los LLM, donde la retroalimentación y las respuestas

Los investigadores emplean el aprendizaje reforzado con retroalimentación humana

de las otras herramientas se proporcionan como entrada al LLM para el próximo

(RLHF) [100] para alinear el modelo. En RLHF, una puesta a punto

rondas. Este estilo de usar LLM en el bucle es común en

El modelo en demostraciones se entrena aún más con modelos de

agentes autónomos.

recompensa (RM) y aprendizaje por refuerzo (RL), como se muestra en la Figura 6.
A continuación analizamos brevemente los oleoductos RM y RL en RLHF.

3. Modelos de lenguaje grandes

Modelado de recompensas: entrena un modelo para clasificar las respuestas generadas
según las preferencias humanas utilizando un objetivo de clasificación. Para entrenar

Esta sección revisa los LLM y describe brevemente sus arquitecturas, objetivos de

al clasificador, los humanos anotan los LLM generados.

capacitación, procesos, conjuntos de datos y ajustes.

respuestas basadas en los criterios HHH.

detalles.

Aprendizaje por refuerzo: en combinación con el modelo de recompensa
se utiliza para la alineación en la siguiente etapa. Los previamente capacitados

3.1. LLM previamente capacitados

El modelo de recompensa clasifica las respuestas generadas por LLM en preferidas.
Aquí, proporcionamos resúmenes de varios LLM previamente capacitados y

versus no preferido, que se utiliza para alinear el modelo con la optimización de

conocidos con descubrimientos importantes, que cambian el rumbo.

políticas próxima (PPO). Este proceso se repite iterativamente.

de investigación y desarrollo en PNL. Estos LLM han mejorado considerablemente el

hasta la convergencia.

rendimiento en los dominios NLU y NLG.
y están ampliamente optimizados para tareas posteriores. Además, nosotros

2.12.3. Instrucción/Utilización

también identificar hallazgos y conocimientos clave de LLM previamente capacitados en

La solicitud es un método para consultar LLM capacitados para generar

Tabla 1 y 2 que mejoran su desempeño.

respuestas, como se ilustra en la Figura 6. Los LLM se pueden solicitar en
varias configuraciones de avisos, donde se pueden adaptar a las instrucciones sin

3.1.1. Propósito general

realizar ajustes finos y en otros casos con ajustes finos en

T5 [10]: En la Figura 7 se muestra un modelo codificador­decodificador que emplea

datos que contienen diferentes estilos de mensajes [16, 101, 102]. un buen
La guía sobre ingeniería rápida está disponible en [32]. A continuación, nosotros

un entrenamiento de texto a texto unificado para todos los problemas de PNL. T5

Analizaremos varias configuraciones de avisos ampliamente utilizadas.

coloca la normalización de capa fuera de la ruta residual en un modelo de transformador

Indicaciones de tiro cero: los LLM son estudiantes de tiro cero y son capaces de

convencional [64]. Utiliza el modelado de lenguaje enmascarado como objetivo previo

responder consultas nunca antes vistas. Este estilo de

al entrenamiento donde los intervalos (tokens consecutivos) se reemplazan con una

Las indicaciones requieren que los LLM respondan las preguntas de los usuarios sin

sola máscara en lugar de máscaras separadas.

ver ningún ejemplo en la solicitud.

para cada ficha. Este tipo de enmascaramiento acelera el entrenamiento ya que

Aprendizaje en contexto: también conocido como aprendizaje en pocas oportunidades, aquí,

produce secuencias más cortas. Después del entrenamiento previo, el modelo es

Se muestran varios pares de demostración de entrada­salida al

ajustado utilizando capas de adaptador [106] para tareas posteriores.
GPT­3 [6]: La arquitectura GPT­3 es la misma que la GPT­2 [5] pero con una atención

modelo para generar la respuesta deseada. Este estilo de adaptación
También se llama aprendizaje de pocas oportunidades. Una discusión sobre el formato

densa y escasa en las capas del transformador.

de plantillas de aprendizaje en contexto (ICL) está disponible en [54, 50, 18, 16].

similar al Sparse Transformer [67]. Muestra que los modelos grandes pueden entrenar

Razonamiento en LLM: los LLM son razonadores de tiro cero y pueden

en lotes más grandes con una tasa de aprendizaje más baja para

Ser provocado para generar respuestas a problemas lógicos, tareas.

decide el tamaño del lote durante el entrenamiento, GPT­3 usa el gradiente

planificación, pensamiento crítico, etc. con razonamiento. generando

escala de ruido como en [107]. En general, GPT­3 aumenta los parámetros del modelo

razones sólo es posible mediante el uso de diferentes estilos de indicaciones,

a 175B, lo que demuestra que el rendimiento de lenguajes grandes
7

Machine Translated by Google

ajuste fino completo y ajuste fino rápido como en [40] donde solo
Los parámetros relacionados con los mensajes se actualizan insertando mensajes en

varias posiciones, delante, en medio y detrás. CPM­2 también propone INFMOE,
un marco de trabajo eficiente en memoria con una estrategia para descargar
dinámicamente parámetros a la CPU para realizar inferencias.
a una escala de 100B. Superpone el movimiento de datos con el cálculo de
inferencia para reducir el tiempo de inferencia.
ERNIE 3.0 [110]: ERNIE 3.0 se inspira en el aprendizaje multitarea para
construir una arquitectura modular utilizando Transformer­XL [111] como

Figura 7: Ejemplo de entrenamiento de texto a texto unificado, imagen fuente de [10].

columna vertebral. El módulo de representación universal es compartido por
todas las tareas, que sirven como bloque básico.
para módulos de representación de tareas específicas, todos ellos capacitados
conjuntamente para la comprensión del lenguaje natural, lenguaje natural
generación y extracción de conocimiento. Este LLM se centra principalmente
en el idioma chino. Afirma entrenar en el
corpus de textos chinos más grandes para la formación LLM, y logró
Lo último en 54 tareas de PNL china.
Jurassic­1 [112]: Un par de modelos de lenguaje autorregresivos, incluido un
modelo J1­Large de 7B parámetros y un modelo J1­Jumbo de 178B parámetros.
El vocabulario de entrenamiento de
Jurassic­1 comprende fragmentos de palabras, palabras completas y
expresiones de varias palabras sin límites de palabras, cuando sea posible
las instancias sin vocabulario se interpretan como bytes Unicode.
En comparación con sus homólogos GPT­3, los modelos Jurassic­1
Figura 8: La imagen es el artículo de [108], que muestra un ejemplo de PanGu­α
arquitectura.

aplicar una arquitectura de autoatención más equilibrada de profundidad a
ancho [113] y un tokenizador mejorado para una predicción más rápida
basado en recursos más amplios, logrando un desempeño comparable en
tareas de aprendizaje de tiro cero y un desempeño superior en

Los modelos mejoran con la escala y son competitivos con los modelos
ajustados.

Tareas de aprendizaje de pocas oportunidades dada la capacidad de alimentar más ejemplos.

como un aviso.

mT5 [11]: Un modelo T5 multilingüe [10] entrenado en el mC4

HyperCLOVA [114]: un modelo de idioma coreano con GPT­3

Conjunto de datos con 101 idiomas. El conjunto de datos se extrae del

arquitectura.

raspado de rastreo común público. El modelo utiliza un vocabulario mayor de

Yuan 1.0 [115]: Entrenado en un corpus chino con 5 TB de

250.000 para cubrir varios idiomas. para evitar
sobreajustado o insuficiente para un idioma, mT5 emplea un sistema de datos

Texto de alta calidad recopilado de Internet. Un dato masivo

procedimiento de muestreo para seleccionar muestras de todos los idiomas. El

El sistema de filtrado (MDFS) integrado en Spark está desarrollado para
procesar los datos sin procesar mediante técnicas de filtrado grueso y fino. A

El artículo sugiere utilizar una pequeña cantidad de conjuntos de datos previos al entrenamiento,

incluidos todos los idiomas al realizar ajustes para una tarea utilizando datos

acelerar el entrenamiento de Yuan 1.0 para ahorrar gastos de energía y

del idioma inglés. Esto permite que el modelo genere correctamente

Emisiones de carbono, diversos factores que mejoran el rendimiento.
de la capacitación distribuida se incorporan en la arquitectura y la capacitación:

salidas no inglesas.

al igual que aumentar el tamaño del estado oculto, mejora la canalización y

PanGu­α [108]: Un modelo autorregresivo que tiene una consulta
capa al final de las capas de transformador estándar, se muestra el ejemplo

Rendimiento del paralelismo del tensor, los microlotes más grandes mejoran

en la Figura 8, para predecir el siguiente token. Su estructura es similar a

rendimiento de paralelismo de canalización y mayor tamaño de lote global

la capa transformadora pero con una incrustación adicional para la

mejorar el rendimiento del paralelismo de datos. En la práctica, el Yuan 1.0

siguiente posición en el mecanismo de atención, dada en la ecuación. 3.

El modelo funciona bien en clasificación de texto, esquema de Winograd,
inferencia del lenguaje natural y tareas de comprensión lectora.

a = pnW

q hWk h

THT

l

Gopher [116]: La familia de modelos Gopher abarca desde

(3)

44M a 280B parámetros de tamaño para estudiar el efecto de escala
sobre el desempeño del LLM. El modelo 280B supera al GPT­3 [6],

CPM­2 [12]: Modelos de lenguaje preentrenados rentables
(CPM­2) pre­entrena bilingüe (inglés y chino) 11B y

Jurrasic­1 [112], MT­NLG [117] y otros en el 81% de los

Modelos de mezcla de expertos (MoE) 198B en el conjunto de datos WuDaoCor­

tareas evaluadas.

pus [109] . El proceso de tokenización elimina “_” blanco

ERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan amplía ERNIE 3.0

fichas de espacio en el tokenizador de oraciones. Los modelos son

entrenando un modelo más grande con 26 veces el número de parámetros

entrenado con herencia de conocimiento, comenzando solo con el idioma chino

de este último. Este modelo más grande superó a otros modelos de última

en la primera etapa y luego agregando inglés y

generación en 68 tareas de PNL. Los LLM producen texto con errores

Datos chinos. Este modelo entrenado se duplica varias veces.

hechos. Para tener control del texto generado con coherencia fáctica, ERNIE

para inicializar el modelo 198B MoE. Además, para utilizar el modelo

3.0 Titan añade otra tarea, Credible.

Para tareas posteriores, CPM­2 experimentó con ambos

y Controllable Generations, hasta su configuración de aprendizaje multitarea.
8

Machine Translated by Google

Introduce pérdidas adicionales de modelado de lenguaje controlable y adversarial
autosupervisadas en el paso previo al entrenamiento, que
permite a ERNIE 3.0 Titan superar a otros LLM en su forma manual
Evaluaciones de conjuntos de tareas de control de calidad factual seleccionadas.

GPT­NeoX­20B [118]: Un modelo autorregresivo que en gran medida
sigue GPT­3 con algunas desviaciones en el diseño de la arquitectura,
entrenado en el conjunto de datos de Pile sin ninguna deduplicación de datos. GPT­
NeoX tiene capas de atención paralela y retroalimentación en un bloque transformador,
como se muestra en la ecuación. 4, eso aumenta el rendimiento en un 15%.
Utiliza incrustación posicional rotativa [66], aplicándola solo a
25% de la dimensión del vector de incrustación como en [119]. Esto reduce

Figura 9: El ejemplo de arquitectura BLOOM procedente de [13].

el cálculo sin degradación del rendimiento. En contraposición
a GPT­3, que utiliza capas densas y escasas, GPT­NeoX­20B
utiliza sólo capas densas. El ajuste de hiperparámetros a esta escala.

relación que el tamaño del modelo debe duplicarse por cada duplicación de tokens de

es difícil; por lo tanto, el modelo elige hiperparámetros de

entrenamiento. Más de 400 modelos de lenguaje que van

el método [6] e interpola valores entre 13B y 175B

de 70 millones a más de 16 mil millones de parámetros en 5 a 500 mil millones de

Modelos para el modelo 20B. La formación modelo se distribuye.

tokens están entrenados para obtener estimaciones para un cálculo óptimo

entre GPU que utilizan paralelismo tensorial y de canalización.
x + Atención(LN1(x)) + FF(LN2(x))

formación con un presupuesto determinado. Los autores entrenan un modelo 70B.
con el mismo presupuesto de cómputo que Gopher (280B) pero con 4

(4)

veces más datos. Supera a Gopher [116], GPT­3 [6] y
otros en diversas tareas posteriores, después de realizar ajustes.

OPT [14]: Es un clon de GPT­3, desarrollado para código abierto.

AlexaTM [122]: Un modelo codificador­decodificador, donde el codificador

un modelo que replica el rendimiento del GPT­3. Formación de OPT
emplea escalado de pérdida dinámica [120] y se reinicia desde un punto anterior

Los pesos y las incorporaciones de decodificadores se inicializan con un codificador

punto de control con una tasa de aprendizaje más baja siempre que la pérdida divergencia

previamente entrenado para acelerar el entrenamiento. El codificador se queda congelado.

se observa. En general, el rendimiento de los modelos OPT­175B es

para los 100.000 pasos iniciales y luego se descongela de un extremo a otro

comparable al modelo GPT3­175B.

capacitación. El modelo está entrenado en una combinación de eliminación de ruido.
y objetivos de modelado de lenguaje causal (CLM), concatenando un token [CLM] al

BLOOM [13]: Un modelo decodificador causal entrenado en ROOTS

principio para el cambio de modo. Durante el entrenamiento, la tarea CLM se aplica

corpus para abrir un LLM de código abierto. La arquitectura de BLOOM es

durante el 20% del tiempo, lo que

como se muestra en la Figura 9, con diferencias como la incrustación posicional de

mejora el rendimiento del aprendizaje en contexto.

ALiBi, una capa de normalización adicional después de la incrustación

PaLM [15]: Un decodificador causal con atención paralela y

capa como lo sugiere la biblioteca bitsandbytes1. Estos cambios

capas de avance similares a la ecuación. 4, acelerar el entrenamiento mediante

estabilizar el entrenamiento con un rendimiento posterior mejorado.
GLaM [91]: El modelo de lenguaje generalista (GLaM) representa un

un factor de 15. Los cambios adicionales al modelo de transformador convencional

familia de modelos de lenguaje que utilizan una estructura de mezcla de expertos

incluyen la activación de SwiGLU, incrustaciones de RoPE,

(MoE) de solo decodificador escasamente activada [121, 90]. para ganar

Atención de consultas múltiples que ahorra costos de cálculo durante la decodificación

más capacidad de modelo y al mismo tiempo reducir el cálculo, los expertos

e incrustaciones de entrada y salida compartidas. Durante el entrenamiento, pérdida.

están escasamente activados donde solo se utilizan los dos mejores expertos

Se observaron picos y, para solucionarlos, se reinició el entrenamiento del modelo.

para procesar cada token de entrada. El modelo GLaM más grande, GLaM

desde un punto de control anterior de 100 pasos omitiendo entre 200 y 500 lotes

(64B/64E), es aproximadamente 7 veces más grande que GPT­3 [6], mientras que solo una parte de

alrededor de la púa. Además, se descubrió que el modelo memoriza alrededor del

los parámetros se activan por token de entrada. El GLaM más grande

2,4% de los datos de entrenamiento en la escala del modelo 540B.

mientras que este número era menor para los modelos más pequeños.

(64B/64E) logra mejores resultados generales en comparación

PaLM­2 [123]: una variante multilingüe más pequeña de PaLM,

a GPT­3 mientras consume solo un tercio del entrenamiento de GPT­3

entrenado para iteraciones más grandes en un conjunto de datos de mejor calidad.

energía.

PaLM­2 muestra mejoras significativas sobre PaLM, al tiempo que reduce

MT­NLG [117]: un decodificador causal 530B basado en la arquitectura GPT­2 que
tiene aproximadamente 3 × parámetros del modelo GPT­3.

Costos de entrenamiento e inferencia debido a su menor tamaño. para disminuir

MT­NLG está capacitado con datos filtrados de alta calidad recopilados de

toxicidad y memorización, agrega fichas especiales con un

varios conjuntos de datos públicos y combina varios tipos de conjuntos de datos en un

fracción de datos previos al entrenamiento, lo que muestra una reducción en la

lote único, que supera a GPT­3 en varias evaluaciones.

generación de respuestas dañinas.
U­PaLM [124]: este método entrena a PaLM para un cálculo adicional del 0,1 % con

Chinchilla [96]: un decodificador causal entrenado en el mismo conjunto de datos
como el Gopher [116] pero con un muestreo de datos un poco diferente

el objetivo UL2 (también denominado UL2Restore) [125], utilizando el mismo conjunto

distribución (muestreada de MassiveText). La arquitectura del modelo es similar a la

de datos supera a la línea de base.

utilizada para Gopher, con la excepción de

significativamente en diversas tareas de PNL, incluidas las de cero disparos, de pocos
disparos, razonamiento de sentido común, CoT, etc. Capacitación con UL2R

Optimizador AdamW en lugar de Adam. Chinchilla identifica la

Implica convertir un decodificador PaLM causal en un decodificador PaLM no causal y
emplear un 50 % de eliminación de ruido secuencial, un 25 %

1https://github.com/TimDettmers/bitsandbytes

funciones de eliminación de ruido regular y 25% de pérdida extrema de eliminación de ruido.

9

Machine Translated by Google

Codex [131]: este LLM está capacitado en un subconjunto de Python público

UL2 [125]: Una arquitectura codificador­decodificador entrenada utilizando un
Objetivo de mezcla de eliminadores de ruido (MoD). Los eliminadores de ruido incluyen 1)

Repositorios de Github para generar código a partir de cadenas de documentos. La

R­Denoiser: un enmascaramiento de intervalo regular, 2) S­Denoiser: que corrompe

programación de computadoras es un proceso iterativo donde los programas

tokens consecutivos de una secuencia grande y 3) X­Denoiser:

a menudo se depuran y actualizan antes de cumplir con los requisitos. De manera

lo que corrompe una gran cantidad de tokens al azar. Durante el entrenamiento previo,

similar, Codex genera 100 versiones de un

UL2 incluye un token de eliminación de ruido de R, S, X para representar una

programa mediante muestreo repetitivo para una descripción dada, que

configuración de eliminación de ruido. Ayuda a mejorar el rendimiento de ajuste para las

produce una solución funcional para el 77,5% de los problemas que pasan

tareas posteriores que vinculan la tarea a uno de los modos de entrenamiento anteriores.

pruebas unitarias. Su potente versión impulsa Github Copilot2 .

Este estilo de entrenamiento del Ministerio de Defensa supera

AlphaCode [132]: un conjunto de grandes modelos de lenguaje, que van

el modelo T5 en muchos puntos de referencia.

desde parámetros 300M a 41B, diseñados para nivel de competencia
Tareas de generación de código. Utiliza la atención multiconsulta [133] para

GLM­130B [33]: GLM­130B es un modelo bilingüe (inglés y chino) entrenado utilizando
una máscara autorregresiva que rellena un objetivo de preentrenamiento similar al GLM

reducir los costos de memoria y caché. Dado que los problemas de programación

[126]. Este estilo de entrenamiento

competitivos requieren en gran medida un razonamiento profundo y una comprensión de

hace que el modelo sea bidireccional en comparación con GPT­3, que es

algoritmos complejos del lenguaje natural, los modelos Alpha­Code están previamente

unidireccional. A diferencia del GLM, el entrenamiento del GLM­130B

entrenados en código GitHub filtrado en populares

Incluye una pequeña cantidad de instrucción previa a tareas múltiples.

lenguajes y luego se ajustó en un nuevo conjunto de datos de programación competitivo

datos (5% de los datos totales) junto con el relleno de mascarilla autosupervisado.

llamado CodeContests. El conjunto de datos de CodeContests

Para estabilizar el entrenamiento, se aplica una capa de incrustación que se
contrae en gradiente.

Contiene principalmente problemas, soluciones y casos de prueba recopilados.
desde la plataforma Codeforces3 .

La preformación emplea objetivos de

modelado de lenguaje estándar, mientras que GOLD [134] con

LLaMA [127, 21]: un conjunto de modelos de lenguaje exclusivos para decodificadores
variando de 7B a 70B parámetros. La serie de modelos LLaMA es

El templado [135] sirve como objetivo de capacitación para el ajuste de los datos de

el más famoso entre la comunidad por la eficiencia de los parámetros

CodeContests. Para evaluar el desempeño de

y ajuste de instrucciones.

AlphaCode, se organizan concursos de programación simulados

LLaMA­1 [127]: Implementa atención causal eficiente [128]

en la plataforma Codeforces: en general, AlphaCode ocupa el primer lugar

al no almacenar ni calcular pesos de atención enmascarados y

supera el 54,3% entre más de 5000 competidores, donde su Codeforces

puntuaciones de clave/consulta. Otra optimización es reducir el número

La calificación se encuentra dentro del 28% superior de los usuarios que participaron recientemente.

CodeT5+ [34]: CodeT5+ se basa en CodeT5 [136], con

de activaciones recalculadas en el pase hacia atrás, como en [129].

Codificador superficial y decodificador profundo, entrenados en múltiples etapas.

LLaMA­2 [21]: Este trabajo está más centrado en afinar un
Mejor y más seguro modelo LLaMA­2­Chat para la generación de diálogo.

inicialmente datos unimodales (código) y luego datos bimodales (código de texto

El modelo previamente entrenado tiene un 40% más de datos de entrenamiento con una mayor

pares). Cada etapa formativa tiene diferentes objetivos formativos y
Activa diferentes bloques de modelo: codificador, decodificador o ambos según la tarea.

longitud del contexto y atención de consultas agrupadas.

El preentrenamiento unimodal incluye span

PanGu­Σ [92]: Un modelo autorregresivo con parámetros

objetivos de eliminación de ruido y CLM, mientras que el preentrenamiento bimodal

copiado de PanGu­α y ampliado a una escala de un billón con Ran­dom Routed Experts

Los objetivos contienen aprendizaje contrastivo, emparejamiento y CLM para

(RRE), se muestra el diagrama arquitectónico

pares texto­código. CodeT5+ agrega tokens especiales con el texto a

en la Figura 10. RRE es similar a la arquitectura MoE, con

habilitar modos de tarea, por ejemplo, [CLS ] para pérdida de contraste,

distinciones en el segundo nivel, donde las fichas se asignan aleatoriamente

[Coincidencia] para hacer coincidir código de texto, etc.

enrutado a expertos en un dominio en lugar de utilizar un método de activación que se

StarCoder [137]: Un modelo solo decodificador con SantaCoder

pueda aprender. El modelo tiene capas inferiores densamente activadas.

arquitectura, empleando atención Flash para ampliar el contexto

y se comparten en todos los dominios, mientras que las capas superiores son escasamente

longitud a 8k. El StarCoder entrena un codificador para filtrar nombres,

activado según el dominio. Este estilo de entrenamiento permite

correos electrónicos y otros datos personales de los datos de entrenamiento. Su variante

extraer modelos específicos de tareas y reduce los efectos catastróficos del olvido en el

optimizada supera a PaLM, LLaMA y LAMDA en

caso del aprendizaje continuo.

Puntos de referencia HumanEval y MBPP.

3.1.2. Codificación

3.1.3. Conocimiento científico

CodeGen [130]: CodeGen tiene una arquitectura similar a

Galactica [138]: un gran corpus curado de ciencia humana

PaLM [15], es decir, atención paralela, capas MLP e incrustaciones de RoPE. El modelo

conocimiento con 48 millones de artículos, libros de texto, apuntes de conferencias,

está entrenado tanto en lenguaje natural como en

millones de compuestos y proteínas, sitios web científicos, enciclopedias y más se

datos del lenguaje de programación de forma secuencial (entrenado en el primer

entrenan utilizando la biblioteca metaseq3,

conjunto de datos, luego el segundo y así sucesivamente) en los siguientes conjuntos de datos

que se basa en PyTorch y fairscale [139]. El modelo se envuelve

1) PILA, 2) BIGQUERY y 3) BIGPYTHON. CodeGen propuso un enfoque de varios

conjuntos de datos de razonamiento con el token <work> para proporcionar un contexto

pasos para sintetizar código. El propósito

de razonamiento paso a paso al modelo, que se ha demostrado que

es simplificar la generación de secuencias largas donde el mensaje anterior y el código

mejorar el desempeño en tareas de razonamiento.

generado se dan como entrada con el siguiente
Solicite generar la siguiente secuencia de código. CodeGen abre un punto de referencia
de programación de múltiples turnos (MTPB) para evaluar la síntesis de programas de

2https://github.com/features/copilot
3https://codeforces.com/

múltiples pasos.
10

Machine Translated by Google

Figura 11: Una imagen de ejemplo muestra una instancia del paradigma de entrenamiento
Flan, tomada de [16].

Figura 10: Este ejemplo ilustra la arquitectura PanGu, como se muestra en la imagen

con un incremento de cálculo mínimo, por ejemplo, 0,2% del preentrenamiento

procedente de [92].

total para PaLM 540B [16].
En esta sección revisamos varios LLM y estrategias perfeccionadas para un
ajuste eficaz.

3.1.4. Dialog
LaMDA [140]: un modelo solo decodificador previamente entrenado en datos

3.2.1. Ajuste de instrucciones con conjuntos de datos creados manualmente

de diálogo público, expresiones de diálogo público y documentos web públicos,

En la literatura se proponen numerosos conjuntos de datos de ajuste de

donde más del 90% de los datos de entrenamiento previo están en inglés. LaMDA

instrucción hechos a mano con diferentes opciones de diseño para ajustar la

está capacitado con el objetivo de producir respuestas que exhiban altos niveles

instrucción LLM. El rendimiento de los LLM ajustados depende de múltiples

de calidad, seguridad y conexión a tierra. Para lograr esto, se incorporan

factores, como el conjunto de datos, la diversidad de instrucciones, las plantillas

técnicas de ajuste discriminativo y generativo para mejorar los aspectos de

de indicaciones, el tamaño del modelo y los objetivos de capacitación.

seguridad y calidad del modelo. Como resultado, los modelos LaMDA se pueden

Teniendo esto en cuenta, en la literatura han surgido diversos modelos ajustados

utilizar como modelo de lenguaje general para realizar diversas tareas.

que utilizan conjuntos de datos creados manualmente.
Los modelos T0 [17] y mT0 (multilingüe) [144] emplean plantillas para convertir
conjuntos de datos existentes en conjuntos de datos rápidos.

3.1.5. Finanzas

Han mostrado mejoras en la generalización a tareas de tiro cero y rezagadas. Tk­
Instruct [18] ajustó el modelo T5 con instrucciones en contexto para estudiar la

BloombergGPT [141]: Un modelo decodificador no causal entrenado utilizando
conjuntos de datos financieros ("FINPILE" del archivo de Bloomberg) y de

generalización de tareas invisibles cuando se les dieron instrucciones en contexto

propósito general. La arquitectura del modelo es similar a BLOOM [13] y OPT

durante el tiempo de prueba. El modelo superó a Instruct­GPT, a pesar de ser

[14]. Asigna 50B parámetros a diferentes bloques del modelo utilizando el

más pequeño en tamaño, es decir, 11B de parámetros en comparación con 175B

enfoque [113].

de GPT­3.

Para una capacitación eficaz, BloombergGPT incluye documentos junto con < |

Aumento de tareas y configuraciones de avisos: el rendimiento de disparos cero

endo f text| > para utilizar la longitud máxima de secuencia, utiliza un tamaño de

y de pocos disparos mejora significativamente al expandir la colección de tareas

lote de calentamiento que va de 1024 a 2048 y reduce manualmente la tasa de

y los estilos de avisos. OPT­IML [97] y Flan [16] seleccionaron conjuntos de datos
de tareas más grandes de 2k y 1,8k, respectivamente. Si bien aumentar el tamaño

aprendizaje varias veces durante el entrenamiento.

de la tarea por sí solo no es suficiente, OPT­IML y Flan agregan más
configuraciones de indicaciones en sus conjuntos de datos, de disparo cero, de

Xuan Yuan 2.0 [142]: Un modelo de chat financiero chino con arquitectura

pocos disparos y CoT. A continuación, CoT Collection [101] afina aún más Flan­

BLOOM [13] entrenado en una combinación de conjuntos de datos de propósito
general, financieros, instrucciones de propósito general y de instituciones

T5 en 1,88 millones de muestras de CoT. Otro método [102] utiliza tareas

financieras. Xuan Yuan 2.0 combinó las etapas de preentrenamiento y ajuste

simbólicas con tareas en T0, Flan, etc.

para evitar olvidos catastróficos.
3.2.2. Ajuste de instrucciones con conjuntos de datos generados por LLM

3.2. LLM perfeccionados

Generar un conjunto de datos de ajuste de instrucciones requiere escribir
cuidadosamente instrucciones y pares de entrada­salida, que a menudo están

Los LLM previamente capacitados tienen excelentes habilidades de

escritos por humanos, son de menor tamaño y menos diversos. Para superar

generalización para tareas invisibles. Sin embargo, debido a que generalmente
están capacitados con el objetivo de predecir el próximo token, los LLM tienen

esto, la autoinstrucción [19] propuso un enfoque para incitar a los LLM disponibles

una capacidad limitada para seguir la intención del usuario y son propensos a

a generar conjuntos de datos de ajuste de instrucciones.

generar respuestas poco éticas, tóxicas o inexactas [20]. Para su utilización

La autoinstrucción superó en un 33% a los modelos entrenados en un conjunto

eficaz, los LLM están ajustados para seguir instrucciones [16, 17, 97] y generar

de datos creado manualmente SUPER­NATURALINSTRUCTIONS (un conjunto

respuestas seguras [20], lo que también da como resultado una mayor

de datos con más de 1600 tareas) [18] . Comienza con una semilla de 175 tareas,

instrucción y 1 muestra por tarea y genera iterativamente
generalización de tareas cruzadas, de pocas oportunidades y de cero disparos [97 , 16, 118],
11

Machine Translated by Google

Tabla 1: Hallazgos e ideas notables de modelos de lenguaje grande previamente entrenados.

Modelos

Hallazgos y conocimientos
• El codificador y el decodificador con parámetros compartidos funcionan de manera equivalente cuando los parámetros no se comparten

T5

• El ajuste fino de las capas del modelo (capas adaptadoras) funciona mejor que la forma convencional de entrenar solo
capas de clasificación

GPT­3

mT5

PanGu­α

• El rendimiento de los LLM de pocas oportunidades es mejor que el de ninguna oportunidad, lo que sugiere que los LLM son meta­
estudiantes

• Los grandes modelos multilingües funcionan de manera equivalente a los modelos de un solo idioma en tareas posteriores.
Sin embargo, los modelos multilingües más pequeños funcionan peor

• Los LLM tienen buenas capacidades de pocos disparos

• El ajuste rápido requiere actualizar muy pocos parámetros mientras se logra un rendimiento comparable al ajuste fino completo del modelo.
• El ajuste rápido requiere más tiempo
para converger en comparación con el ajuste completo del modelo. • La inserción de tokens rápidos entre oraciones puede
CPM­2

permitir el modelo para comprender las relaciones entre oraciones y secuencias largas • En un análisis, CPM­2 encuentra que los estímulos
funcionan como proveedor (contexto
adicional) y agregador
(agregar información con el texto de entrada) para el modelo

• Una arquitectura LLM modular con un módulo de representación universal y representación de tareas específicas.
ERNIE 3.0

El módulo de programación ayuda en la fase de ajuste
• Optimizar los parámetros de una red de representación de tareas específicas durante la fase de ajuste es
una forma eficiente de aprovechar el potente modelo previamente entrenado

• El rendimiento de LLM está altamente relacionado con el tamaño de la red. • Para
mejorar el rendimiento del tiempo de ejecución, se pueden realizar más operaciones en paralelo (ancho) en lugar de
Jurásico­1

secuencial (profundidad)
• Para representar y ajustar eficientemente más texto en la misma longitud de contexto, el modelo utiliza un vocabulario más amplio para
entrenar un tokenizador SentencePieza sin restringirlo a límites de palabras. Esto beneficia aún más en tareas de aprendizaje de pocas
oportunidades.

HiperCLOVA

yuanes 1,0

• Al emplear un ajuste basado en indicaciones, se puede mejorar el rendimiento de los modelos, superando a menudo
los de los modelos de última generación cuando los gradientes hacia atrás de las entradas son accesibles

• La arquitectura del modelo que sobresale en casos de preentrenamiento y ajuste fino puede exhibir un comportamiento contrastante en el
aprendizaje de pocas oportunidades y de cero oportunidades.

Ardilla de tierra

ERNIE 3.0 Titán

• Las codificaciones relativas permiten que el modelo evalúe secuencias más largas que el entrenamiento.

• Mejora la pérdida de confrontación autosupervisada adicional para distinguir entre texto real y generado.
el rendimiento del modelo en comparación con ERNIE 3.0

• Atención paralela + capas FF aceleran el entrenamiento un 15% con el mismo rendimiento que con cascada
capas
GPT­NeoX­20B

• La inicialización de las capas de salida de avance antes de los residuos con el esquema en [143] evita que las activaciones crezcan al
aumentar la profundidad y el ancho. • El entrenamiento en Pile
supera a GPT­3 en cinco disparos.
La tabla continúa en la página siguiente

12

Machine Translated by Google

Modelos

OPTAR

Hallazgos y conocimientos
• Reinicie el entrenamiento desde un punto de control anterior con una tasa de aprendizaje más baja si la pérdida
diverge. • El modelo es propenso a generar texto repetitivo y quedarse atrapado en un bucle.

• El rendimiento de Galactica ha seguido mejorando en el conjunto de validación y en los puntos de referencia dentro y
fuera del dominio, incluso con múltiples repeticiones del corpus, lo que es superior a la investigación existente sobre LLM.
Galáctica

• Un enfoque de token de memoria de trabajo puede lograr un rendimiento sólido sobre los métodos existentes en los
puntos de referencia matemáticos MMLU y MATH. Establece un nuevo estado del arte en varios procesos posteriores
Tareas como PubMedQA (77,6%) y MedMCQA dev (52,9%).

• La capacidad del modelo se puede mantener con un cálculo reducido reemplazando la capa de avance
en cada capa del transformador con una mezcla de expertos (MoE)
• El modelo entrenado con datos filtrados muestra consistentemente mejores desempeños en tareas NLG y NLU, donde el efecto del
filtrado es más significativo en las primeras tareas. • Los corpus de preentrenamiento filtrados
GLAM

desempeñan un papel crucial en la capacidad de generación de LLM, especialmente para
las tareas posteriores
• La ampliación de los modelos GLaM MoE se puede lograr aumentando el tamaño o el número de expertos en la capa MoE. Dado un
presupuesto fijo de cálculo, más expertos contribuyen a un mejor desempeño.
mance

LaMDA

• El modelo se puede ajustar para aprender a llamar a diferentes recursos y herramientas de información externa.

• Para una mayor efectividad y eficiencia, se puede construir un modelo de transformador asimétricamente con un codificador menos
profundo y un decodificador más profundo. • Para lograr mejores
rendimientos, es necesario emplear estrategias como el escalado masivo.
código alfa

muestreo adicional, seguido del filtrado y agrupamiento de muestras en un conjunto compacto
• La utilización de arquitecturas de transformadores novedosas y eficientes en el muestreo diseñadas para facilitar
el muestreo a escala es crucial •
Simplificar las descripciones de los problemas puede mejorar eficazmente el rendimiento del modelo

Chinchilla

• El tamaño del modelo y el número de tokens de entrenamiento deben escalarse proporcionalmente: por cada doble
bling del tamaño del modelo, la cantidad de tokens de entrenamiento también debe duplicarse

• Los modelos centrados en el inglés producen mejores traducciones al traducir al inglés en comparación con los modelos no centrados en el inglés.

Inglés
• Los modelos generalizados pueden tener un rendimiento equivalente para la traducción de idiomas a pequeños especializados.
Palmera

modelos
• Los modelos más grandes tienen un mayor porcentaje de memorización de datos de entrenamiento.
• El rendimiento aún no se ha saturado ni siquiera a una escala de 540B, lo que significa que es probable que los modelos más grandes
desempeñarse mejor

• La arquitectura codificador­decodificador es más adecuada para capacitar a los LLM dada la atención bidireccional a la
contexto que solo decodificador
AlexaTM

• Se puede agregar la tarea de modelado de lenguaje causal (CLM) para beneficiar al modelo con un contexto eficiente.
aprendiendo

• Colocar la norma de capa al comienzo de cada capa transformadora mejora la estabilidad del entrenamiento.
La tabla continúa en la página siguiente

13

Machine Translated by Google

Modelos

Hallazgos y conocimientos
• El entrenamiento con una combinación de eliminadores de ruido supera a PaLM cuando se entrena más para algunos FLOP más

U­PaLM

• El entrenamiento con una mezcla de eliminadores de ruido mejora la capacidad de relleno y la generación de texto abierto.
diversidad

UL2

GLM­130B

CódigoGen

Llama

• La capacitación en cambio de modo permite un mejor rendimiento en tareas posteriores. • Las
indicaciones de CoT superan las indicaciones estándar para UL2
• Los datos previos al entrenamiento con una pequeña proporción de datos de instrucciones multitarea mejoran el modelo general
actuación
• La solicitud de varios pasos para la síntesis de código conduce a una mejor comprensión de la intención del usuario y a la generación de código.

eración

• Se observa una mejora constante del rendimiento al escalar el modelo. • Los modelos más pequeños
pueden lograr buenos rendimientos con más datos de entrenamiento y tiempo de computación.
• Los modelos dispersos brindan los beneficios de los modelos grandes a un menor costo de cálculo
• Los expertos enrutados aleatoriamente reducen los efectos catastróficos del olvido, lo que a su vez es esencial para

PanGu­Σ

aprendizaje continuo
• Los expertos enrutados aleatoriamente permiten extraer un submodelo específico de dominio en la implementación, que es
rentable manteniendo un rendimiento similar al original
• La capacitación previa con datos de propósito general y específicos de la tarea mejora el desempeño de la tarea sin perjudicar

BloombergGPT

XuanYuan 2.0

ing otras capacidades del modelo
• Combinar las etapas de preentrenamiento y ajuste en un solo entrenamiento evita olvidos catastróficos
• El LM causal es crucial para la capacidad de generación de un modelo en arquitecturas de codificador­decodificador.

CódigoT5+

• Múltiples objetivos de capacitación, como corrupción de tramos, LM causal, emparejamiento, etc., se complementan entre sí para
lograr un mejor rendimiento.

codificador estrella

• El mensaje HHH de Anthropic permite al modelo seguir instrucciones sin necesidad de realizar ajustes
• El modelo entrenado con datos no filtrados es más tóxico pero puede funcionar mejor en tareas posteriores después

Llama­2

sintonia FINA
• El modelo entrenado con datos sin filtrar requiere menos muestras para la alineación de seguridad
• La calidad de los datos es importante para entrenar mejores

PaLM­2

modelos • El tamaño del modelo y de los datos debe escalarse en proporciones 1:1 •
Los modelos más pequeños entrenados para iteraciones más grandes superan a los modelos más grandes

14

Machine Translated by Google

Tabla 2: Ideas y hallazgos clave del estudio de modelos de lenguaje grandes adaptados a la instrucción.

Modelos

T0

Hallazgos y conocimientos
• Las indicaciones para tareas múltiples permiten una generalización inmediata y superan las líneas de base. •
Incluso una sola indicación por tarea de conjunto de datos es suficiente para mejorar el rendimiento.
• Para ayudar al modelo a filtrar y utilizar información relevante de manera efectiva, los etiquetadores humanos desempeñan un papel
crucial al responder preguntas sobre la utilidad de los documentos recuperados. • La interacción de un modelo de

WebGPT

lenguaje optimizado con un entorno de navegación web basado en texto puede mejorar
Recuperación y síntesis de un extremo a otro mediante aprendizaje por imitación y aprendizaje por refuerzo.
• Generar respuestas con referencias puede hacer que los etiquetadores juzguen fácilmente la precisión objetiva de las respuestas.
• El ajuste de instrucciones conduce a una generalización más fuerte de tareas invisibles •

Tk­INSTRUIR

Más tareas mejoran la generalización, mientras que solo aumentar las instancias de tareas no ayuda • Los modelos
entrenados supervisados son mejores que los modelos generalizados • Los modelos
previamente entrenados con instrucciones y ejemplos funcionan bien para diferentes tipos de entradas
• El ajuste de la instrucción permite una generalización cero a tareas nunca antes vistas. • La capacitación
multilingüe conduce a una generalización cero aún mejor tanto para inglés como para otros idiomas.
Inglés

mT0 y BLOOMZ

• La capacitación sobre indicaciones traducidas automáticamente mejora el rendimiento de las tareas pendientes en idiomas distintos del inglés.
indicaciones

• El ajuste fino solo en inglés en el modelo de lenguaje multilingüe previamente entrenado es suficiente para generalizar a otras tareas
de lenguaje previamente entrenado
• Crear un lote con múltiples ejemplos de tareas es importante para un mejor rendimiento. • Sólo el muestreo
proporcional de ejemplo no es suficiente; los conjuntos de datos de entrenamiento también deben ser proporcionales.
para una mejor generalización/rendimiento
OPT­IML

• El rendimiento de las tareas totalmente retenidas y parcialmente supervisadas mejora al escalar las tareas o categorías, mientras
que las tareas totalmente supervisadas no tienen ningún efecto.
• Incluir pequeñas cantidades, es decir, el 5 % de los datos de preentrenamiento durante el ajuste fino, es
efectivo. • Sólo el 1 % de los datos de razonamiento mejora el rendimiento; agregar más deteriora el rendimiento. • Agregar
datos de diálogo empeora el rendimiento.
• El criterio de los etiquetadores y las reglas de alineación bien definidas ayudan al modelo a generar mejores respuestas. • Los
buenos objetivos de diálogo se pueden dividir en reglas detalladas de lenguaje natural para el agente y el

Gorrión

evaluadores

• La combinación de aprendizaje por refuerzo (RL) con reclasificación produce un rendimiento óptimo en
términos de preferencia, tasas de ganancia y resiliencia frente a investigaciones adversas
• El ajuste con CoT mejora el rendimiento en tareas pendientes • El ajuste junto con
los datos de CoT mejora las capacidades de razonamiento • El ajuste de CoT mejora
el razonamiento de disparo cero • El rendimiento mejora
Flan

con más tareas • El ajuste de instrucciones mejora la
usabilidad que de otro modo sería un desafío para los pre­ modelos entrenados • Mejorar el rendimiento del modelo con el ajuste
de instrucciones es eficiente en términos de computación • Las indicaciones multitarea permiten habilidades
de generalización de tiro cero en LLM

AsistenteCodificador

LLaMA­2­Chat

LIMA

• El ajuste fino con datos de ajuste de instrucciones reescritos en un conjunto complejo mejora el rendimiento
• El modelo aprende a escribir respuestas seguras con ajustes en demostraciones seguras, mientras que
El paso de RLHF mejora aún más la seguridad del modelo y lo hace menos propenso a ataques de jailbreak
• Los datos de menor calidad son suficientes para una generalización precisa del modelo.

15

Machine Translated by Google

nuevas instrucciones (52k) e instancias (82k pares de entrada­salida) usando GPT­3 [6].

Alinearse directamente con SFT: el PPO en el proceso de RLHF es complejo, requiere

Por el contrario, Dynosaur [145] utiliza los metadatos de conjuntos de datos en

mucha memoria e inestable, y requiere múltiples modelos, recompensa, valor, políticas

Huggingface para solicitar a los LLM que generen conjuntos de datos de ajuste de

y modelos de referencia.

instrucciones de múltiples tareas.

Es posible evitar este sofisticado proceso de alineación incorporando cambios mínimos

LLaMA Tuned: Varios modelos en la literatura ajustan las instrucciones de

en el proceso de ajuste fino supervisado (SFT) como en [158, 159, 160], con un

LLaMA [146] con conjuntos de datos generados por GPT­3 [6] o GPT­4 [147] .

rendimiento mejor o comparable al de PPO. La optimización de preferencias directas

Entre estos, Alpaca [148], Vicuña [149] y LLaMA­GPT­4 [150] son algunos

(DPO) [158] entrena un modelo directamente en las respuestas preferidas por los

modelos ajustados de propósito general, donde Alpaca se entrena con 52k

humanos para maximizar la probabilidad de respuestas preferidas frente a las no

muestras de text­davinci­003, Vicuna con 70k muestras de ShareGPT.com y

preferidas, con un peso de importancia por muestra.

LLaMA­GPT­4 recreando las instrucciones de Alpaca de GPT­4. Goat [151]
afina LLaMA para tareas aritméticas (1 millón de muestras) generando datos

Ajuste fino de la clasificación de recompensas RAFT [159] ajusta el modelo en función

desde ChatGPT y supera a GPT­4, PaLM, BLOOM, OPT, etc., atribuyendo

de las respuestas clasificadas según el modelo de recompensa. La optimización de

su éxito a la consistente tokenización de números de LLaMA.

clasificación de preferencias (PRO) [161] y RRHF [160] penalizan el modelo para
clasificar las respuestas con preferencias humanas y pérdida supervisada.
Por otro lado, la cadena de retrospectiva (CoH) [162] proporciona retroalimentación al

HuaTuo [152] es un modelo de conocimiento médico, perfeccionado con un conjunto de

modelo en lenguaje en lugar de recompensa, para aprender respuestas buenas versus

datos de control de calidad generado de 8k instrucciones.

malas.

Instrucciones complejas: Evol­Instruct [153, 154] solicita a los LLM que conviertan las

Alinearse con la retroalimentación sintética: Alinear los LLM con la retroalimentación

instrucciones dadas en un conjunto más complejo.

humana es lento y costoso. La literatura sugiere un proceso semiautomático para alinear

Las instrucciones evolucionan de forma iterativa con la reescritura de instrucciones en

los LLM instándolos a generar respuestas útiles, honestas y éticas a las consultas, y

términos complejos y la creación de nuevas instrucciones. Con este estilo de generación

realizando ajustes utilizando el conjunto de datos recién creado. La IA constitucional

automatizada de instrucciones, WizardLM [153] (LLaMA afinado en instrucciones de

[163] reemplaza la retroalimentación humana en RLHF con IA, llamándola RL a partir de

250k) supera a Vicuna y Alpaca, y WizardCoder [154] (StarCoder afinado) supera a

retroalimentación de IA (RLAIF). AlpacaFarm [164] diseña indicaciones para imitar la

Claude­Plus, Bard y otros.

retroalimentación humana utilizando las API de LLM. A diferencia de la IA constitucional,
AlpacaFarm inyecta ruido en retroalimentación para replicar los errores humanos. Self­
Align [98] solicita al LLM ejemplos de ICL, instruyendo al LLM sobre lo que debe contener
la respuesta para que se considere útil y ética.

3.2.3. Alinearse con las preferencias humanas
Posteriormente, el mismo LLM se perfecciona con el nuevo conjunto de datos.

La incorporación de las preferencias humanas en los LLM presenta una ventaja
significativa para mitigar comportamientos indeseables y garantizar resultados precisos.

Alinearse con las indicaciones: los LLM pueden orientarse con indicaciones para generar

El trabajo inicial sobre alineación, como InstructGPT [20], alinea GPT­3 utilizando un

respuestas deseables sin capacitación [165, 166]. La indicación de autocorrección en

enfoque de 3 pasos, ajuste de instrucciones, modelado de recompensas y ajuste con

[166] concatena instrucciones y CoT con preguntas, guiando al modelo a responder sus

aprendizaje por refuerzo (RL). Se consulta el GPT­3 supervisado y afinado en las

instrucciones siguiendo una estrategia para garantizar la seguridad moral antes de la

demostraciones para generar respuestas, que los etiquetadores humanos clasifican de

respuesta real. Se ha demostrado que esta estrategia reduce significativamente el daño

acuerdo con los valores humanos, y se entrena un modelo de recompensa con los datos

en las respuestas generadas.

clasificados. Por último, el GPT­3 se entrena con optimización de políticas próximas
Red­Teaming/Jailbreaking/Ataques adversarios: los LLM exhiben comportamientos

(PPO) utilizando recompensas en los datos generados por el modelo de recompensa.
LLaMA 2­Chat [21] mejora la alineación dividiendo el modelado de recompensas en

dañinos, alucinaciones, filtración de información personal y otras deficiencias a través

recompensas de ayuda y seguridad y utilizando muestreo de rechazo además de PPO.

de investigaciones adversas.

Las cuatro versiones iniciales de LLaMA 2­Chat se ajustan con muestreo de rechazo y

Los modelos son susceptibles de generar respuestas dañinas aunque estén alineados

luego con PPO además del muestreo de rechazo.

por razones de seguridad [167, 168]. El equipo rojo es un enfoque común para abordar
los resultados ilícitos, donde se insta a los LLM a generar resultados dañinos [168, 169].

El conjunto de datos recopilado a través del equipo rojo se utiliza para ajustar los modelos
Alineación con evidencia respaldada: este estilo de alineación permite que el modelo

por motivos de seguridad. Si bien el equipo rojo depende en gran medida de anotadores

genere respuestas con pruebas y hechos, reduce las alucinaciones y ayuda a los

humanos, otro trabajo [170] LLM del equipo rojo para encontrar indicaciones que

humanos de manera más efectiva, lo que aumenta la confianza en los resultados del

conduzcan a resultados perjudiciales para otros LLM.

modelo. De manera similar al estilo de entrenamiento RLHF, se entrena un modelo de
recompensa para clasificar las respuestas generadas que contienen citas web en las
3.2.4. Continuar con el entrenamiento previo

respuestas a las preguntas, que luego se usa para entrenar el modelo, como en
GopherCite [155], WebGPT [156] y Sparrow [157]. . El modelo de clasificación en

Aunque el ajuste mejora el rendimiento de un modelo, conduce a un olvido catastrófico

Sparrow [157] se divide en dos ramas, recompensa de preferencia y recompensa de

de información aprendida previamente.

regla, donde los anotadores humanos prueban el modelo para romper una regla. Estas

La concatenación de datos de ajuste fino con algunas muestras de preentrenamiento

dos recompensas juntas clasifican una respuesta para entrenar con RL.

seleccionadas aleatoriamente en cada iteración evita el olvido de la red [171, 142]. Esto
también es eficaz para adaptar los LLM en casos en los que los datos de ajuste son
pequeños y la capacidad original
16

Machine Translated by Google

Se debe mantener la calidad. Entrenamiento previo continuo basado en indicaciones

liviano y el otro con atención pesada y capas de retroalimentación. Todos los tokens

(PCP) [172] entrena el modelo con texto e instrucciones relacionadas

se procesan desde el peso ligero.

a las tareas y luego, finalmente, las instrucciones ajustan el modelo para las
tareas posteriores.

rama, y solo los tokens importantes se enrutan a la rama pesada. LongNet [178]

3.2.5. Eficiencia de la muestra

LongLoRA [179] propone atención por turnos cortos, utilizada durante

reemplaza la atención estándar con
Atención dilatada, ampliando la longitud de la secuencia a mil millones de tokens.

ajuste para reducir los costos de atención densa. Sin embargo, el modelo

Si bien los datos de ajuste son generalmente mucho más pequeños que los

durante la inferencia utiliza atención densa y logra un rendimiento similar al del ajuste

los datos previos al entrenamiento, todavía tienen que ser lo suficientemente grandes

fino de la atención total.

para un rendimiento aceptable [16, 97, 18] y requieren recursos informáticos

Extrapolación sin entrenamiento: LM­Infinite [176] y ventanas de contexto paralelo

proporcionales. Estudiar los efectos sobre el rendimiento con menos

(PCW) [180] muestran la extrapolación de longitud

datos, la literatura existente [173, 174] encuentra que los modelos entrenados

es posible utilizando LLM previamente capacitados. LM­Infinite sugirió atención en

con menos datos puede superar a los modelos entrenados con más datos.

forma de Λ aplicada dentro de la ventana de contexto original

En [173], el 25% del total de datos posteriores se encuentra suficiente

límites. Del mismo modo, PCW fragmenta entradas más grandes en el grupo pre­entrenado.

para un rendimiento de última generación. Selección de 0,5% basado en núcleos

longitudes de contexto y aplica las mismas codificaciones posicionales a

del total de datos de ajuste de instrucciones mejora el rendimiento del modelo en un

cada trozo.

2% en [174], en comparación con el ajuste completo de datos. Menos es más para la
alineación (LIMA) [175] usa solo 1000

3.4. LLM aumentados

demostraciones cuidadosamente creadas para afinar el modelo y ha
logró un rendimiento comparable al GPT­4.

Los LLM son capaces de aprender de los ejemplos concatenados con la entrada,
lo que se conoce como aumento de contexto, aprendizaje en contexto (ICL) o

3.3. Ventana de contexto creciente

indicaciones de pocas tomas. Muestran una excelente generalización a tareas
invisibles con pocas indicaciones, lo que permite a los LLM responder consultas más

Los LLM se entrenan con ventanas de contexto limitadas debido a la

allá de la capacidad adquirida durante la capacitación [6, 55]. Estas habilidades

costosa atención y los altos requisitos de memoria. un modelo

emergentes permiten

entrenado en longitudes de secuencia limitadas no logra generalizar a lo invisible

para adaptar el modelo sin realizar ajustes finos, un proceso costoso.

longitudes en el momento de la inferencia [176, 49]. Alternativamente, LLM con

Aparte de esto, las alucinaciones, que producen resultados inexactos, inseguros,

Las codificaciones posicionales ALiBi [65] pueden realizar longitudes de disparo cero

o respuestas objetivamente incorrectas, es común para los LLM, lo cual es

extrapolación. Sin embargo, ALiBi tiene menos poder expresivo [66]

evitarse aumentando los datos contextuales. Si bien el usuario puede proporcionar

y rendimiento inferior en múltiples puntos de referencia [46], y

muestras en contexto en la consulta [54, 32], aquí nos referimos específicamente a

Muchos LLM utilizan la incrustación posicional de RoPE que no puede

los métodos que acceden al almacenamiento externo mediante programación,

realizar una extrapolación de tiro cero. Una mayor longitud de contexto tiene

llamándolos LLM aumentados.

beneficios como una mejor comprensión de documentos más largos,

La literatura sugiere varios diseños de memoria externa para aumentar los LLM, a

más muestras en aprendizaje en contexto, ejecución de procesos de razonamiento

largo plazo [181, 182, 183, 184], a corto plazo [185],

más grandes, etc. Ampliar la longitud del contexto durante el ajuste es lento,

simbólico [186] y no simbólico [187, 188]. la memoria

ineficiente y computacionalmente costoso [49].

se puede mantener en diferentes formatos como documentos, vectores o bases de

Por lo tanto, los investigadores emplean varias técnicas de extrapolación de ventanas

datos. Algunos sistemas mantienen representaciones de memoria intermedias para

de contexto que se analizan a continuación.

retener información a través de múltiples iteraciones [184, 182], mientras que otros

Interpolación de posición: en lugar de extrapolar, [49] muestra

extraen información importante.

que la interpolación de codificaciones de posición dentro de la ventana de
contexto previamente entrenada es más efectiva. El trabajo demuestra que

de los conjuntos de datos y guárdelo en la memoria para recuperarlo [189]. El

sólo 1000 pasos de ajuste son suficientes para lograr mejores resultados en

Las operaciones de lectura y escritura de memoria se realizan con

ventanas más grandes sin reducir el rendimiento en comparación con el

o sin la cooperación de LLM [182, 190, 184, 191], actuando como

una señal de retroalimentación en [185]. A continuación, analizamos diferentes
tipos de LLM aumentados.

tamaño del contexto original. Giraffe [46] utiliza escalamiento de potencia en
RoPE, y YaRN [47] propuso una interpolación compatible con NTK.

3.4.1. LLM de recuperación aumentada

Mecanismo de atención eficiente: la atención global densa es

Los LLM pueden tener memoria limitada e información desactualizada,

una de las principales limitaciones en la formación de LLM de ventana de contexto
más amplio. El uso de variantes de atención eficientes, como la atención local,

dando lugar a respuestas inexactas. La recuperación de información relevante del

dispersa y dilatada, reduce el costo de cálculo.

almacenamiento externo actualizado permite a los LLM

de modo significativo. LongT5 [48] propone atención global transitoria (TGlobal),

Responda con precisión con referencias y utilice más información. Con el aumento

aplicando atención a tokens locales y globales.

de la recuperación, se han desarrollado modelos más pequeños.

(promedio de tokens en ventana). El modelo reemplaza la atención.

Se ha demostrado que funciona a la par con modelos más grandes. Por ejemplo, el

en T5 [10] con atención TGlobal, entrena previamente el modelo en

El modelo 11B puede llegar a ser competitivo con el 540B PaLM en [25] y

Longitud de secuencia 4098, ajustes finos en tamaños de ventana más grandes, como

Gopher de 7,5B a 280B en [183]. Recuperación de lenguaje aumentado.

con un tamaño de hasta 16k y mejora el rendimiento de las tareas en entradas más largas.

El modelado (RALM) tiene dos componentes principales, que se muestran en

Esto muestra la capacidad de extrapolación de la atención TGlobal con

Figura 12, a saber: 1) recuperador y 2) modelo de lenguaje. En

sólo ajustes finos. COLT5 [177] utiliza dos ramas, una con

RALM, el perro perdiguero juega un papel crucial en el impulso de LLM
17

Machine Translated by Google

ación. Las muestras recuperadas se clasifican para generar datos reales para entrenar
a los recuperadores con aprendizaje contrastivo en [196, 198].
RoBERTa está capacitado para tareas posteriores en [197] para la recuperación de
muestras de ICL. REPLUG [199] entrena al recuperador con señales supervisadas de
las salidas congeladas generadas por LLM.
Entrenamiento de Retriever y LLM: se logran más beneficios entrenando tanto al retriever
como al modelo en [25, 200, 201]. En este caso, el error se propaga al recuperador,
actualizando tanto el modelo de lenguaje como el recuperador. Si bien el modelado de
lenguaje enmascarado (MLM) es un objetivo de preentrenamiento común [25, 201], el
transformador de recuperación pre­entrenado (RPT) [200] utilizó la predicción de
fragmentos de documentos como un objetivo de preentrenamiento para el modelado de
texto largo.

Figura 12: Diagrama de flujo de LLM de recuperación aumentada. El recuperador
extrae un contexto similar a la entrada y lo envía al LLM, ya sea en un lenguaje
simple o codificado a través de Fusion­in­Decoder (FiD). Dependiendo de la tarea,
la recuperación y generación pueden repetirse varias veces.

Aumento de contexto codificado: la concatenación de documentos recuperados con la
consulta se vuelve inviable a medida que aumentan la longitud de la secuencia y el
tamaño de la muestra. Codificar el contexto y fusionarlo con el decodificador (Fusion­in­
Decoder) utilizando atención cruzada hace posible aumentar más muestras sin aumentar
significativamente los costos de cálculo [202, 183, 200, 25].

respuesta, donde la información incorrecta puede llevar a los LLM a un comportamiento
falso. Esto conduce al desarrollo de varios métodos para recuperar información precisa

Web aumentada: la memoria almacenada localmente, pero externa a LLM, tiene

y fusionarla con la consulta para obtener un mejor rendimiento.

información limitada. Sin embargo, en Internet existe una gran cantidad de información

Aumento de recuperación de disparo cero: este tipo de aumento mantiene la arquitectura

métodos recuperan el contexto relacionado con la consulta a través de una búsqueda en

y los pesos originales de LLM sin cambios y utiliza BM25 [192], vecinos más cercanos o

la web y lo reenvían a los LLM [203, 204, 156].

que se actualiza periódicamente. En lugar de almacenar información localmente, varios

modelos congelados previamente entrenados como Bert [7] como recuperador. La
información recuperada se proporciona como entrada al modelo para la generación de
respuestas, y se ha demostrado que mejora el rendimiento con respecto a los LLM sin
3.4.2. LLM aumentados con herramientas

recuperación [188, 193]. En algunos escenarios, se requieren múltiples iteraciones de
recuperación para completar la tarea. El resultado generado en la primera iteración se

Mientras que RAG se basa en el recuperador para proporcionar contexto al LLM para

envía al recuperador para que busque documentos similares. La recuperación activa

responder consultas, los LLM aumentados con herramientas aprovechan las capacidades

prospectiva (FLARE) [187] genera inicialmente la respuesta y corrige el resultado

de razonamiento de los LLM para planificar de forma iterativa dividiendo las tareas en

recuperando documentos relevantes si la respuesta contiene tokens de baja confianza.

subtareas, seleccionando las herramientas necesarias y tomando acciones para

De manera similar, RepoCoder [194] recupera fragmentos de código de forma recursiva

completar la tarea [205 , 206, 207, 27]. En la Figura 13 se muestra una canalización

para completarlo.

genérica de LLM mejorados con herramientas , donde se seleccionan diferentes módulos
en la Figura 13 en un bucle hasta completar la tarea.

Entrenamiento con aumento de recuperación: para reducir las fallas en la generación de

Aumento de herramientas Zero­Shot: las habilidades de razonamiento y aprendizaje

aumento de recuperación (RAG), los investigadores entrenan o afinan a los recuperadores

en contexto de los LLM les permiten interactuar con herramientas sin capacitación.

y LLM con un canal de aumento de recuperación. A continuación analizamos la literatura

Razonamiento automático y uso de herramientas (ART) [207] crea una biblioteca de

en función de su enfoque en los respectivos procesos de capacitación del oleoducto.

tareas con demostraciones de pasos de razonamiento y llamada a herramientas externas.

LLM de capacitación: Transformador de recuperación mejorada (RETRO) [183] muestra

inferencias. Aparte de esto, [208] muestra que la documentación de la herramienta es

que los LLM más pequeños con capacitación previa con tubería RAG superan a los

suficiente para enseñar a los LLM a usar herramientas sin demostraciones. RestGPT

Recupera ejemplos de tareas similares y proporciona el contexto al LLM para realizar

LLM más grandes, como GPT­3 entrenado sin RAG.

[209] integra LLM con API RESTful descomponiendo las tareas en pasos de planificación

RETRO utiliza un subconjunto de tokens de 2 billones de MassiveText como

y selección de API. El selector de API comprende la documentación de la API para

base de datos. La canalización de recuperación divide la consulta de entrada
en subconjuntos y recupera fragmentos relevantes de la base de datos para

utiliza herramientas como tokens mediante la concatenación de incrustaciones de

seleccionar una API adecuada para la tarea y planificar la ejecución. ToolkenGPT [210]

cada subconjunto, codificados junto con representaciones intermedias de

herramientas con otras incrustaciones de tokens. Durante la inferencia, el LLM genera

entrada para generar tokens. Utiliza atención de fragmentos cruzados para

los tokens de herramienta que representan la llamada a la herramienta, detiene la

atender fragmentos anteriores de forma autorregresiva. Un estudio sobre

generación de texto y reinicia utilizando la salida de ejecución de la herramienta.

RETRO [195] muestra que los modelos previamente entrenados sin RAG
pero ajustados usando RAG carecen de las ganancias de rendimiento
obtenidas mediante el entrenamiento previo con RAG.

Capacitación con aumento de herramientas: los LLM están capacitados para interactuar

Training Retriever: la calidad de las respuestas generadas por los LLM depende en gran

con diversas herramientas, mejorando las capacidades de planificación para superar las

medida de los ejemplos en contexto. Por lo tanto, [196, 197, 198, 199] entrenan a los

limitaciones del aumento de herramientas de disparo cero [211, 27, 212, 213]. Las

recuperadores para que recuperen muestras precisas de unos pocos disparos mientras

instrucciones de Gorilla [211] ajustan LLaMA con recuperación de información de la

mantienen el LLM congelado para generación.

documentación API. Utiliza la autoinstruccion [19]
18

Machine Translated by Google

como el cerebro de los agentes. Los LLM se han incorporado en la web.
agentes [156, 157], agentes de codificación [219], agentes de herramientas [27, 213],
agentes encarnados [26] y agentes conversacionales [185], que requieren un ajuste
mínimo o nulo". A continuación resumimos la investigación en agentes autónomos
basados en LLM. Para obtener una información más detallada
discusión, consulte [220, 221].
LLM que dirigen agentes autónomos: los LLM son los agentes cognitivos
controladores de los agentes autónomos. Generan planes, razonan sobre tareas,
incorporan memoria para completar tareas y
Adaptar el esquema en función de la retroalimentación del entorno. Dependiendo de
las capacidades adquiridas de los LLM, muchos
Los métodos se afinan, proponen un mejor enfoque de indicaciones o utilizan diferentes
módulos para mejorar el desempeño de los agentes. Se describen brevemente los
módulos y estrategias empleados en agentes autónomos.
discutido a continuación.

Planificación y razonamiento: completar una tarea compleja requiere
pensamiento lógico humano, planificación de los pasos necesarios y
razonar direcciones actuales y futuras. Métodos de indicación
como la cadena de pensamientos [103], el árbol de pensamientos [105] y la
autoconsistencia [104] son fundamentales para los agentes, lo que incita a los LLM a

Figura 13: Un diagrama de flujo básico de LLM aumentados con herramientas. Dada una entrada y

razonar sus acciones y elegir entre diferentes caminos para completar la tarea. Cuando

un conjunto de herramientas disponibles, el modelo genera un plan para completar la tarea. El

a los LLM se les solicita una descripción de la tarea y

Los LLM aumentados con herramientas utilizan diferentes módulos de forma iterativa, como recuperador,
ejecución de herramientas, lectura­escritura en memoria, retroalimentación, etc., dependiendo de la tarea.

una secuencia de acciones, pueden generar con precisión acciones planificadas sin
ningún ajuste fino [222]. Razonamiento a través de la planificación
(RAP) [223] incorpora un LLM reutilizado como modelo mundial

canalización de generación de datos con GPT­4 al proporcionar en contexto

razonar sobre resultados futuros y explorar caminos alternativos

Ejemplos recuperados de la documentación de la API. Herramienta aumentada

para completar la tarea. Retroformer [224] utiliza una retrospectiva

modelo de lenguaje (TALM) [27] afina T5 [10] para el uso de herramientas

LLM para mejorar las principales capacidades de planificación y razonamiento de LLM

con un enfoque de juego automático, donde completa iterativamente la herramienta

al proporcionar señales útiles de tareas.

tareas de manipulación y las incluye nuevamente en el conjunto de entrenamiento.

Comentarios: Los LLM en sistemas de circuito abierto generan planes y asumen que el

ToolLLM [213] recopila 16k API de RapidAPI. muestra

agente los completará con éxito. Sin embargo,

API de la lista para generar un conjunto de datos de ajuste de instrucciones utilizando

el escenario real es diferente con fallas y respuestas variables del medio

ChatGPT en escenarios de una sola herramienta y de múltiples herramientas. Para

ambiente. Para completar correctamente las tareas,

conjuntos de datos de alta calidad, ToolLLM sugirió una búsqueda en profundidad basada en
Método de árbol de decisión (DFSDT) para generar verdades sobre el terreno con

Muchos métodos utilizan LLM en un circuito cerrado donde la respuesta a la acción se
proporciona como retroalimentación a los LLM para reevaluar y

razonamiento y planificación diversos.

actualizar el plan según sea necesario [225, 226, 227, 185]. Otra dirección de

Aumento de herramientas multimodales: el razonamiento compositivo

investigación explota los LLM como funciones de recompensa para capacitar

La capacidad de los LLM les permite manipular herramientas en entornos multimodales

políticas de aprendizaje por refuerzo (RL) en lugar de humanos [228].

[205, 206, 214]. Siguiendo la tubería mostrada

Memoria: los LLM pueden aprender del contexto proporcionado en el

En la Figura 13, el LLM describe un plan, que generalmente se ejecuta en un

inmediato. Además de la memoria interna, varios sistemas emplean memoria externa

secuencia: Planificar → Selección de herramienta → Ejecutar → Inspeccionar →

para guardar el historial de respuestas. Reflex­ion [185] mantiene una memoria

Generar, para responder a la consulta del usuario. Aquí, la base de datos de

episódica para utilizar respuestas anteriores como retroalimentación para mejorar la

herramientas es rica en modalidades, incluyendo texto, imágenes, etc. Muchas de

toma de decisiones futuras. Retro­former [224] mejora sus respuestas empleando corto

Los sistemas de aumento de herramientas multimodales emplean multimodal.

plazo

LLM [31, 215, 214, 206], mientras que otros utilizan una modalidad única

y la memoria a largo plazo, donde la memoria a corto plazo contiene respuestas

LLM y generar un plan sobre el uso de herramientas de diferentes modalidades para

recientes y la memoria a largo plazo mantiene resumidas las respuestas fallidas.

resolver consultas multimodales [216].

intenta agregar el mensaje como reflexión.
Sistemas de agentes múltiples: los LLM pueden desempeñar roles definidos por el usuario y

3.5. Agentes impulsados por LLM

comportarse como un experto en un dominio específico. En sistemas multiagente,

Los agentes de IA son entidades autónomas, capaces de planificar,

A cada LLM se le asigna un rol único, simulando el comportamiento humano y

toma de decisiones y realización de acciones para lograr objetivos complejos.

colaborando con otros agentes para completar un complejo

objetivos. En sus inicios, los agentes de IA se basaban en reglas, estaban diseñados

tarea [219, 229].

para tareas limitadas y tenían capacidades limitadas, como

LLM en entorno físico: los LLM son buenos en

como Clippy [217] y Deep Blue [218]. En contraste con esto,

seguir instrucciones, sin embargo, utilizarlas para fines físicos

Las capacidades de LLM para responder a escenarios dinámicos lo han hecho

Las tareas fundamentadas requieren adaptación, ya que carecen del mundo real.

posible incorporarlos en diversas aplicaciones, incluidos agentes impulsados por LLM

conocimiento. Esto podría llevar a generar respuestas ilógicas.

[214, 206], donde los LLM se comportan

para una situación física particular [230, 26]. Decir Can [230]
19

Machine Translated by Google

informar a los LLM sobre las operaciones de tareas de bajo nivel disponibles.

parámetros con las incorporaciones del modelo [237, 40, 241]. Los avisos discretos

LLM (Say) construye un plan de alto nivel para completar la tarea y

fijos específicos de la tarea se concatenan con incrustaciones de entrada en [40].

una función de capacidad aprendida (Can) explora la posibilidad de

Como las indicaciones discretas traen inestabilidad, las indicaciones

ejecutar el plan en el mundo real. SayCan usa RL para entrenar

están codificados a través de un mapeo que se puede aprender en P­Tuning [237],

la función de prestación condicionada por el lenguaje. PaLM­E permite

nombrar indicaciones continuas, a las que se añaden indicaciones discretas. Sólo

el LLM para resolver tareas fundamentadas mediante la capacitación de LLM multimodal

el codificador de avisos se puede entrenar en el

alimentando entradas directamente desde los sensores.

modelo. En una extensión de P­Tuning, se muestran indicaciones continuas.

Manipulación: en el área de manipulación [226, 231], LLM

concatenado con cada capa de la red en [241]. Las indicaciones progresivas [242]

mejorar la destreza y adaptabilidad de un robot, sobresaliendo en tareas

evitan olvidos y transferencias catastróficos

como el reconocimiento de objetos, la captación y la colaboración. Analizan

conocimientos aprendidos previamente añadiendo secuencialmente conocimientos

información visual y espacial para determinar el enfoque más eficaz para interactuar

incrustaciones de indicaciones a las incrustaciones de tareas previamente congeladas.

con los objetos.

Ajuste de prefijos: un conjunto de vectores de prefijos específicos de tareas entrenables

Navegación: los LLM mejoran la capacidad de un robot para navegar en entornos

se añaden a las capas congeladas del transformador en el ajuste de prefijo [41].

complejos con precisión y adaptabilidad [232, 233,

Los vectores de prefijo son tokens virtuales atendidos por el

234, 235]. Generan caminos y trayectorias factibles para

fichas de contexto a la derecha. Además, el ajuste de prefijo adaptativo [243] aplica

robots, que tienen en cuenta detalles ambientales intrincados [236].

un mecanismo de activación para controlar la información.

Esta habilidad es valiosa en escenarios que requieren precisión y

del prefijo y los tokens reales.

Navegación dinámicamente adaptable en entornos como almacenes, transporte,

Ajuste de sesgo: ajuste fino solo de términos de sesgo en empresas pequeñas y medianas

instalaciones sanitarias y residencias.

Se ha encontrado que los datos de entrenamiento son efectivos en BitFit [244]. Este

3.6. LLM eficientes

datos de entrenamiento y rendimiento comparable con más entrenamiento
datos.

El método logra un rendimiento de ajuste completo para tareas con menos

Implementar LLM en producción es costoso. Reduciendo su
costos de funcionamiento y al mismo tiempo preservar el rendimiento es una opción atractiva

3.6.2. Cuantización

área de investigación. Esta sección resume los enfoques sugeridos para mejorar

Los LLM requieren mucha computación y memoria para realizar inferencias. La

la eficiencia de los LLM.

implementación de un modelo GPT­3 con parámetro 175B necesita al menos
al menos 5 GPU A100 de 80 GB y 350 GB de memoria para almacenar

3.6.1. Ajuste eficiente de parámetros

Formato FP16 [44]. Requisitos tan exigentes para el despliegue

Ajuste de LLM con decenas o cientos de miles de millones de parámetros,

Los LLM dificultan que las organizaciones más pequeñas los utilicen.

como GPT­3 (175B), BLOOM (176B), MT­NLG

La compresión de modelos es una solución eficaz, pero tiene un coste

(540B), etc., requiere mucho tiempo y cálculo.

de rendimiento degradado, especialmente a grandes escalas superiores a

Para evitar un ajuste fino completo del modelo, se prueban numerosas técnicas de

6B. Estos modelos exhiben valores atípicos de magnitud muy grande que no

ajuste fino eficiente en parámetros (PEFT) [40, 237, 41, 38, 39].

no existe en modelos más pequeños [245], lo que lo hace desafiante y requiere

para lograr un rendimiento aceptable de ajuste del modelo a un precio reducido

métodos especializados para cuantificar LLM [44, 246].

costos. En comparación con el ajuste fino completo [238], PEFT realiza

Cuantización posterior al entrenamiento: en este tipo de cuantificación se requiere

mejor en configuraciones de bajos recursos, logra un rendimiento comparable en

un entrenamiento mínimo o nulo, sin comprometer significativamente el rendimiento

escenarios de recursos medios y se desempeña peor que

del modelo. LLM­8­bit [245] utiliza multiplicación de matrices de precisión total para

ajuste completo bajo alta disponibilidad de recursos. Una visión general

pesos asociados con características atípicas y 8 bits para las características

En la Figura 14 se muestran diferentes enfoques PEFT .

restantes. Las salidas de multiplicación de menor precisión se convierten a FP­16

Ajuste del adaptador: agrega algunos parámetros entrenables dentro del

y se concatenan con otras. Los modelos cuantificados tienen homogéneos.

bloque transformador. La capa adaptadora es una secuencia de características.
reducción de escala, no linealidad y ampliación de escala [106]. Variantes de

incrustaciones de palabras, que pueden degradar su rendimiento. A

El ajuste del adaptador inyecta capas del adaptador secuencialmente [106] y en

Para solucionar este problema, la destilación de conocimientos a nivel de token se emplea en [45]

paralelo [38], mientras que la mezcla de adaptador (AdaMix) [239]

junto con factores de escala de cuantificación independientes para cada

Emplea múltiples módulos adaptadores en una sola capa. adamix

módulo debido a la diferente distribución del peso. La distribución de funciones es

enruta las instancias de entrada aleatoriamente a uno de los múltiples módulos de

asimétrica y aparece en diferentes canales; parte aislada

escala reducida y superior. Se promedia la mezcla de adaptadores.

La supresión [247] cambia y escala las distribuciones de activación por canal para

para realizar inferencias y evitar latencia adicional. La adaptación de bajo rango

una cuantificación efectiva. SmoothQuant [44] cuantifica activaciones y pesos al

(LoRA) [240] aprende matrices descompuestas de bajo rango para

formato INT8 suavizando

congelar los pesos originales. Los pesos aprendidos se fusionan con los

activaciones y migrar la dificultad de cuantificación hacia

Pesos originales para inferencia, evitando latencia.

pesos. Multiplica el inverso del factor de suavizado por

Ajuste de indicaciones: las indicaciones son una forma eficaz de adaptar una

pesos, lo que introduce algunos valores atípicos en los pesos, pero es

LLM previamente capacitado para la tarea posterior. Sin embargo, manuales

más fácil de cuantificar que las activaciones no suavizadas. OPTQ [246]

Las indicaciones traen incertidumbre en la predicción del modelo, donde un

utiliza el algoritmo de compresión cerebral óptima (OBC) [248] para

el cambio en una sola palabra reduce el rendimiento [237]. Inmediato

Cuantifique el modelo capa por capa y actualice los pesos para compensar el error

el ajuste alivia este problema al ajustar solo 0,001% ­3%

de cuantificación. Para mejorar la velocidad y el rendimiento, OPTQ actualiza los

parámetros adicionales [241]. Concatena un mensaje entrenable

pesos en orden arbitrario, emplea
20

Machine Translated by Google

Figura 14: Ilustración de paradigmas de ajuste fino eficientes en parámetros, donde x es la entrada y h es el estado oculto, figura cortesía [38]. Adaptador paralelo y LoRA encajan
la categoría de ajuste del adaptador.

actualizaciones lentas y utiliza mejores núcleos Cholesky. Consciente de valores atípicos

El modelo no requiere ajustes finos, lo que ahorra costos computacionales. Escasez

La cuantificación de peso (OWQ) [249] utiliza el algoritmo OPTQ para

por capas ponderada de valores atípicos (OWL) [257]

cuantización pero asigna mayor precisión a pesos vulnerables,

extiende Wanda con poda en capas no uniformes. muestra que

provocando valores atípicos y menor precisión para otros.

el número de valores atípicos varía según las diferentes capas; por lo tanto, el

Entrenamiento consciente de la cuantificación: para compensar la degradación del

El modelo debe tener proporciones de poda variables para un mejor rendimiento en

rendimiento, se afina un modelo cuantificado en

cada capa. La poda contrastiva (CAP) [43] poda iterativamente el modelo entrenando

entrenamiento consciente de la cuantificación (QAT) [250, 251, 252]. Al­pha Tuning

el modelo disperso utilizando pérdida contrastiva entre instantáneas preentrenadas,

cuantifica el modelo utilizando cuantificación de codificación binaria (BCQ) [253] y

ajustadas y de instantáneas de

afina solo los factores de escala de cuantificación. Este enfoque mejora el

modelos dispersos anteriores para aprender tareas específicas e independientes de las tareas

rendimiento sobre

conocimiento.

ajuste eficiente de parámetros del modelo previamente entrenado. Del mismo modo,

Poda estructurada: Aquí, los parámetros se eliminan en

adaptación eficiente en parámetros y consciente de la cuantificación.

grupos, filas, columnas o matrices, lo que acelera la

(PEQA) [254] reduce la precisión de las capas completamente conectadas

inferencia debido a la utilización efectiva del núcleo del tensor de hardware

y afina sólo los parámetros de escala de cuantificación. LLM­QAT [252] genera

[255]. LLM­Pruner [42] emplea un sistema estructurado de 3 etapas.

datos de entrenamiento a partir de la red previamente entrenada

estrategia de poda, identificando los grupos de estados ocultos que provocan que

y entrena un modelo de estudiante cuantificado con destilación de conocimientos.

se activen entre sí durante el avance, manteniendo los grupos importantes y

QLoRA [251] afina el LLM preentrenado cuantificado de 4 bits

eliminando los menos importantes, y ajustando el modelo podado con LoRA.

con LoRA [240] usando un flotante normal de 4 bits, que muestra mejor

Máscara inducida por escasez
aprendizaje (SIMPLE) [258] poda la red usando aprendizaje

rendimiento sobre un entero de 4 bits y flotante.

máscaras. De manera similar, otro método elimina los LLM aprendiendo
máscaras y eliminar componentes de rango 1 sin importancia del

3.6.3. Poda

matriz de peso factorizada [256].

La poda es un enfoque alternativo a la cuantificación para comprimir el tamaño
del modelo, reduciendo así los costos de implementación de LLM

3.7. LLM multimodales

de modo significativo. En comparación con la poda independiente de tareas, la poda específica de tareas

Inspirados por el éxito de los LLM en aplicaciones de procesamiento del lenguaje

La poda se puede lograr fácilmente con un buen rendimiento, donde un
El modelo se ajusta en la tarea posterior y se poda para

natural, se está realizando un número cada vez mayor de trabajos de investigación.

inferencia más rápida. Es posible podar los LLM para individuos

ahora facilitando a los LLM percibir diferentes modalidades de información como

tareas, pero el costo de podar e implementar modelos específicos de tareas es alto.

imagen [259, 260, 261], video [262, 263, 264], audio [265, 264, 266], etc. LLM

Para superar esto, muchas organizaciones estructuradas y no estructuradas

multimodales (MLLM) presentes

Se han propuesto métodos de poda para LLM para mantener un rendimiento

beneficios sustanciales en comparación con los LLM estándar que procesan

razonable en todas las tareas y al mismo tiempo reducir el modelo.

solo texto. Al incorporar información de diversas modalidades, los MLLM pueden

tamaño [255, 42, 256].

lograr una comprensión más profunda del contexto,

Poda no estructurada: Este tipo de poda elimina pesos menos importantes sin

lo que lleva a respuestas más inteligentes infundidas con una variedad de

mantener ninguna estructura. Existente

expresiones. Es importante destacar que los MLLM se alinean estrechamente con los humanos.

Los métodos de poda de LLM aprovechan las características únicas de los LLM,

experiencias perceptuales, aprovechando la naturaleza sinérgica de nuestra

poco comunes en modelos más pequeños, donde un

entradas multisensoriales para formar una comprensión integral de

Un pequeño subconjunto de estados ocultos se activa con gran magnitud [245].

el mundo [266, 26]. Junto con una interfaz fácil de usar,

Poda por pesos y activaciones (Wanda) [255]

Los MLLM pueden ofrecer interacciones intuitivas, flexibles y adaptables,

poda los pesos en cada fila según la importancia, calculado

permitiendo a los usuarios interactuar con asistentes inteligentes a través de un

multiplicando los pesos por la norma de entrada. la podada

espectro de métodos de entrada. Según las formas de construcción
21

Machine Translated by Google

Según los modelos, los MLLM actuales generalmente se pueden dividir en tres

sesgo visual específico para generar una cadena de razonamiento implícitamente. En

secuencias: preentrenamiento, ajuste e indicaciones. En esta sección, discutiremos

Además de los problemas de CoT, a los LLM también se les puede solicitar

más detalles de estas corrientes principales, así como

descripciones y herramientas multimodales, dividiendo efectivamente complejos

como la importante aplicación de MLLM en el razonamiento visual.

tareas en subtareas [279, 280].

Capacitación previa: esta corriente de MLLM tiene como objetivo admitir diferentes

Aplicación de razonamiento visual: los sistemas de razonamiento visual recientes

modalidades utilizando modelos unificados de extremo a extremo. Por ejemplo,

[281, 282, 206, 283] tienden a aplicar LLM para una mejor visualización.

Flamingo [259] aplica atención cruzada cerrada para fusionar la visión y

análisis de información e integración del lenguaje visual. A diferencia de trabajos

modalidades lingüísticas, que se recogen de personas previamente formadas y

anteriores [284, 285] que se basan en VQA limitado

codificador visual congelado y LLM, respectivamente. Además, BLIP­2 [260] propone

conjuntos de datos y redes neuronales a pequeña escala, actuales con ayuda de LLM

una estrategia de dos etapas para entrenar previamente un Querying

Los métodos ofrecen beneficios de mayor capacidad de generalización, capacidad

Transformador (Q­Former) para la alineación entre visión y

emergente e interactividad [58]. Para realizar el razonamiento visual.

modalidades de lenguaje: en la primera etapa, el aprendizaje de la representación

con la ayuda de LLM, técnicas de orientación y ajuste

visión­lenguaje se inicia desde un codificador visual congelado;

También se puede utilizar: por ejemplo, se aplica PointClip V2 [282].

y en la segunda etapa, un LLM congelado inicia el aprendizaje generativo de visión a

LLM para generar indicaciones específicas en 3D, que están codificadas como

lenguaje para la generación de imagen a texto de disparo cero. De manera similar,

características textuales y luego se combinan con características visuales para

MiniGPT­4 [267] implementa dispositivos previamente entrenados y

reconocimiento 3D; y GPT4Tools [31] emplea LoRA [240] para

ViT congelado [268], Q­Former y Vicuña LLM [149], solo entrenan la capa de

ajustar los LLM siguiendo las instrucciones relacionadas con las herramientas. Servicio
como controlador [283], tomador de decisiones [286] o refinador semántico [281, 287],

proyección lineal para la alineación de las modalidades de visión y lenguaje.

los LLM facilitan significativamente el progreso de
Ajuste fino: derivado del ajuste de instrucciones [16] para PNL

investigación del razonamiento visual.

tareas [20, 16, 97], los investigadores están afinando LLM previamente capacitados
utilizando instrucciones multimodales. Siguiendo este método, los LLM

3.8. Resumen y discusión

se pueden ampliar fácil y eficazmente como chatbots multimodales [267, 261, 29] y

3.8.1. Arquitectura

solucionadores de tareas multimodales [269, 30, 270].
La cuestión clave de este flujo de MLLM es recopilar datos multimodales de

Debido a la gigantesca escala de los LLM, los cambios menores en la arquitectura

seguimiento de instrucciones para realizar ajustes [58]. Para abordar este problema,

y las estrategias de capacitación tienen un gran impacto en el desempeño.

las soluciones de adaptación de referencia [269,

y estabilidad. Aquí, resumimos los módulos arquitectónicos clave.

271, 272], autoinstrucción [19, 31, 273] y composición híbrida [274, 270] ,

utilizado en varios LLM, lo que lleva a un mejor rendimiento, reducción

respectivamente. Para mitigar la brecha

tiempo de entrenamiento y memoria, y una mejor estabilidad del entrenamiento.

Entre la modalidad del lenguaje original y las modalidades adicionales, se introduce

Normalización de capas: el rendimiento y la estabilidad del entrenamiento.

la interfaz de aprendizaje para conectar diferentes modalidades de modelos

de los LLM se ven afectados significativamente por la normalización de capas. La

congelados previamente entrenados. Particularmente,

prenorma, es decir, normalizar los insumos en lugar de los productos, es más

Se espera que la interfaz que se puede aprender funcione con un ajuste eficiente en

común entre los LLM que estabilizan la formación [6, 127, 108].

los parámetros: por ejemplo, se aplica LLaMA­Adapter [275].

BLOOM [13] y AlexaTM [122] utilizan una capa adicional

un módulo adaptador eficiente basado en transformador para capacitación,

Normalización antes de incrustar la capa para estabilizar el entrenamiento.

y LaVIN [274] aprende dinámicamente la característica multimodal

de modelos a gran escala, mientras que la capacidad de generalización de tiro cero

pesos usando un adaptador de mezcla de modalidades. Diferente de

del modelo puede verse afectada negativamente [13]. Sin embargo, otro

la interfaz de aprendizaje, los modelos expertos pueden convertir directamente

El estudio [33] encuentra que la prenorma degrada el rendimiento del modelo ajustado

multimodalidades en el lenguaje: por ejemplo, VideoChat­Text [262] incorpora Whisper

en comparación con la posnorma, y no hay estabilidad.

[276], un modelo experto en reconocimiento de voz,

beneficios de la prenorma más allá de la escala 100B. Por lo tanto, GLM­130B [33]

para generar los subtítulos de videos dados para la comprensión

utilizó una norma profunda que es una variante de la posnorma para

de los siguientes LLM.

mejor rendimiento de las tareas posteriores después del ajuste fino.

Indicaciones: A diferencia de la técnica de ajuste fino que

Codificación posicional: al igual que otros componentes básicos del modelo,

actualiza directamente los parámetros del modelo específicos de la tarea

La codificación posicional también afecta el rendimiento y el entrenamiento.

conjuntos de datos, la técnica de indicaciones proporciona cierto contexto, ejemplos

Estabilidad de los LLM. BLOOM [13] considera que ALiBi supera

o instrucciones al modelo, cumpliendo tareas especializadas

Codificaciones posicionales aprendidas y rotativas. Al contrario de esto,

sin cambiar los parámetros del modelo. Dado que la incitación puede

GLM­130B [33] identifica la codificación posicional rotativa como

reducir significativamente la necesidad de datos multimodales a gran escala,

Mejor que ALiBi. Entonces, no hay ninguna conclusión en la literatura.

esta técnica se utiliza ampliamente para construir MLLM. Particularmente,

sobre codificaciones posicionales todavía.

para resolver problemas multimodales de Cadena de Pensamiento (CoT) [103],

Atención Paralela: En este tipo de atención, feed­forward y

A los LLM se les pide que generen tanto el proceso de razonamiento como
la respuesta dadas entradas multimodales [277]. En este frente, en la práctica se

Las capas de atención son paralelas entre sí en lugar de secuenciales en
un bloque transformador. Se ha demostrado que reduce el tiempo de

explotan diferentes paradigmas de aprendizaje: por ejemplo,

formación en un 15%. No hay evidencia de caída del rendimiento.

Multimodal­CoT [277] implica dos etapas de generación de fundamentos e inferencia

debido a este cambio en la literatura y es utilizado por los modelos

de respuestas, donde la entrada de la segunda etapa

PaLM [15], GPT­NeoX [118] y CodeGen [130].

es una combinación de la entrada original y la salida de la primera

Atención multiconsulta Tiene atención de claves y valores compartidos.

escenario; y CoT­PT [278] aplica tanto la sintonización rápida como la especificación.

cabezas en un bloque transformador mientras que las cabezas de atención de consultas están

22

Machine Translated by Google

proyectado como de costumbre. Esto reduce el uso de memoria y acelera

MT­NLG [117] encontró una mayor variación para la inicialización del peso

muestreo en decodificación autorregresiva. No se ha observado ninguna degradación

conduce a un entrenamiento inestable, por lo que se valida una inicialización pequeña

del rendimiento con este cambio y hace que la capacitación sea eficiente permitiendo

esquema [288]. Varios modelos realizan una inicialización de peso aleatoria, lo que

lotes de mayor tamaño. Atención multiconsulta

puede provocar una mala inicialización. Galactica [138] sugiere un calentamiento más

se utiliza en [15, 132].

prolongado para anular el efecto.

Mezcla de expertos: este tipo de arquitectura permite escalar fácilmente modelos a

Tasa de aprendizaje: una tasa de aprendizaje adecuada es importante para un

billones de parámetros [92, 91]. Sólo un

entrenamiento estable. Se sugiere utilizar un valor más bajo [13, 15, 124]

Pocos expertos se activan durante el cálculo, lo que los hace

con calentamiento y caída (coseno o lineal). Por lo general, el aprendizaje

eficiente en computación. El rendimiento de los modelos MoE es mejor.

La tasa de ing está dentro del rango 1e.

que los modelos densos para la misma cantidad de datos y requiere menos

(530B) [117] y GPT­NeoX (20B) [118] sugieren interpolar tasas de aprendizaje basadas

−4 a 8e

−4

. Además, MT­NLG

cálculo durante el ajuste fino para lograr un rendimiento similar

en el tamaño del modelo usando GPT­3 [6]

a modelos densos como se analiza en [91]. Las arquitecturas MoE son

Modelos que oscilan entre 13B y 175B. Esto evita sintonizar el

menos propensos a olvidos catastróficos, por lo que son más adecuados

Hiperparámetro de tasa de aprendizaje.

para el aprendizaje continuo [92]. Extracción de submodelos más pequeños para

Paralelismo de entrenamiento: paralelismo 3D, una combinación de datos,

las tareas posteriores son posibles sin perder rendimiento,

pipeline y paralelismo tensorial es el método de entrenamiento más utilizado.

hacer que la arquitectura MoE sea compatible con el hardware [92].

enfoque de paralelismo en LLM [33, 15, 14, 13, 117, 115, 112].

Activación dispersa versus densa: GPT­3 [6] usa transformadores dispersos [67]

Además del paralelismo 3D, BLOOM [13] utiliza un optimizador cero [37] para

mientras que GLaM [91] y PanGu­ [92] usan MoE [121]

fragmentar los estados del optimizador. PanGu­α [108] y

arquitecturas para reducir los costos computacionales y aumentar la

PanGu­Σ [92] va más allá del paralelismo 3D y aplica el paralelismo 5D que además

Tamaño y capacidad del modelo. Según la literatura, escasa

contiene paralelismo optimizador y

Los módulos no degradan el rendimiento del modelo [67]. Sin embargo, se requieren

rematerialización.

más experimentos para verificar esta afirmación.

Cambio de modo: agrega tokens relacionados con tareas al principio

3.8.2. Estrategias de entrenamiento

Tareas de comprensión del lenguaje y generación de lenguaje natural.

del texto durante el entrenamiento. Estas fichas se refieren a lo natural.
Los modelos de capacitación a gran escala requieren trucos para reducir los costos

que se ha demostrado que mejoran el rendimiento de las tareas posteriores

de capacitación, evitar la divergencia de pérdidas y lograr un mejor rendimiento.

en [125, 124, 122]. Durante el ajuste fino y la inferencia, los tokens

Resumimos y discutimos algunos de estos trucos clave.

se añaden en función de las tareas posteriores.

utilizado en diferentes LLM.

Generación de texto controlable: generar texto creíble y controlado a partir de un

Precisión mixta: es un método famoso para que los LLM reduzcan

modelo previamente entrenado es un desafío. GPT­3 [6]

uso de la memoria y mejorar la eficiencia del entrenamiento. En precisión mixta, los

y otros LLM utilizan el aprendizaje en contexto para controlar los generados.

pases hacia adelante y hacia atrás se realizan en FP16

texto. Si bien el aprendizaje en contexto ayuda a controlar el texto generado, ERNIE

formato mientras que los estados del optimizador y los pesos maestros se mantienen

3.0 Titan [35] sugiere utilizar la pérdida por confrontación.

en formato FP32 [120]. Un inconveniente asociado con este cambio de formato es la

para clasificar el texto generado en función de su credibilidad y sugerencias suaves como

inestabilidad del entrenamiento debido a un rango de valores más pequeño.

género, tema, palabras clave, sentimiento y duración para un mejor control

lo que resulta en picos de pérdidas [33]. Una alternativa al FP16 es el BF16

en el texto generado.

que tiene un rango comparativamente mayor y realiza operaciones sensibles a la
precisión como acumulación de gradiente y softmax en

3.8.3. Modelos supervisados versus modelos generalizados

FP32 [13]. BF16 tiene mejor rendimiento y estabilidad en el entrenamiento

Aunque los modelos generalizados son capaces de realizar diversas tareas con

pero usa más memoria y es compatible con hardware específico,

buen desempeño, aún no han superado a los modelos entrenados en entornos

por ejemplo, GPU A100. Por lo tanto, su adopción en LLM es

supervisados. el supervisado

limitado.

Los modelos entrenados siguen siendo de última generación en diversas tareas de PNL.

Inestabilidad del entrenamiento: la divergencia de pérdidas o los picos son comunes

un gran margen como se muestra en [6, 15, 18].

Problema en los LLM que ocurre varias veces durante la capacitación. Este
ocurre en presencia de recorte de gradiente [15]. para mitigar

3.8.4. Disparo cero versus pocos disparos

este problema, muchos enfoques sugieren reiniciar el entrenamiento desde
un punto de control anterior [15, 33, 91], omitiendo 200­500 antes

Los LLM funcionan bien en entornos de cero y pocos intentos. Pero

lotes de datos en el punto de divergencia en [15] y reorganización

la diferencia de rendimiento entre disparo cero y pocos disparos es

lotes en [91]. La reducción del gradiente de la capa de incrustación demuestra

grande para modelos previamente entrenados [6, 15], nombrando a los LLM como

Estabiliza aún más el entrenamiento ya que su norma de gradiente es significativamente

metaaprendices [6]. Las evaluaciones de tiro cero de los LLM tienen un rendimiento

más grande que las otras capas [33]. Otra sugerencia para mejorar.

inferior a los métodos no supervisados en la traducción automática neuronal [6]. La

La estabilidad del entrenamiento para modelos más grandes es no utilizar sesgos en densos.

literatura muestra que el entrenamiento previo no es suficiente para un buen desempeño

y capas de normas como en [15].

de tiro cero [15, 16]. Para mejorar el rendimiento del disparo cero,

Inicialización de peso: juega un papel importante en la convergencia del modelo y la

La literatura sugiere utilizar un ajuste fino de la instrucción que mejore

estabilidad del entrenamiento. GPT­NeoX [118] se inicializa

el rendimiento de tiro cero significativamente y supera las líneas de base. También se

capas de avance antes de los residuos con como en [143] y

2

ha demostrado que el ajuste fino de la instrucción mejora

L √ re

otras capas con el esquema de inicialización pequeño [288]. Este

Generalización de tiro cero a tareas invisibles. Otro modelo, Flan­PaLM [16], desbloquea

evita que las activaciones crezcan exponencialmente al aumentar la profundidad.

el razonamiento de disparo cero con el entrenamiento CoT.
23

Machine Translated by Google

3.8.5. Codificador vs Decodificador vs Codificador­Decodificador

sugirió varios conjuntos de datos de preentrenamiento y ajuste para mejorar las
capacidades de los LLM. Resumimos estos esfuerzos en la Tabla 8. Si bien hay

Tradicionalmente, estas arquitecturas funcionan bien para diferentes
tareas, por ejemplo, solo codificador para tareas NLU, solo decodificador

numerosos conjuntos de datos de entrenamiento disponibles en la

para NLG y codificador­decodificador para modelado secuencia2secuencia. Los modelos

literatura, cubrimos los más utilizados en nuestro resumen.

solo con codificador son famosos por modelos más pequeños como

5.2. Conjuntos de datos y tareas de evaluación

como Bert [7], RoBERTa [289], etc., mientras que los LLM son
solo decodificador [6, 118, 13] o codificador­decodificador [10, 11, 122].

La evaluación de los LLM es importante para evaluar su competencia y limitaciones.

Si bien los modelos sólo con decodificador son buenos para tareas NLG, varios

Este proceso mide la capacidad del modelo para comprender, generar e interactuar con

LLM, PaLM [15], OPT [14], GPT­3 [6], BLOOM [13],

el lenguaje humano.

LLaMA [146], son modelos solo decodificadores con importantes mejoras de rendimiento

en un espectro de tareas. Evaluación de un modelo de lenguaje (LM)

tanto en tareas NLU como NLG. En contradicción con esto, T5 [10] y UL2 [125] identifican

se divide en dos categorías más amplias: 1) comprensión del lenguaje natural (NLU) y 2)

codificador­decodificador

generación del lenguaje natural (NLG).

Los modelos superan en rendimiento a los modelos que solo tienen decodificador. En otro estudio,

Se enfatiza que las tareas en NLU y NLG están categorizadas suavemente y a menudo

PaLM [15] encuentra un aumento en el tamaño de los modelos solo decodificadores

se usan indistintamente en la literatura.

puede reducir la brecha de rendimiento entre solo decodificador y

Comprensión del lenguaje natural: esta tarea mide la capacidad de comprensión del

Arquitecturas codificador­decodificador.

lenguaje de los LM. Abarca múltiples

Aunque las arquitecturas de sólo decodificador se han convertido en una tendencia para

tareas, incluido el análisis de sentimientos, clasificación de texto, natural

LLM, muchos enfoques propuestos recientemente [125, 122] utilizan

inferencia del lenguaje (NLI), respuesta a preguntas (QA), razonamiento de sentido

tokens de cambio de modo en texto con arquitecturas de codificador­decodificador para

común (CR), razonamiento matemático (MR), lectura

habilitar modos específicos de tareas. De manera similar, CodeT5+ [34]

comprensión (RC), etc.

utiliza una arquitectura codificador­decodificador con múltiples objetivos de entrenamiento

Generación de lenguaje natural: esta tarea evalúa el lenguaje

para diferentes tareas, activando el codificador, decodificador o

Capacidades de generación de LLM mediante la comprensión de las capacidades proporcionadas.

ambos según las tareas. Estas variaciones en la arquitectura

contexto de entrada. Incluye tareas como resúmenes, finalización de frases, traducción

y los objetivos de entrenamiento permiten que un modelo funcione bien en diferentes

automática (TA), generación de diálogos, etc.

entornos. Debido a esta configuración dinámica, el futuro

de los LLM se pueden atribuir a arquitecturas de codificador­decodificador.
Se proponen numerosos conjuntos de datos para cada tarea, evaluando
LLM contra diferentes características. Para proporcionar una visión general
4. Configuraciones del modelo

de conjuntos de datos de evaluación, analizamos brevemente algunos conjuntos de datos famosos

dentro de cada categoría y ofrecer una lista completa de conjuntos de datos

En esta sección proporcionamos diferentes estadísticas de modelos
en la Tabla 9. Además, mostramos una descripción detallada de los conjuntos de datos
previamente entrenados y ajustados por instrucciones. Esto incluye información como
de capacitación y las tareas de evaluación y puntos de referencia utilizados por varios

lugar de publicación, tipo de licencia, creadores de modelos, pasos capacitados,

LLM previamente capacitados en la Tabla 10 y LLM ajustados en la Tabla 11. También

paralelismo, etc. en la Tabla 3 y la Tabla 4. Detalles de arquitectura

comparamos los principales ­realizar LLM en varios

de LLM previamente capacitados están disponibles en la Tabla 5. Proporcionar estos

Tareas de PNL en la Tabla 12.

Los detalles para los modelos ajustados por instrucciones son innecesarios porque
afina modelos previamente entrenados para conjuntos de datos de instrucciones. Por eso,

5.2.1. Multitarea

Los detalles arquitectónicos son los mismos que las líneas de base. Además,
MMLU [297]: Un benchmark que mide el conocimiento

Las configuraciones de optimización para varios LLM están disponibles en la Tabla 6.

adquirido por los modelos durante el preentrenamiento y evalúa los modelos en

y Tabla 7. No incluimos detalles sobre precisión, calentamiento,

ajustes de disparo cero y de pocos disparos en 57 sujetos, probando ambos

y caída de peso en la Tabla 7. Estos detalles no son tan importantes

conocimiento del mundo y capacidad de resolución de problemas.

como otros para mencionar para modelos ajustados por instrucciones, y no son

SuperGLUE [2]: un sucesor más desafiante y diverso

proporcionada por los periódicos.

Según el punto de referencia GLUE [299] , SuperGLUE incluye una variedad
de tareas de comprensión del lenguaje, como responder preguntas,
inferencia del lenguaje natural y resolución de correferencia. Es

5. Conjuntos de datos y evaluación

diseñado para proporcionar una prueba rigurosa de comprensión del lenguaje
y requiere avances significativos en áreas como muestra eficiente,

Generar conjuntos de datos de capacitación y evaluación es costoso debido a la

transferencia, multitarea y aprendizaje no supervisado o autosupervisado.

demanda de datos a gran escala de los LLM. Por lo tanto, conjuntos de datos
Para la capacitación y la evaluación comparativa, estos modelos son temas clave.

BIG­bench [298]: El BIG­bench (Comportamiento de Inteligencia

importancia. Un resumen de los conjuntos de datos comúnmente utilizados por los LLM

Generative Models Benchmark) es un punto de referencia a gran escala diseñado para

se proporciona a continuación.

probar las capacidades de los LLM en una amplia gama de
Tareas que incluyen razonamiento, creatividad, ética y comprensión.

5.1. Conjuntos de datos de entrenamiento

de dominios específicos.
GLUE [299]: El punto de referencia de Evaluación de la comprensión del lenguaje

El desempeño de los LLM depende en gran medida de la formación.
la calidad, el tamaño y la diversidad de los datos. Preparar conjuntos de datos de entrenamiento

general (GLUE) es una colección de recursos para entrenar, evaluar y analizar la

lograr una alta calidad a gran escala es laborioso. Los investigadores tienen

comprensión del lenguaje natural.
24

Machine Translated by Google

Tabla 3: Resumen de LLM previamente capacitados (>10B). Solo se resumen los LLM analizados individualmente en las secciones anteriores. “Datos/Tokens” es el nombre del modelo.
datos previos al entrenamiento, que son la cantidad de tokens o el tamaño de los datos. “Limpieza de datos” indica si se realiza o no la limpieza de datos. Esto incluye heurísticas.
(Heur), deduplicación (Dedup), filtrado de calidad (QF) y filtrado de privacidad (PF), el “Costo” es el costo de capacitación calculado que se obtiene multiplicando las GPU/TPU.
tarifa por hora con la cantidad de GPU y el tiempo de entrenamiento. El costo real puede variar debido a muchas razones, como el uso de GPU internas o la obtención de una tarifa con descuento.
reentrenamiento, número de empleados que trabajan en el problema, etc. “Paralelismo de entrenamiento” indica entrenamiento distribuido utilizando paralelismo de datos (D), paralelismo tensorial
(T), paralelismo de canalización (P), paralelismo de modelo (M), paralelismo de optimizador (OP) y rematerialización (R), donde para la columna "Biblioteca", "DS" es una forma abreviada de
Velocidad profunda. En la columna "Uso comercial", asumimos que un modelo es para fines no comerciales si su licencia no está disponible.

Modelos

Publicación

Licencia

Evento

Tipo

T5 [10]

JMLR'20

Modelo

No. de

Propósito de los creadores

GPT­3 [6]

NeurIPS'20

Apache­2.0 Google General 11B OpenAI General 175B
­
Apache­2.0 Google General 13B

mT5 [11]

NAACL'21

Apache­2.0 Huawei General 200B Tsinghua General

PanGu­α [108]

arXiv'21

CPM­2 [12]

198B Codificación OpenAI 12B Baidu General 10B AI21
MIT
General 178B Naver General 82B
­
General 245B
­

Códice [131]

AI Open'21
arXiv'21

ERNIE 3.0 [110]

arXiv'21

Jurásico­1 [112]

White­Paper'21 Apache­2.0

[115] arXiv'21 arXiv'21

Apache­2.0
­

­
Titan [35] arXiv'21 Baidu General 260B GPT­NeoX­20B [118] BigScience'22 Apache­2.0 EleutherAI General

×

20B OPT [14]
MIT

RAIL­1.0 BigScience General 176B Galáctica [138]

arXiv'22

MetaGLaM [91]
Apache­2.0 Ciencia 120B
­
Google General 1.2T LaMDA [140]
­
Diálogo de Google 137B MT­NLG

arXiv'22
[117]

arXiv'22
Ciencia'22
arXiv'22
arXiv'22
arXiv'22

UL2 [125]

Apache­v2.0 Codificación de Google 41B Chinchilla [96]
­
Google General 70B PaLM [15]
­
Google General 540B Alexa™ [122]
Apache v2.0 Amazon General 20B U­PaLM [124]
­
Generalidades de Google 540B

ICLR'23

Apache­2.0 Google General 20B Apache­2.0 Múltiple

­

­

­

­

­

M

RemoRemo

GPU

­

­

D+M+PM

Megatrón+DS

Ascend 910

96

40G A100

­

­

80G A100

­

­

­
­

×

400B

×
×

16B Codificación OpenRAIL­M BigCode 15,5B

­

­

D+M

Lingvo

D+T+P

Megatrón+DS
jax+haiku

METRO

­

TPUv4

­

­

­

jax+haiku

TPU v4

­

­

D+M

JAX+T5X

128

A100 2880h 1,47 mil TPU v4 120 h

512
512

­

768
­

TPUv4

DS
­

METRO

­

JAX+T5X
­

METRO

METRO

­

­

D+M

2048

80G A100 504h 4,12 Mil

512

Ascender 910 2400h

Filtrado 110k 51.5B Dedup 250k
­
1T Dedup+QF+PF 500k

512
80GB

40G A100 1272h 1,97 Mil
­
A100

2T Filtrado mínimo ­ Ddedup+PF+QF

16

40G A100

512
­

80G A100 624h 1,28 Mil D+T+P 80G A100 1,7Mh
­

­

JAXEx

D+M

Clf+Heur+Dedup 329B 139k 569B Dedup
­
­
366B

­

RemoRemo

6144

­

LLaMA­2.0 Meta General 70B Google General
×

­

0,25 mil TPU v4 40G A100 1440 h 3,37
­
­
mil

650k 577B Heur+Dedup 350k 1.4T
×

jax+haiku

METRO

­

­

megatrón
­

D+T+P

920h 13.19 Mil D+M D+M+P+D*
­
­

1384h 4,96 mil
­

4480

205k 967B Hora+Dedup 1.4T QF+Dedup
­
255k 780B Hora 500k 1.1T
Filtrado 20k 2M 1T

­
menteespora
JAXEx
­

M Megatron+DS+PyTorch
D+T
megatrón
80G A100 2520h 3,87 Mil D+T+P 80GB A100 ­ TPU v4 TPU
Megatrón+DS
­
­
v3 80G A100 TPU v4
metaseq
­
­
GSPMD

384

­

321 h 1,32 millones
­
­

GPU
TPU v3

128

­

A100

2128
4096
­

1024

×

­

­
V100

×

Generación de códigos [130]

­

­

­
384

1024

×

General 130B GLM [33]
ICLR'23 Apache­2.0 Salesforce Coding 16B LLaMA [127] arXiv'23 Meta
­
General 65B PanGuΣ [92] arXiv'23 BloombergGPT [141] arXiv23 Xuan Yuan 2.0 [142] arXiv23 CodeT5+
­
Huawei General 1.085T
[34] arXiv'23 StarCoder [137] arXiv '23
­
LLaMA­2 [21] arXiv'23 PaLM­2 [123]
Bloomberg Finance 50B RAIL­1.0 Du
Xiaoman Finance 176B BSD­3 Codificación Salesforce
arXiv'23

­

­D+OP+P+O+R
­
D+M

×

×

ICLR'23

METRO

­

­

992

×

arXiv'22

­

­

×

×

Apache­v2.0 MS.+Nvidia General 530B AlphaCode [132]

­

1024

300B Clf+Dedup+PF 26k

106B Dedup 600k 600B 3M
­
2.81T Filtrado 270B

Meta General 175B FLORACIÓN [13]

arXiv'22
ICML'22

V100
­

×

180B Heur+Clf+Dedup 300B QF+Dedup
­
300B Heur+Dedup 150k
­
825GB Ninguno 150k 180B
Dedup 366B Dedup+PR 225k

TensorFlow de malla
­

Ascender 910
­

800

­

Biblioteca

D+M

­

­

­

Capacitación

Paralelismo

2048
­

120k
­

375B Hora+Dedup
300B

Tren. Costo
­

×

×

Tiempo

­

260k 1,1TB Hora+Desduplicación 1M
2,6TB Deduplicación 100B
­
hora

×

Capacitación Calculado

Tratamiento

TPUv3

1024
­

×

arXiv'22

No. de

Unidades de procesamiento Tipo de unidad

1M 1T Hora+Descarga
­
300B Deduplicación+QF
­
1 millón 1 tonelada

Google General 280B ERNIE 3.0

tuza [116]

Datos
Limpieza

Pasos

Usar

­

HyperCLOVA [114] EMNLP'21 Yuan 1.0

Datos/

Entrenado Fichas

Comercial

parámetros

xFormadores

­D+OP+P+O+R

­

­

menteespora
PyTorch
DS

METRO

­

­

PAG

­

­

DS
Megatrón­LM
­

­

­

­

­

Tabla 4: Resumen de LLM optimizados con instrucción (>10B). Todas las abreviaturas son las mismas que en la Tabla 3. Las entradas en “Datos/Tokens” que comienzan con “S­” representan el número
de muestras de entrenamiento.

Modelos

Publicación

Licencia

Evento

Tipo

Modelo

No. de

Comercial

Propósito de los creadores parámetros

­
WebGPT [156] arXiv'21 OpenAI General 175B T0 [17]
ICLR'22 Apache­2.0 BigScience General 11B Tk­Instruct [18] EMNLP'22
AI2+[16] ICLR'22 Apache­2.0
MIT General 11B OPT­IML [97] arXiv'22 General 175B Flan­U­PaLM
­
Meta
Google General 540B mT0 [144]

Pre­entrenado

Pasos

Tratamiento

Tren.

Tipo de unidad

Tiempo Tren. Costo

Calculado

Tren.

×

GPT­3

­

­

­

T5

­

250B
­

512

TPU v3 270 h 0,48 mil

­

256

TPUv3

­

T5 1000
×

2B
­

OPTAR 8k

­

­

­

­

ACL'23 Apache­2.0 HuggingFace+ General 13B Sparrow [157] arXiv'22
­
Google Dialog 70B WizardCoder [154] arXiv'23 Apache­2.0 HK Bapt.

×

Chinchilla

Codificación 15B

×

StarCoder 200 S­78k

General 65B Koala [290]

No. de

Modelos

U­PaLM 30k mT5

Alpaca [148]

Datos/

Entrenado Fichas

Usar

Github'23 Apache­2.0 Stanford General 13B Vicuña [149]

LLaMA 3­Época S­52k

Github'23 Apache­2.0 LMSYS General 13B LIMA [175] arXiv'23 Meta+
­

­

LLaMA 3­Época S­125k

Github'23 Apache­2.0 UC­Berkley General 13B

×

LLaMA 15­Época S­1000
LLaMA 2­Época S­472k

Unidades de procesamiento

­

­

­

4h 0,0036 mil
­

Biblioteca de paralelismo
­

­
­
Google T5

128

40G A100 ­

512
­

TPUv4
­

­

­

­

­

­

­

64
­

TPUv3
­

­

­

METRO

­

­

­

­

­

­

600
­

8
­

80G A100 3h
­

­

­

­

­

8

A100

6h

100

D+T Megatrón
­
JAX+T5X

FSDP PyTorch
FSDP PyTorch
­
­

­

JAX/LINO

sistemas. Incluye una variedad de tareas que prueban una amplia gama de

El contenido de siete dominios lo convierte en una prueba rigurosa de la capacidad de

fenómenos lingüísticos, lo que lo convierte en una herramienta integral para evaluar la

los modelos para manejar una amplia gama de temas y conversaciones.

comprensión del lenguaje en la IA.

contextos.

5.2.2. Comprensión del lenguaje

el significado de las palabras en función del contexto, lo que ayuda en tareas relacionadas.

WiC [307]: este conjunto de datos evalúa la capacidad de un modelo para discernir

a la desambiguación del sentido de las palabras.

WinoGrande [344]: un conjunto de datos a gran escala inspirado en el Schema

Wikitext103 [308]: Con más de 100 millones de tokens de

Challenge original de Winograd [347] prueba modelos en su
capacidad para resolver la ambigüedad de los pronombres y fomenta el desarrollo de

Artículos principales de Wikipedia, este conjunto de datos es un rico recurso para tareas

modelos que comprendan el contexto amplio en la naturaleza.

que requieren comprender dependencias a largo plazo, como el modelado del lenguaje

texto del idioma.

y la traducción.

CoQA [306]: un conjunto de datos conversacionales de respuesta a preguntas,

PG19 [309]: Esta es una biblioteca digital de libros diversos de

CoQA desafía los modelos con preguntas que se basan en el historial de conversaciones

Proyecto Gutenberg. Está diseñado específicamente para facilitar la investigación sobre

y requieren respuestas de texto en formato libre. Es diverso

el aprendizaje no supervisado y el modelado del lenguaje, con un
25

Machine Translated by Google

Tabla 5: Detalles de arquitectura de LLM. Aquí, "PE" es la incrustación posicional, "nL" es el número de capas, "nH" es el número de cabezas de atención, "HS" es el
Tamaño de los estados ocultos.

Modelos
T5 (11B)

Causal­dic

mT5 (13B)

Enc­Dic

Atención

Objetivo

Enc­Dic

GPT3 (175B)
PanGu­α (200B)

Capacitación

Tipo

Estándar

Corrupción en tramos
Siguiente ficha
Corrupción en tramos
Siguiente ficha

Causal­dic

Vocabulario

Tokenizador

Norma

PE Sesgo de activación nL nH HS

Denso+Escaso
Estándar

32k
­

FrasePieza
­

250k

FrasePieza

Estándar

40k

BPE

250k
­

FrasePieza

GeLU aprendido
Capa
ReLU relativa anterior a RMS
­
Capa
ReLU relativa anterior a RMS

BPE+

GeLU aprendido antes de la capa

12288

pieza de palabra

GeLU relativa posterior a la capa

­ 48 64 4096

CPM­2 (198B)

Enc­Dic
Causal­dic

Corrupción en tramos
Siguiente ficha

Estándar

Códice (12B)

Estándar

ERNIE 3.0 (10B)

Causal­dic

Siguiente token

Estándar

­

Jurásico­1 (178B)

Causal­dic

Siguiente ficha

Estándar

HiperCLOVA (82B)

Causal­dic

Siguiente ficha

256k
­

Yuan 1,0 (245 mil millones)

Causal­dic

Siguiente ficha

Denso+Escaso
Estándar

Topo (280B)

Causal­dic

Siguiente ficha

Estándar

ERNIE 3.0 Titán (260B)
GPT­NeoX­20B OPT

Causal­dic

Siguiente ficha

Estándar

Causal­dic

Siguiente ficha

Paralelo

(175B)

Causal­dic

Siguiente ficha

FLOR (176B)

Causal­dic

Galáctica (120B)
GLAM (1.2T)

­

ReLU relativa anterior a RMS

Pieza de oración

GeLU aprendido antes de la capa

BPE*
­

GeLU aprendido antes de la capa
­
­

× 24 128 1024
­
­

96 96 12288
­

76 96 13824
­ 64 80 10240
­

­ 76 ­ 16384

FrasePieza
BPE

Estándar

50k
­

BPE

Capa
­

GeLU rotatorio
­
ReLU

Siguiente ficha

Estándar

250k

BPE

Capa

ALiBi GeLU

Causal­dic

Siguiente ficha

Estándar

50k

BPE+personalizado

Capa

GeLU aprendido

MoE­diciembre

Siguiente ficha

Estándar

256k

FrasePieza

Capa

GeLU relativa

64 128 32768

LaMDA (137B)

Causal­dic

Siguiente ficha

Estándar

32k

BPE

Capa

GeGLU relativo

­ 64 128 8192

MT­NLG (530B)

Causal­dic

Siguiente ficha

Estándar

50k

BPE

Código Alfa (41B)

Enc­Dic

Siguiente ficha

Consulta múltiple

8k

FrasePieza

Chinchilla (70B)

Causal­dic

Siguiente ficha

Estándar

Palma (540B)

Causal­dic

FrasePieza
Siguiente token Paralelo+consulta múltiple 256k Capa Cuerda SwiGLU
Eliminación de ruido Estándar 150k Pre­Capa aprendida GeLU
Pref.&Regla RM 32k FrasePieza­NFKC Pre­RMS GeLU relativo FrasePieza
­

AlexaTM (20B)

Enc­Dic

Gorrión (70B)

Causal­dic

U­PaLM (540B)

Dec no causal

UL2 (20B)
GLM (130B)

Dec no causal
Causal­dic

Relleno en blanco AR
Siguiente ficha

Llama (65B)

Causal­dic

PanGu­Σ (1085B)
BloombergGPT (50B)
Xuan Yuan 2.0 (176B)
CódigoT5+ (16B)

FrasePieza
FrasePieza

Ministerio de Defensa

CódigoGen (16B)

Estándar 130k

FrasePieza

Paralelo

­

BPE

Siguiente ficha

Estándar

BPE

Causal­dic

Siguiente ficha

Estándar

32k
­

Causal­dic

Siguiente ficha

Estándar

131k

Causal­dic

Siguiente

Ser

250k
­

Unigrama
BPE

Enc­Dic

Codificador estrella (15,5 mil millones)

Causal­dic

Llama (70B)
PaLM­2

Causal­dic
­

Token SC+NT+Cont.+Partido

Estándar

Siguiente ficha

BPE

Consulta múltiple

49k

Código específico
BPE

consulta agrupada

32k
­

BPE
­

FIM

Ministerio de Defensa

Paralelo

80 128 16384

GeLU relativa posterior a la capa

­ 48 192 12288
96 96

­

70 112 14336

­

44 64

× 96 80 10240

105 128 20480
­

­ 64 128 6144

32k FrasePieza­NFKC Pre­RMS GeLU relativo

MoD paralelo+multiconsulta 256k
Estándar 32k

Enc­Dic

GeLU relativa pre­RMS

GeLU aprendido antes de la capa
­
­

­
­

­ 24 64 ­ 96 96

32k
­

pieza de palabra

­

­ 64 128 16384

80 64 8192

Capa
­

cuerda SwiGLU
­

× 118 48 18432
78 32 4096
16
­

cuerda GeGLU
Profundo
­
Soga
Capa
Cuerda SwiGLU pre­RMS
­
FastGeLU
Capa fusionada
ALiBi GeLU
Capa
Capa
­
­

64 8192

× 118 48 18432
­ 64 16 4096
70 96 12288
­ 34 24 80 64 8192
­

­

­ 40 40 5120
70 40 7680

ALiBi GeLU
­

­

­

70 112 14336
­
­

­

Aprendió

­

­ 40 48 6144
­
­

­

­

­

­

­

­

SwiGLUE de cuerda pre­RMS
­
­

­

ARC [332]: Una versión más grande del ARC­Challenge, esto

atención especial al contenido de formato largo.
C4 [10]: C4, un conjunto de datos limpio y multilingüe, ofrece miles de millones de

el conjunto de datos contiene niveles de escuela primaria fáciles y desafiantes,

tokens a partir de datos rastreados en la web. Es un recurso completo para

Preguntas científicas de opción múltiple. Es una prueba integral de

entrenar modelos avanzados de Transformer en varios idiomas.

la capacidad de un modelo para comprender y responder preguntas complejas.
ARC­Easy [332]: un subconjunto del conjunto de datos ARC, ARC­Easy, contiene

LCQMC [310]: La correspondencia de preguntas chinas a gran escala

preguntas que son respondidas correctamente por

Corpus (LCQMC) es un conjunto de datos para evaluar el desempeño
de modelos en tareas de correspondencia semántica. Contiene pares de preguntas

un algoritmo basado en recuperación o un algoritmo de co­ocurrencia de palabras.

en chino y su estado de correspondencia, lo que lo convierte en un valioso

Es un excelente punto de partida para los modelos que comienzan a explorar la

recurso para la investigación sobre la comprensión del idioma chino.

respuesta avanzada a preguntas.

5.2.3. Cierre de la historia y finalización de oraciones

conjunto de datos, ARC­Challenge incluye complejos, nivel de escuela primaria

ARC­Challenge [332]: Una pregunta­respuesta rigurosa

preguntas que exigen un razonamiento más allá de la simple recuperación, poniendo

StoryCloze [324]: Introduce una nueva “Prueba StoryCloze”, una

a prueba las verdaderas capacidades de comprensión de los modelos.

Marco de razonamiento de sentido común para evaluar la comprensión, generación y
aprendizaje de guiones de una historia. Se considera un modelo

5.2.5. Comprensión del lenguaje contextual

Capacidad para comprender y generar historias coherentes y sensatas.
LAMBADA [325]: Este conjunto de datos evalúa la comprensión del texto contextual

RACE [337]: El conjunto de datos RACE es una herramienta de comprensión de lectura.

a través de una tarea de predicción de palabras. Los modelos deben predecir la última

conjunto de datos recopilados de exámenes de inglés en China, que

palabra de un pasaje, lo cual es fácil para los humanos cuando

compara modelos de IA para comprender y responder preguntas en pasajes largos y

dado el pasaje completo, pero no cuando se le dio solo la última oración.

complejos, simulando el desafío
de un examen del mundo real.
RACE­Middle [337]: Otro subconjunto de RACE [337]

5.2.4. Conocimiento físico y comprensión del mundo.

El conjunto de datos, RACE­Middle, contiene inglés de nivel de escuela secundaria.

PIQA [330]: Un conjunto de datos que prueba el conocimiento físico de

preguntas del examen. Ofrece una evaluación ligeramente menos desafiante pero
orientada académicamente de las habilidades de comprensión de un modelo.

modelos, con el objetivo de comprender qué tan bien están aprendiendo sobre
el mundo real.

RACE­High [337]: un subconjunto del conjunto de datos RACE [337] ,
RACE­High consiste en preguntas de exámenes de inglés de nivel de escuela

TriviaQA [331]: un conjunto de datos que prueba modelos sobre comprensión lectora
y tareas de respuesta a preguntas (QA) de dominio abierto.

secundaria. Está diseñado para evaluar la capacidad de comprensión de

con un enfoque en el control de calidad al estilo de recuperación de información (IR).

modelos en un contexto más académico y desafiante.
26

Machine Translated by Google

Tabla 6: Resumen de las configuraciones de optimización utilizadas para LLM previamente capacitados. Los valores de disminución de peso, recorte de gradiente y abandono son 0,1, 1,0 y 0,1, respectivamente.
para la mayoría de los LLM.

LR

Secuencia
Modelos

Tamaño del lote

11

T5 (11B)

2

GPT3 (175B)

32K

mT5 (13B)

1024
­

PanGu­α (200B)
CPM­2 (198B)

Longitud LR Calentamiento

1024

raíz cuadrada inversa

raíz

­

­

­

­

­

1024 0,01 1024

­

cuadrada inversa del coseno

­

­

­

­

­

2e­5 1024 0,001

­

­

­

­

6e­5 512 1e­4

­

­

­

­

­

­

­

­

­

­

­

­

­

­

coseno

­

­

­

­

­

coseno decae al 10%

­

­

­

­

­

0,01

­

6e­5

­

­
­

6144

6e­5 6e­5

lineal

Jurásico­1 (178B)

3,2M

2048 1,6e­4

coseno

HiperCLOVA (82B)

1024

­

Yuan 1,0 (245 mil millones)

<10M

4e­5 512

­

2048

3M
­

1e­4

2048 1,2e­4 lineal 2048 6e­5 coseno 2048 7e­6

OPT (175B)

2M

1024 0,01

FLOR (176B)

2048

Galáctica (120B)
GLAM (1.2T)
LaMDA (137B)

­

×

2M

­

decaimiento lineal al
­

1M
­

256K

­

­

­

decaimiento del coseno al 10%

2048 2048 0,01 2M 32 1024

decaimiento del coseno al 10%
­

Palma (540B)
AlexaTM (20B)

1024 1e­4 2048

­

U­PaLM (540B)

1e­4 1024 2048

­

UL2 (20B)

8e­5
4224

­

2M

­

­

coseno

BloombergGPT (50B)
Xuan Yuan 2.0 (176B)

fichas

2e­4 8k 3e­4 4k

512

1.5e­4

CódigoT5+ (16B)

2048

­
­

×

­

­

­
­

­

­

­

­

­

­

­

­

­

­

­

­

­

­

×

­

­

­

­

­

­

­

­

×
­

­

­

decaimiento del coseno al

2048 6e­5 1024

­

­

coseno

2e­5 2048 6e­5

­

­

raíz cuadrada inversa ­
coseno

1.5e­4 1024

PanGu­Σ (1.085T)

­

decaimiento lineal al
5% del coseno

2048 5e­5 2048

4 millones de

­

de raíz cuadrada inversa

­

­

FP32 +

10% de raíz cuadrada inversa

­

1536+768 1e­4 1,5M 2048 1e­4

Llama­2 (70B)

­

­

Código Alfa (41B)
Chinchilla (70B)

Codificador estrella (15,5 mil millones)

­

×

decaimiento del coseno al 10%

CódigoGen (16B)

­

­

2048 5e­5 1920 2048

Llama (65B)

­

­

MT­NLG (530B)

GLM (130B)

­

coseno decae al 10%
lineal
2048 0,97e­5
coseno

1538

GPT­NeoX­20B

­

coseno

2048

ERNIE 3.0 (12B)

Topo (280B)
ERNIE 3.0 Titán (260B)

Graduado de peso

×

512

­

Códice (12B)

Precisión

Optimizadores

AdaFactorAdam AdamWFP16 BF16 Abandono del clip de decaimiento mixto

Decadencia

­
­
­

­

­

×
­

10%

coseno

­

lineal

­

­

coseno

­

­

2048 2048 512

­

coseno

4 millones de fichas

Tabla 7: Resumen de las configuraciones de optimización utilizadas para los LLM ajustados por instrucciones. Los valores para el recorte y la eliminación del gradiente son los mismos que los de los modelos previamente entrenados, mientras que
ningún modelo utiliza la caída de peso para ajustar las instrucciones.

Secuencia
Modelos

Tamaño del lote

Optimizadores

LR_Decay

Longitud LR Calentamiento

Graduado

AdaFactor Adam AdamW Abandono del clip

antes de Cristo: 512,

­ 6e­5

­

­

­

T0 (11B)

RM:

1280 1e­3 ­

­

­

­

Tk­Instrucción (11B)

32

lineal

WebGPT (175B)

OPT­IML (175B)

1024

Flan­U­PaLM (540B)

1e­5

­

2048 5e­5 ­

×

1e­3 ­

­

1024 128 32

­

­

­

­

constante

2e­6

×

2048 2e­5

10%

­

­

Alpaca (13B)
Vicuña (13B)

128

512 1e­5

coseno

­

­

128

­2048 2e­5

LIMA (65B)

32

2048 1e­5

AsistenteCodificador (15B)

­

constante

Salón: 8+16, RL:16
512

Gorrión (70B)

­

­

desintegración del coseno al

×

QuAC [338]: Este conjunto de datos simula una búsqueda de información

­

­

­

×

coseno

­

coseno lineal

­

×

razonamiento causal de sentido común de dominio abierto. cada pregunta
comprende una premisa y dos alternativas, y el modelo debe

Diálogo entre estudiantes y profesores usando Wikipedia oculta.
texto. Introduce desafíos únicos que no se encuentran en los conjuntos de datos de

seleccionar la alternativa más plausible, probando la capacidad de un modelo para

comprensión de máquinas, lo que lo convierte en un recurso valioso para el avance

comprender y razonar sobre causa y efecto.

de los sistemas de diálogo.
WSC [347]: El Desafío del Esquema de Winograd (WSC) es un
Tarea de comprensión lectora en la que un sistema debe resolver.

5.2.6. Razonamiento de sentido común

referencias en un texto, que a menudo requieren conocimiento mundial y

HellaSwag [345]: un conjunto de datos que desafía a los modelos a elegir el

razonamiento sobre el texto.

El mejor final para un contexto utiliza el filtrado adversario para crear un
Zona de complejidad 'Ricitos de Oro', donde el texto generado es absurdo

CSQA [348]: CommonsenseQA es un sistema de preguntas y respuestas
conjunto de datos que requiere conocimiento de sentido común para evaluar la

a los humanos, pero a menudo clasificados erróneamente según los modelos.

COPA [391]: Este conjunto de datos evalúa el progreso de un modelo en

Capacidad de los modelos de IA para comprender y responder preguntas.
27

Machine Translated by Google

Tabla 8: Detalles de varios conjuntos de datos de ajuste y preentrenamiento conocidos. Aquí, alineación significa alinearse con las preferencias humanas.
Tipo

Conjunto de datos

Fuente

Tareas de tamaño/muestras

Comentarios de creación

C4 [10]

Preentrenamiento

806GB

­

rastreo común

Automatizado Un conjunto de datos limpio y multilingüe con miles de millones

mC4 [11]

Preentrenamiento

38,49 TB

­

rastreo común

Automatizado Una extensión multilingüe del C4

de fichas

conjunto de datos, mC4 identifica más de 100
idiomas usando cld3 de 71 sitios web mensuales
raspaduras de rastreo común.
Rastreo común, PubMed Central,
PILA [291]

Preentrenamiento

825GB

­

OpenWebText2, ArXiv, GitHub,
Libros3 y otros

RAÍCES [292]

Preentrenamiento

1,61 TB

­

Texto masivo [116]

Preentrenamiento

10,5 TB

­

Wikipedia [293]

Preentrenamiento

­

­

Pijama rojo [294]

Preentrenamiento

5TB

­

CommonCrawl, C4, Wikipedia,

PushShift.io Reddit

Preentrenamiento

21,1GB

­

Github, Libros, StackExchange
Reddit

Gran Python [130]

Preentrenamiento

5,5 TB

Codificación

GitHub

12M

62

Fuente de aviso

498 conjuntos de datos de cara abrazada

Automatizado Un conjunto de datos masivo compuesto por 22
subconjuntos de datos constituyentes.

46 lenguajes naturales y 13 de programación automatizados.
calibres

MassiveWeb, Libros, Noticias,
Wikipedia, Github, C4
Wikipedia

Automatizado El 99% de los datos están en inglés.
Volcado automatizado de wikipedia
Réplica automatizada de código abierto del conjunto de datos LLaMA
Envíos y comentarios automatizados en Reddit
de 2005 a 2019

Grupo de avisos (P3) [17]

Instrucciones

Automatizado ­
Manual Un subconjunto de PromptSource, creado a partir de
177 conjuntos de datos que incluyen resúmenes,
Control de calidad, clasificación, etc.

xP3 [144]

Instrucciones

Instrucciones sobrenaturales (SNI) [18] Instrucciones

81 millones

71

P3+Conjuntos de datos multilingües

12,4 millones

1616

Múltiples conjuntos de datos

Manual Ampliación de P3 a un total de 46 idiomas
Manual Ampliación de P3 con conjuntos de datos multilingües
adicionales, un total de 46 idiomas

Flan [16]

Instrucciones

15M

1836

OPT­IML [97]

Instrucciones

18,1M

1667

Autoinstrucción [19]

Instrucciones

82k

175

­

alpacas [148]

Instrucciones

52k

­

­

Vicuña [149]

Instrucciones

125k

­

CompartirGPT

LLaMA­GPT­4 [150]

Instrucciones

52k

­

Instrucciones antinaturales [295]

Instrucciones

68k

­

LIMA [175]

Instrucciones

1k

­

142k

­

­

Manual

39k

­

­

Manual

Muffin+T0­SF+NVI2
­

Manual Total 60 idiomas
­
Manual
52.000 instrucciones generadas de forma automatizada con 82.000
muestras de 175 tareas iniciales utilizando GPT­3
Método automatizado de autoinstrucción empleado para generar datos
a partir de text­davinci­003
Conversaciones automatizadas compartidas por usuarios en
ShareGPT usando API públicas

Alpaca

Conjunto de datos de Alpaca recreado automatizado con GPT­4 en
inglés y chino

15 Semillas (SNI)
Múltiples conjuntos de datos

Automatizado ­
Manual Muestras cuidadosamente creadas para probar el rendimiento
con ajustes finos en menos datos

Antrópico­HH­RLHF [296]
Antrópico­HH­RLHF­2 [168]

Alineación
Alineación

5.2.7. Comprensión lectora

implicación, predecir si una oración dada se sigue lógicamente de otra y evaluar
la comprensión de un modelo de

BoolQ [353]: un conjunto de datos derivado de consultas de búsqueda de Google,

Relaciones lógicas en un texto.

BoolQ desafía los modelos para responder preguntas binarias (sí/no).
Las preguntas surgen de forma natural y van acompañadas de una

WebQA [357]: un conjunto de datos para responder preguntas de dominio abierto,

párrafo de un artículo de Wikipedia que contiene la respuesta. Él

WebQA ofrece una gran colección de preguntas y respuestas basadas en la web.

Es una prueba de comprensión lectora y razonamiento.

pares. Está diseñado para evaluar la capacidad de los modelos de IA para
comprender y responder preguntas basadas en contenido web.

SQUADv2 [354]: Conjunto de datos de respuesta a preguntas de Stanford

CMRC2018 [359]: Este conjunto de datos es una prueba del idioma chino.

(SQuAD) [352] es una colección de preguntas planteadas por la multitud

capacidad de los modelos para razonar de manera integral y está diseñado con

trabajadores en un conjunto de artículos de Wikipedia, donde la respuesta a cada
pregunta es un segmento de texto de la lectura correspondiente

un formato desafiante de extracción de tramos que traspasa los límites

paso. SQuADv2 combina el conjunto de datos SQuAD1.1 original

del rendimiento de la máquina.

con más de 50.000 preguntas sin respuesta. El objetivo es evaluar la capacidad
de un modelo para comprender y responder preguntas basadas

5.2.8. Razonamiento matemático

en un contexto dado y para determinar cuándo una pregunta no tiene
respuesta.

Capacidades de resolución de problemas matemáticos de los modelos de IA.

MATEMÁTICAS [372]: Este conjunto de datos es una plataforma para evaluar la

DROP [355]: DROP, o razonamiento discreto sobre el contenido de los párrafos,

Contiene un conjunto diverso de problemas matemáticos, que van desde aritmética

está diseñado para probar la capacidad de un modelo para comprender una

al cálculo, y está diseñado para probar la capacidad del modelo para comprender

amplia variedad de fenómenos de lectura. Fomenta

y resolver problemas matemáticos complejos.

evaluación integral y confiable de las capacidades de comprensión lectora.

Math23k [373]: Este desafía la capacidad de un modelo para comprender y
resolver problemas matemáticos. contiene
23.000 problemas planteados de aritmética china que requieren modelos

RTE [356]: El Reconocimiento de la Vinculación Textual (RTE)
Los conjuntos de datos provienen de una serie de concursos anuales sobre textos.

Realizar razonamientos y cálculos basados en el problema.
28

Machine Translated by Google

Tabla 9: Conjuntos de datos de evaluación categorizados utilizados en la evaluación de LLM.

Tipo
Multitarea
Comprensión del lenguaje

Conjuntos de datos/puntos de referencia

MMLU [297], SuperGLUE [2], BIG­bench [298], GLUE [299], BBH [298], CUGE [300], Zero­
CLUE [301], FewCLUE [302], Charla de habilidades combinadas [303], HELM [304], KLUE­STS [305]
CoQA [306], WiC [307], Wikitext103 [308], PG19 [309], LCQMC [310], QQP [311], WinoGender [312],
CB [313], FinRE [314], SanWen [315], AFQMC [301], BQ Corpus [316], CNSS [317], CKBQA 13 [318],
CLUENER [301], Weibo [319], AQuA [320], OntoNotes [321], HeadQA [322], Conjunto de datos de Twitter [323]

Cierre de historia y
Completar oraciones
Conocimiento físico y
Comprensión mundial
Lenguaje contextual

StoryCloze [324], LAMBADA [325], LCSTS [326], AdGen [327], E2E [328], CHID [329], CHID­
FC [302]
PIQA [330], TriviaQA [331], ARC [332], ARC­Easy [332], ARC­Challenge [332], PROST [333], Open­BookQA [334], WebNLG
[335], DogWhistle Insider y Outsider [336]
CARRERA [337], CARRERA­Media [337], CARRERA­Alta [337], QuAC [338], StrategyQA [339], Quiz Bowl [340],

Comprensión

cMedQA [341],cMedQA2 [342], MATINF­QA [343]

Razonamiento de sentido común

WinoGrande [344], HellaSwag [345], COPA [346], WSC [347], CSQA [348], SIQA [349], C3 [350],
CLUEWSC2020 [301], CLUEWSC [301], CLUEWSC­FC [302], Registro [351]

Comprensión lectora

SQuAD [352], BoolQ [353], SQUADv2 [354], DROP [355], RTE [356], WebQA [357], CMRC2017 [358],
CMRC2018 [359], CMRC2019 [360], COTE­BD [361], COTE­DP [361], COTE­MFW [361], Multi­tiRC [362], Preguntas
Naturales [363], CNSE [317], DRCD [364], DuReader [365], Dureaderrobust [366],
DuReader­QG [365], SciQ [367], Sogou­log [368], Dureaderrobust­QG [366], QA4MRE [369], KorQuAD
1.0 [370], CAIL2018­Tarea1 y Tarea2 [371]

Razonamiento matemático

MATEMÁTICAS [372], Math23k [373], GSM8K [374], MathQA [375], MGSM [376], MultiArith [377], AS­Div [378], MAWPS
[379], SVAMP [380]

Resolución de problemas

Inferencia del lenguaje natural
y razonamiento lógico

HumanEval [131], DS­1000 [381], MBPP [382], APLICACIONES [372], CodeContests [132]
ANLI [383], MNLI­m [384], MNLI­mm [384],QNLI [352], WNLI [347], OCNLI [301], CMNLI [301],
ANLI R1 [383], ANLI R2 [383], ANLI R3 [383], HANS [385], OCNLI­FC [302], LogiQA [386], Strate­gyQA [339]

Comprensión multilingüe MLQA [387], XNLI [388], PAWS­X [389], XSum [390], XCOPA [391], XWinograd [392], TyDiQA­
OroP [393], MLSum [394]
Veracidad y verificación de hechos TruthfulQA [395], MultiFC [396], Verificación de hechos sobre la fiebre [397]
Sesgos y ética en la IA
ETHOS [398], StereoSet [399], BBQ [400], Winobias [401], CrowS­Pairs [402]
Toxicidad

RealToxicityPrompts [403], clasificación de toxicidad de CivilComments [404]

Traducción de idiomas

WMT [405], WMT20 [406], WMT20­enzh [406], EPRSTMT [302], CCPM [407]

Conocimiento científico

AminoProbe [138], BioLAMA [138], Reacciones químicas [138], Cúmulos de galaxias [138], Mineral

Diálogo

Asistente de Wikipedia [408], Diálogos empáticos [409], diálogos generados por DPC [96] , ConvAI2 [410],
Conv.Kd [411]

Clasificación de temas

TNEWS­FC [302], YNAT [305], KLUE­TC [305], CSL [301], CSL­FC [302], IFLYTEK [412]

Grupos [138]

5.2.10. Comprensión interlingüística

descripción.
GSM8K [374]: un conjunto de datos de diversas palabras matemáticas de la escuela primaria

XNLI [388]: Un punto de referencia multilingüe, XNLI amplía la

problemas, probando la capacidad de un modelo para realizar un razonamiento matemático

MultiNLI [419] corpus a 15 idiomas, incluidos los de bajos recursos

y matemático de varios pasos.

otros como el urdu. Prueba modelos sobre comprensión de oraciones en varios

5.2.9. Resolución de problemas y razonamiento lógico

vinculación, contradicción y neutralidad.

idiomas, con 112.500 pares anotados en tres categorías:
ANLI [383]: un conjunto de datos a gran escala diseñado para probar la solidez

PAWS­X [389]: PAWS­X, o Adver­sarios de paráfrasis multilingüe de Word

de los modelos de aprendizaje automático en inferencia del lenguaje natural

Scrambling, es una versión multilingüe del

(NLI) se crea a través de un proceso iterativo y contradictorio donde

Conjunto de datos PAWS [420] para identificación de paráfrasis. incluye

los humanos intentan generar ejemplos que los modelos no pueden correctamente

ejemplos en siete idiomas y está diseñado para evaluar la

clasificar.

Rendimiento de modelos de identificación de paráfrasis multilingües.

HumanEval [131]: un conjunto de datos para evaluar la capacidad de resolución
de problemas de los modelos de IA, que incluye un conjunto diverso de
5.2.11. Veracidad

tareas que requieren diversas habilidades cognitivas, lo que la convierte en una
herramienta integral para evaluar la inteligencia general en IA.

Truthful­QA [395]: Un punto de referencia único que mide un

StrategyQA [339]: un conjunto de datos de respuesta a preguntas que requiere

Veracidad del modelo de lenguaje al generar respuestas. El

razonamiento sobre múltiples piezas de evidencia para evaluar

El conjunto de datos incluye preguntas en varias categorías como salud,

la capacidad de razonamiento estratégico de los modelos de IA, ampliando
los límites de lo que las máquinas pueden entender y responder.

derecho y política, algunos diseñados para probar el modelo contra conceptos
erróneos humanos comunes.
29

Machine Translated by Google

Tabla 10: Ilustración de conjuntos de datos de capacitación y tareas de evaluación empleadas por LLM previamente capacitados. Aquí, "QA" es respuesta a preguntas, "Clf" es clasificación, "NLI"
es inferencia del lenguaje natural, "MT" es traducción automática, "RC" es comprensión lectora, "CR" es razonamiento de sentido común, "MR" es razonamiento matemático,
"Mmm." es la memorización.

Punto de referencia

Veraz/
Modelos

banco
Conjunto de datos de entrenamiento

grande

MMLU

Súper
PEGAMENTO

Control de calidad Clf NLI MT

cerrar/
Terminación

Codificación RC CR MR

Inclinación/

Toxicidad/
Memoria.

T5
GPT­3
mT5
PanGu­α
CPM­2
Códice
ERNIE­3.0

C4 [10]
Rastreo común, WebText, Books Corpora,
Wikipedia
mC4 [11]
1,1 TB de corpus de texto
chino WuDaoCorpus
[109] 54 millones de repositorios públicos de
corpus de texto chino de Github, búsqueda de Baidu, web

texto, QA­largo, QA­corto, Poesía y Cou­
plet Datos específicos del dominio de medicina,
derecho y área financiera conocimiento de Baidu

gráfico con más de 50 millones de datos

Jurásico­1

Wikipedia, OWT, Libros, C4, Pila [291],
arXiv, GitHub

HiperCLOVA

Blogs coreanos, sitios comunitarios, noticias,

yuanes 1,0

Ardilla de tierra

KiN Wikipedia coreana, Wikipedia
(inglés y japonés), Modu­Corpus: Mes­
senger, Noticias, Corpus de lenguas
habladas y escritas, Corpus web
Rastreo común, SogouT, Noticias Sogou,
Baidu Baike, Wikipedia, Libros
subconjuntos de MassiveWeb Libros, C4, Noticias,
Ejemplos de GitHub y Wikipedia de Mass­siveText

ERNIE­3.0 TITAN Igual que ERNIE 3.0 y el conjunto de datos ad­versarial de
ERNIE 3.0, ERNIE 3.0 controlable
conjunto de datos

GPT­NeoX­20B
OPTAR

Pila [291]
RoBERTa [289], Pila [291], PushShift.io
Reddit [413]

FLORACIÓN

RAÍCES [13]

Galáctica

arXiv, PMC, Semantic Scholar, Wikipedia,

StackExchange, LibreText, libros de
texto abiertos, RefSeq Genome, OEIS, LIPID
MAPAS, NASAExoplaneta, rastreo común,
ScientificCC, AcademicCC, repositorios de
GitHub Khan Problems, GSM8K, OneS­
mallStep

GLAM

LaMDA
MT­NLG

Páginas web filtradas, Conversaciones en
redes sociales Wikipedia, Foros, Libros, Noticias
Infiniset : Documentos públicos, Diálogos, Manifestaciones

Dos instantáneas de Common Crawl y
Libros3, OpenWebText2, Stack Exchange,
Resúmenes de PubMed, Wikipedia, PG­19

[242], BookCorpus2, NIH ExPorter, Pila,
CC­Historias, RealNews

código alfa

Repositorios de GitHub seleccionados, pruebas
CodeCon: Codeforces, Descripción2Code, Co­
deNet

Chinchilla

MassiveWeb, Libros MassiveText, C4,
Noticias, GitHub, Wikipedia

Palmera

páginas web, libros, Wikipedia, noticias, artículos,
código fuente, conversaciones en redes sociales

AlexaTM
U­PaLM
UL2
GLM­130B

Wikipedia, mC4

CódigoGen

PanGu­Σ

Pila, BigQuery, BigPython
CommonCrawl, C4, Github, Wikipedia,
Libros, arXiv, StackExchange
WuDaoCorpora, CLUE, Pila, C4, Python

BloombergGPT

inPile, Pile, C4, Wikipedia

Llama

Igual que PaLM
­
­

código

CódigoT5+

CodeSearchNet, Código Github The

codificador estrella

Stack v1.2

Llama­2
PaLM­2

Documentos web, Código, Libros, Matemáticas,

Conversación

30

Machine Translated by Google

Tabla 11: Una ilustración de los conjuntos de datos de capacitación y los puntos de referencia de evaluación utilizados en los LLM ajustados. “SNI” es un corto de Super­NaturalInsturctions.

Modelos

T0
WebGPT

banco
Conjunto de datos de entrenamiento

grande

Veraz/
MMLU BBH RAFT FLAN SNI PromptSource TyDiQA HumanEval MBPP

Inclinación/

Toxicidad

Conjunto de indicaciones

Hecho de ELI5­
ELI5 [414],
comprobar [156], TriviaQA [331],
ARC­Desafío [332], ARC­
Fácil [332], datos escritos a mano,
Demostraciones de humanos,
comparaciones entre modelos generados
respuestas

Tk­INSTRUCCIÓN SNI [18] mT0 xP3
[144]

OPT­IML

Fuente del mensaje [17], FLAN [16],
SNI [415], UnificadoSKG [416],
CrossFit [417], ExMix [418],
T5 [10], Razonamiento

Flan

Muffin, T0­SF, NIv2, CoT
WizardCoder Código Alpaca

5.2.12. Sesgos y ética en la IA

vicio para manejar preguntas comunes; o aplicado para generar contenido para
plataformas digitales como sitios web, mediante la creación de contenido similar al humano.

ETHOS [398]: ETHOS es un conjunto de datos de detección de discursos de odio

construido a partir de comentarios de YouTube y Reddit. Es una herramienta en el

texto basado en indicaciones dadas [424]. Además, los LLM desempeñan un papel

luchar contra el discurso de odio en línea, ofreciendo binarios y etiquetas múltiples

crucial en el análisis de datos, donde pueden filtrar grandes volúmenes de

variantes para una moderación sólida del contenido.

datos de texto, resumir puntos clave y encontrar patrones que
A los humanos les lleva mucho más tiempo identificarlos [425]. A pesar de su amplia

StereoSet [399]: StereoSet es un conjunto de datos integral diseñado para medir y

gama de aplicaciones, es fundamental recordar que los LLM,

evaluar la presencia de estereotipos.

similares a cualquier sistema de IA, son tan buenos como los datos que tienen

Sesgos en los modelos lingüísticos. Se centra en cuatro dominios clave:

sido entrenado.

género, profesión, raza y religión. Contrastar el sesgo estereotipado con la capacidad
de modelar el lenguaje proporciona una valiosa

Medicina: la aplicación de los LLM en el campo de la medicina es

herramienta para comprender y mitigar sesgos en lenguaje extenso

remodelar la prestación de servicios de salud y la investigación. Por ejemplo, LLM

modelos.

se utilizan cada vez más en los sistemas de apoyo a las decisiones clínicas para
proporcionar a los médicos recomendaciones de tratamiento basadas en evidencia
[426, 427, 428]. Al analizar los datos de los pacientes y los médicos

6. Aplicaciones

literatura, pueden ayudar a identificar diagnósticos potenciales, sugerir
pruebas apropiadas y recomendar estrategias de tratamiento óptimas.

Aplicación de modelos de lenguaje grande (LLM) a una variedad de

Además, los LLM también pueden mejorar las interacciones de los pacientes con

Las tareas posteriores se han convertido en una tendencia popular tanto en las

sistemas de salud; por ejemplo, se pueden utilizar en aplicaciones de chatbot [429,

comunidades como en las industrias de investigación relacionadas con la IA, y

430, 431] para responder las consultas de los pacientes sobre los síntomas.

diariamente se descubren y exploran muchos usos emergentes. LLM, que son

o medicamentos, programar citas e incluso brindar consejos de salud esenciales. Para

capaz de comprender y generar texto similar al humano, tiene

la investigación médica, los LLM se utilizan para

encontró aplicaciones significativas en una variedad de campos. Este

extraer y filtrar información de una cantidad considerable de

La sección proporciona una descripción general de las aplicaciones de LLM en medicina.

literatura médica, identificar estudios relevantes, resumir hallazgos e incluso predecir

educación, ciencia, matemáticas, derecho, finanzas, robótica y

tendencias futuras de investigación [432, 433, 434].

codificación. Si bien cada uno de estos ámbitos plantea desafíos diferentes,

Para la educación médica, los LLM pueden ayudar a crear materiales de capacitación,

Los LLM abren oportunidades para hacer contribuciones significativas

generar preguntas de examen y proporcionar explicaciones detalladas.

a estos dominios a través de su generalización.

de temas médicos complejos y ofrecer comentarios personalizados a

Propósito general: los LLM se consideran ampliamente como

estudiantes [435, 436, 437, 438]. También pueden simular al paciente.

herramientas de uso general para una amplia variedad de tareas [421]. Este

interacciones, permitiendo a los estudiantes practicar y mejorar su

se debe a su capacidad inherente para comprender, generar y

habilidades clínicas. A un nivel más amplio, los LLM pueden ayudar en público.

manipular texto de apariencia humana de una manera contextualmente relevante. Esto

iniciativas de salud mediante el análisis de datos de los medios para detectar brotes de

les permite realizar tareas que van desde simples

enfermedades, monitorear el sentimiento público hacia las políticas de salud y

traducción de idiomas y respuesta a preguntas a temas más complejos.

Difundir información de salud de forma clara y comprensible.

tareas como resúmenes, generación de texto e incluso ayuda en programación [422].

manera [439]. Los LLM se pueden emplear para apoyar la salud pública

La utilidad de los LLM se ve reforzada aún más por

iniciativas que abordan cuestiones relacionadas, como la privacidad de los datos, la

su capacidad para adaptarse al estilo y tono específicos del texto

la necesidad de explicabilidad y el riesgo potencial de propagar sesgos [440, 441].

están procesando, lo que hace que los resultados sean más fáciles de usar y
consciente del contexto. En aplicaciones cotidianas, los LLM se pueden utilizar como

Educación: la integración de los LLM en el sector educativo ofrece oportunidades para

asistentes personales, que ayudan a los usuarios a redactar correos electrónicos o

mejorar las experiencias de aprendizaje, docente

programar citas [423]; También se pueden implementar en el servicio de atención al cliente.
31

Machine Translated by Google

Tabla 12: Comparación de desempeño de los LLM con mejor desempeño en varias tareas de NLU y NLG. Aquí, "N­Shots" indica la cantidad de mensajes de ejemplo proporcionados
al modelo durante la evaluación, que representa su capacidad en entornos de aprendizaje de pocos intentos o de cero disparos, "f" representa la versión ajustada y "B" representa la
punto de referencia.

Tarea
Multitarea

Modelo Top­1 Top­2 (Tamaño)

Conjunto de datos/punto de referencia

Banco GRANDE (B)

MMLU (B)

Comprensión del lenguaje SuperGLUE (B)

Top­3

Puntuación (N­disparos) Modelo (Tamaño)

Puntuación (N­tiros)

Modelo (Tamaño)

Tuza (280B) 53,97 (5 disparos)

Palma (540B)

53,7 (5 tiros)

GPT­4 (­) 86,4 (5 disparos)

Géminis (Ultra) 83,7 (5 disparos)

Flan­PaLM­2(f) (Grande)

81,2 (5 tiros)

ERNIE 3.0 (12B) 90.6 (­)

T5 (11B)

88,9 (­)

PaLM­2 (Grande)

86,8 (un disparo)

PaLM(f) (540B) 90,4 (­)

GPT­4 (­) 95,3 (10 disparos)

Géminis (Ultra) 87,8 (10 disparos)

Comprensión de la historia y
Generación

HellaSwag

Conocimiento físico y

PIQA

PaLM­2 (Grande) 85,0 (un disparo) LLaMa (65B) 82,8 (cero disparo)

MT­NLG (530B)

CuriosidadesQA

Comprensión mundial
Lenguaje contextual

HistoriaCloze

Razonamiento de sentido común

Razonamiento matemático
Resolución de problemas y

GPT3 (175B) 87,7 (pocos disparos) PaLM­2 (grande) 87,4 (un disparo)

79,82 (­)

OPCIÓN (175B)

81,99 (tiro cero)

PaLM­2 (Grande) 86,1 (un disparo) LLaMA­2 (70B) 85,0 (un disparo)

Palma (540B)

LAMBADA

Palma (540B)

89,7 (pocos disparos) MT­NLG (530B) 87,15 (pocos disparos)

PaLM­2 (Grande)

86,9 (un disparo)

WinoGrande

GPT­4 (­)

87,5 (5 tiros)

Palma (540B)

81.1 (tiro cero)

Comprensión

Comprensión de lectura
Veracidad

Puntuación (N­tiros)

Chinchilla (70B) 65,1 (5 disparos)

SIQA

Llama (65B)

(disparo cero) Chinchilla (70B) 51,3 (disparo cero)

PaLM(f) (540B)

92,2 (­)

Veraz­QA

Llama (65B)

57 (­)

MATEMÁTICAS

Géminis (ultra)

GSM8K

GPT­4 (­)

evaluación humana

Razonamiento lógico

PaLM­2 (Grande) 83,0 (un disparo) 52,3

Topo (280B)

50,6 (tiro cero)

PaLM­2 (Grande)

90,9 (un disparo)

34,3 (4 disparos)

Llama­2 (65B)

13,5 (4 disparos)

80,7 (8 disparos)

U­PaLM (540B)

T5 (11B)

91,2 (­)

53,2 (4 disparos)

PaLM­2 (Grande)

92,0 (5 disparos)

PaLM­2 (Grande)
GPT­4 (­)

Géminis(f) (Ultra) 74,4 (tiro cero)

apoyo y desarrollo de contenidos educativos. Para los estudiantes, por

81,4 (un disparo)

67.0 (tiro cero)

Código Llama (34B)

58,5 (­)
48,8 (tiro cero)

tificar errores de razonamiento o cálculo y sugerir correcciones,

analizar sus estilos de aprendizaje, desempeño y preferencias,

Sirviendo como una herramienta invaluable tanto para el aprendizaje como para la verificación.

Los LLM pueden proporcionar prácticas y materiales de estudio personalizados

fines [451, 452]. Los LLM se pueden emplear para verificar la validez de las pruebas

preguntas para desarrollar experiencias de aprendizaje personalizadas [442].

matemáticas, ofreciendo un filtro preliminar antes de

Para los profesores, los LLM pueden ayudar a crear planes de lecciones y calificaciones.

revisión humana. Si bien no sustituyen el trabajo meticuloso de los matemáticos,

tareas y generar educación diversa e inclusiva.

pueden ayudar a simplificar el proceso.

contenido, lo que ahorra significativamente más tiempo para la enseñanza y los estudiantes.

de verificación de pruebas [453, 454]. Además, los LLM mejoran la accesibilidad a las

interacción [443, 444]. En el aprendizaje de idiomas, los LLM sirven como

matemáticas al traducir conceptos complejos y

compañeros de conversación avanzados capaces de simular conversaciones en

hallazgos en un lenguaje comprensible para los no especialistas [455],

múltiples idiomas, corregir gramática, mejorar

donde la brecha entre las matemáticas teóricas y las aplicadas

vocabulario y ayuda a la pronunciación para las necesidades de fluidez

contextos como la física, la ingeniería y la economía pueden ser

en la práctica [445]. Además, los LLM mejoran la accesibilidad.

puenteado.

en educación brindando apoyo a estudiantes con discapacidades. Pueden generar

Derecho: los LLM pueden ayudar con el análisis temático de documentos legales,

transcripciones en tiempo real para personas con discapacidad auditiva, ofrecer

incluida la generación de codificación inicial para conjuntos de datos, la identificación

asistencia de lectura para personas con discapacidad visual,

de temas y la clasificación de datos según estos temas.

y simplificar textos complejos para personas con problemas de aprendizaje [441]. A

Este esfuerzo de colaboración entre expertos legales y LLM ha

medida que los LLM continúan evolucionando, sus aplicaciones en

demostrado ser eficaz en el análisis de textos legales como los tribunales

La educación puede beneficiar a más estudiantes y profesores de diferentes

opiniones sobre robo, mejorando tanto la eficiencia como la calidad de

perspectivas en la práctica.

la investigación [456]. Además, los LLM han sido evaluados para

Ciencia: similar a las aplicaciones médicas, los LLM pueden acelerar

su capacidad para generar explicaciones de términos legales, centrándose

el proceso de investigación analizando y resumiendo rápidamente la literatura científica.

para mejorar la precisión y relevancia de los hechos incorporando

Al presentar resúmenes de investigación comprensibles y accesibles, los LLM pueden

sentencias de la jurisprudencia. Introduciendo la jurisprudencia pertinente en el

ayudar a los investigadores a mantenerse actualizados con los últimos hallazgos,

LLM, los modelos aumentados pueden generar explicaciones de mayor calidad con

incluso en campos fuera de su área.

menos información objetivamente incorrecta [457]. Además, los LLM pueden formarse

de experiencia [446, 447]. Además, los LLM pueden ayudar a los científicos.

con conocimientos de dominio especializados.

en la formulación de nuevas hipótesis y preguntas de investigación desde

realizar tareas de razonamiento jurídico [458] y responder preguntas jurídicas [459].

Su capacidad para procesar conjuntos de datos a gran escala les permite revelar
Finanzas: LLM como BloombergGPT [141], capacitados en amplios conjuntos de datos

conocimientos que podrían no ser inmediatamente evidentes para los humanos.
investigadores [448]. Además, para la redacción científica, los LLM pueden

financieros patentados, exhiben un rendimiento superior

ayudar a los investigadores a redactar documentos, sugerir mejoras y

en tareas financieras. Esto indica el valor de dominio específico.

garantizar el cumplimiento de pautas de formato específicas [449, 450].

capacitación en la creación de LLM que puedan comprender con mayor precisión

Esto no sólo ahorra tiempo sino que también mejora la claridad de la comunicación

y procesar lenguaje y conceptos específicos de la industria. La introducción de FinGPT

científica, permitiendo que trabajen equipos interdisciplinarios.

[460] como modelo de código abierto ofrece recursos transparentes y accesibles para

juntos de manera más efectiva.

desarrollar aplicaciones novedosas.

Matemáticas: Además de proporcionar investigación matemática y

como el robo­asesoramiento, el comercio algorítmico y las soluciones de código bajo,

Apoyo educativo, los LLM pueden ayudar a resolver problemas matemáticos.

que en última instancia amplían las capacidades de los servicios financieros. Tanto

problemas dando explicaciones paso a paso y guiando a los usuarios

BloombergGPT como FinGPT muestran la adaptabilidad de los LLM al ámbito

a través de pruebas y cálculos complejos. Pueden ayudar a identificar

financiero, y el primero muestra
32

Machine Translated by Google

Proporcionar respuestas precisas a preguntas precisas. Sin embargo, la generalización

el poder de los conjuntos de datos personalizados y este último enfatiza un enfoque
centrado en los datos y técnicas de adaptación de bajo rango para la personalización.

permite que el modelo haga inferencias y produzca

Además, los LLM demuestran una capacidad para romper

respuestas a entradas que no ha visto antes, lo cual es esencial

convertir tareas financieras complejas en planes viables, lo que permite

para manejar diversas tareas del mundo real. El desafío es lograr el equilibrio adecuado:

soluciones de extremo a extremo que antes eran inviables con un solo modelo [461].

demasiada memorización puede llevar a un ajuste excesivo, haciendo que el modelo
sea inflexible y teniendo problemas con nuevas cosas.

Robótica: en la investigación de robótica, los LLM tienen aplicaciones prometedoras,

entradas [470].

como mejorar la interacción entre humanos y robots [28, 462,

Desigualdad económica y de investigación: el alto costo de la capacitación y el

463, 464], planificación de tareas [227], planificación de movimiento [236], navegación

despliegue de LLM puede hacer que su desarrollo se concentre en organizaciones bien

[236, 465], manipulación de objetos [226], personalizado

financiadas, lo que podría empeorar

robots [466], etc. Los LLM permiten a los robots comprender el entorno de manera

Desigualdades económicas y de investigación en IA [471].

efectiva y generar planes para completar tareas de manera colaborativa [230, 26].

Razonamiento y planificación: algunas tareas de razonamiento y planificación,

Pueden facilitar el aprendizaje continuo.

incluso tan aparentemente simple como una planificación de sentido común, que

al permitir que los robots accedan e integren información desde un

los humanos encuentran fáciles, permanecen mucho más allá de las capacidades actuales

amplia gama de fuentes, ayudando a los robots a adquirir nuevas habilidades, adaptarse

de LLM evaluados utilizando un marco de evaluación. esto no es

a los cambios y afinar sus caminos [214, 223, 224].

completamente inesperado, considerando que los LLM generan principalmente
La finalización de textos se basa en la probabilidad y no ofrece garantías sólidas en
términos de capacidad de razonamiento [472].

7. Desafíos y direcciones futuras

Alucinaciones: los LLM exhiben "alucinaciones", donde
generar respuestas que, aunque parezcan plausibles, son incorrectas o no se alinean

Los LLM como GPT­4 y sus predecesores han tenido un impacto significativo

con la información proporcionada [473]. El

Procesamiento avanzado del lenguaje natural. Sin embargo, ellos también

Las alucinaciones se pueden clasificar en tres categorías.

traer consigo una serie de desafíos. El costo computacional, la robustez del adversario
• Alucinación con entrada en conflicto, en la que los LLM producen

y la interpretabilidad se encuentran entre los desafíos técnicos intrínsecos a estos

Contenido que diverge de las aportaciones proporcionadas por los usuarios.

modelos. Además, a medida que estos modelos se amplían para manejar tareas más
complejas

• Alucinaciones que entran en conflicto con el contexto, donde los LLM generan

tareas o para operar en entornos más complejos o dinámicos,

Contenido que contradice la información que han generado.

Nuevos desafíos en escalabilidad, privacidad y procesamiento en tiempo real.

más temprano.

surgir. En la frontera de la investigación fundamental, la integración
La multimodalidad y la eficacia del aprendizaje por transferencia se están explorando

• La alucinación que contradice los hechos involucra la generación de LLM

intensamente. Además, el aspecto de aprendizaje continuo de estos modelos, que

de contenido que no se alinea con el mundo establecido

apunta a tener modelos que puedan adaptarse

conocimiento.

a nueva información a lo largo del tiempo, presenta una nueva serie de desafíos.
Estos desafíos no sólo subrayan las complejidades técnicas

Ingeniería de indicaciones: las indicaciones sirven como insumos para los LLM y

involucrados, pero también resaltan el impacto más amplio y el futuro

Su sintaxis y semántica juegan un papel crucial en la determinación.

trayectoria de los LLM en aplicaciones del mundo real. La siguiente

la salida del modelo. Las variaciones inmediatas, a veces contraintuitivas para los

Las secciones profundizan en estos desafíos, arrojando luz sobre los esfuerzos en

humanos, pueden resultar en cambios significativos en el modelo.

curso y potenciales para abordarlos.

resultados y se abordan a través de una ingeniería rápida, que

Costo computacional: la capacitación de LLM requiere amplios recursos computacionales,

Implica diseñar consultas en lenguaje natural para guiar a los LLM.

lo que aumenta los costos de producción y

respuestas de manera efectiva [474, 32].

plantea preocupaciones medioambientales debido al consumo sustancial de energía

Conocimiento limitado: información adquirida durante el entrenamiento previo.

durante la formación a gran escala. Rendimiento mejorado

es limitado y puede quedar obsoleto después de algún tiempo. Volver a entrenar el

ocurre a medida que aumentan los recursos computacionales, pero la tasa de

modelo utilizando datos actualizados es costoso. para generar

La mejora disminuye gradualmente cuando tanto el modelo como el

Para obtener respuestas objetivamente precisas, la gente utiliza un canal de aumento

El tamaño del conjunto de datos permanece fijo, siguiendo la ley de potencia de los

de recuperación [188]. Sin embargo, los modelos previamente entrenados no son

rendimientos decrecientes [467].

entrenado con generación de aumento de recuperación (RAG) [6, 21],

Sesgo y equidad: los LLM pueden heredar y amplificar los sesgos sociales en sus datos

por lo tanto, es necesario adaptar el proceso de formación [183, 25].

de capacitación. Estos sesgos pueden manifestarse en la

Seguridad y controlabilidad: el uso de LLM conlleva riesgos

resultados del modelo, lo que lleva a posibles problemas éticos y de equidad [468].

de generar contenidos nocivos, engañosos o inapropiados,
ya sea por accidente o cuando se le dan indicaciones específicas. asegurando

Sobreajuste: aunque los LLM poseen capacidades de aprendizaje sustanciales, son

Que estos modelos se utilicen de forma segura es una preocupación importante [475].

susceptibles a sobreajustes ruidosos y peculiares.

Multimodalidad: aprendizaje multimodal, donde se encuentran los LLM

patrones dentro de sus extensos datos de entrenamiento. En consecuencia, este

capacitado en datos diversos como texto, imágenes y videos, tiene como objetivo

puede hacer que generen respuestas ilógicas [469]. El debate sobre
memorización versus generalización en los LLM trata sobre

en alineación de datos, estrategias de fusión y computación superior

encontrar el equilibrio adecuado. La memorización permite que el modelo

demandas.

recordar detalles específicos de sus datos de entrenamiento, asegurando que pueda

Olvido catastrófico: los LLM a menudo reciben capacitación previa en grandes

crear modelos con una comprensión más rica pero enfrenta desafíos

33

Machine Translated by Google

conjuntos de datos y luego ajustarlos en datos específicos del dominio, reduciendo

estos modelos. Las GPU han desempeñado un papel crucial a la hora de satisfacer las necesidades

recursos de capacitación, pero enfrenta problemas como la adaptación del dominio y

Requisitos de hardware para la formación de LLM, con la red.

Olvido catastrófico, que dificulta la retención del original.

La industria también evoluciona para optimizar el hardware para la formación.

conocimientos al aprender nuevas tareas.

cargas de trabajo. Sin embargo, el tamaño creciente de los LLM, que ha

Robustez adversaria: modelos de lenguajes grandes (LLM)

estado superando el progreso del hardware, hace que la inferencia de modelos sea

han demostrado grandes capacidades en diversas tareas, pero son vulnerables a

cada vez más costosa. La cuantificación del modelo es un enfoque prometedor

ataques adversarios, donde la entrada ligera y deliberada

para cerrar la brecha cada vez mayor entre el tamaño de LLM y el hardware

las modificaciones pueden inducirles a error. Especialmente con modelos como

capacidad [484]. Aunque la aceleración de hardware especializada

BERT, el ajuste fino adversario puede mejorar la robustez, aunque a veces compromete

como GPU o TPU pueden reducir significativamente el costo computacional

la generalización [476]. Como

costo, haciendo que las aplicaciones en tiempo real sean más factibles, es posible que no

Los LLM se integran más en sistemas complejos, examinando sus

resolver completamente todas las limitaciones, lo que requiere más avances

propiedades de seguridad se vuelve crucial, dado el campo emergente

en tecnología de hardware.

de ataques adversarios a LLM dentro de ML confiable [477].

Marcos regulatorios y éticos: los rápidos avances

Esta vulnerabilidad es notable en dominios críticos para la seguridad, lo que requiere

en inteligencia artificial han dado lugar a sofisticados Grandes

herramientas sólidas de evaluación adversaria para garantizar el LLM.

Modelos de lenguaje (LLM) como GPT­4 de OpenAI [147] y

fiabilidad [478].

Bardo de Google. Estos acontecimientos subrayan el imperativo

Interpretabilidad y explicabilidad: la naturaleza de la "caja negra"

de supervisión regulatoria para gestionar los aspectos éticos y sociales

de los LLM plantea desafíos para comprender su toma de decisiones, lo cual es crucial

desafíos que acompañan al uso generalizado de los LLM [485]. Para

para una aceptación y confianza más amplias.

Por ejemplo, los LLM pueden generar contenido que puede usarse positiva o

especialmente en ámbitos sensibles. A pesar de su avanzada

negativamente, enfatizando la necesidad de una ética proactiva.

capacidades, la falta de conocimiento sobre su funcionamiento limita su

marcos y medidas políticas para guiar a sus responsables

eficacia y confiabilidad [479, 480]. Se están realizando esfuerzos

utilizar y asignar responsabilidades por sus resultados [486]. Revisión de cuentas

hecho para hacer que los LLM sean más explicables para promover la confianza del usuario

se identifica como un mecanismo de gobernanza prometedor para garantizar

y garantizar el uso responsable de la IA. Entendiendo la lógica

que los sistemas de inteligencia artificial, incluidos los LLM, estén diseñados e implementados

Detrás de las respuestas de los LLM es esencial para fomentar la confianza y

ética, jurídica y técnicamente sólida [487].

asegurando que se alineen con los valores humanos y los estándares legales.
Preocupaciones de privacidad: Preocupaciones de privacidad en lenguaje grande
Los modelos (LLM) han aumentado con su crecimiento en complejidad

8. Conclusión

y tamaño, particularmente en lo que respecta al intercambio de datos y su posible uso indebido.

Este artículo ha revisado exhaustivamente la evolución de los LLM. Contribuye a

Existe el riesgo de creación de contenido malicioso, omisión de filtros,
y cuestiones de privacidad de datos, especialmente en el comercio electrónico, donde

resumir hallazgos importantes de los LLM en la literatura existente y proporciona un

proteger la privacidad del cliente es crucial. Si los modelos están entrenados

análisis detallado de los aspectos de diseño, incluidas las arquitecturas,

sobre datos privados, surgen preocupaciones adicionales si dichos modelos
puesto a disposición del público. Los LLM tienden a memorizar frases de

conjuntos de datos y canales de capacitación. Identificamos componentes

sus conjuntos de entrenamiento, que un adversario podría explotar para extraer

arquitectónicos cruciales y estrategias de capacitación empleadas por diferentes LLM.

datos sensibles, lo que representa una amenaza a la privacidad personal [481, 482].

Estos aspectos se presentan como resúmenes y

Procesamiento en tiempo real: el procesamiento en tiempo real en modelos de lenguaje

discusiones a lo largo del artículo. Además, hemos discutido las diferencias de

grande (LLM) es fundamental para diversas aplicaciones,

desempeño de los LLM en zero­shot y

especialmente con la creciente popularidad de las aplicaciones móviles de IA

configuraciones de pocas tomas, exploró el impacto del ajuste fino y comparó modelos

y preocupaciones sobre la seguridad y privacidad de la información.

supervisados y generalizados y arquitecturas de codificador, decodificador y codificador­

Sin embargo, los LLM suelen tener cientos de capas y millones

decodificador. También se proporciona una revisión integral de los LLM multimodales,

de parámetros, que impiden el procesamiento en tiempo real debido a la

los LLM de recuperación aumentada, los agentes impulsados por los LLM, los LLM

Altas demandas computacionales y almacenamiento de peso limitado en

eficientes, los conjuntos de datos, la evaluación, las aplicaciones y los desafíos. Este

plataformas de hardware, particularmente en entornos informáticos de borde [483]. Si

artículo está anticipado.

bien ciertos esfuerzos como MobileBERT apuntan

servir como un recurso valioso para los investigadores, ofreciendo ideas

para reducir los requisitos de memoria, todavía enfrentan sustanciales

sobre los avances recientes en los LLM y proporcionando conceptos y detalles

sobrecarga de ejecución debido a la gran cantidad de capas del modelo,

fundamentales para desarrollar mejores LLM.

lo que lleva a una alta latencia de inferencia.
Dependencias a largo plazo: modelos de lenguajes grandes (LLM)

Referencias

han demostrado avances considerables en la comprensión y
[1] A. Chernyavskiy, D. Ilvovsky, P. Nakov, Transformers:“¿el fin de la historia” para el

generar texto, pero a menudo tienen dificultades para preservar el contexto

procesamiento del lenguaje natural?, en: Machine Learning and

y manejar dependencias a largo plazo, particularmente en entornos complejos,

Descubrimiento de conocimiento en bases de datos. Línea de investigación:

conversaciones de varios turnos o documentos largos. Esta limitación

Conferencia europea, ECML PKDD 2021, Bilbao, España, 13 al 17 de septiembre de 2021.
Actas, Parte III 21, Springer, 2021, págs. 677–693. 1

puede dar lugar a respuestas incoherentes o irrelevantes.

[2] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,

Aceleración de hardware: el crecimiento de los LLM presenta importantes desafíos de

O. Levy, S. Bowman, Superglue: Un punto de referencia más estricto para sistemas

hardware debido al aumento de la capacidad computacional.

de comprensión del lenguaje de propósito general, Avances en sistemas de

y demandas de memoria asociadas con el entrenamiento y el despliegue.

procesamiento de información neuronal 32 (2019). 1, 24, 29

34

Machine Translated by Google

modelos de chat básicos y ajustados, preimpresión de arXiv arXiv:2307.09288 (2023). 2,

[3] D. Adiwardana, M.­T. Luong, DR So, J. Hall, N. Fiedel, R. Thoppilan, Z. Yang, A. Kulshreshtha,

7, 10, 16, 25, 33 [22] J. Wei, Y.

G. Nemade, Y. Lu, et al., Towards a human­like open­domain chatbot, arXiv preprint arXiv
:2001.09977 (2020). 1 [4] BA y Arcas, ¿Nos entienden los grandes modelos lingüísticos?,

Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yo­gatama, M. Bosma, D. Zhou, D.

Daedalus 151 (2) (2022) 183–197. 2

Metzler, et al., Habilidades emergentes de modelos de lenguaje grandes, preimpresión de
arXiv arXiv:2206.07682 (2022). 2 [23] T. Webb, KJ Holyoak, H. Lu, Razonamiento

[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., Los modelos de lenguaje

analógico emergente en modelos de lenguaje grandes, Nature Human Behavior 7 (9) (2023) 1526–

son aprendices multitarea no supervisados, blog de OpenAI 1 (8) (2019) 9. 2 , 7 [6] T.

1541. 2 [24] DA Boiko, R. MacKnight, G. Gomes, Capacidades de investigación científica

Brown, B. Mann, N.

autónoma emergente de grandes modelos de lenguaje, preimpresión de arXiv arXiv:2304.05332

Ryder, M. Subbiah, JD Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et

(2023). 2 [25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi­
Yu, A. Joulin, S. Riedel, E. Grave,

al., Modelos de lenguaje son estudiantes de pocas oportunidades, Avances en sistemas

Aprendizaje de pocas posibilidades con recuperación de modelos de lenguaje aumentado,

de procesamiento de información neuronal 33 (2020) 1877–1901. 2, 6, 7, 8, 9, 16, 17, 22,
23, 24, 25, 33 [7] J. Devlin, M.­W. Chang, K. Lee, K. Toutanova, Bert: Entrenamiento

preimpresión de arXiv arXiv:2208.03299 (2022). 2, 17, 18, 33

previo de transformadores bidireccionales profundos para la comprensión del lenguaje,
preimpresión de arXiv arXiv:1810.04805 (2018). 2, 18, 24 [8] ME Peters, M. Neumann, M.
Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettlemoyer,

[26] D. Driess, F. Xia, MS Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q.

Representaciones de palabras contextualizadas profundas, en: NAACL­HLT, Asociación de

Vuong, T. Yu, et al., Palm­e: Un modelo de lenguaje multimodal incorporado, preimpresión

Lingüística Computacional , 2018, págs. 2227–2237. 2

de arXiv arXiv:2303.03378 (2023). 2, 19, 21, 33 [27] A. Parisi, Y. Zhao, N. Fiedel, Talm:
Modelos de
lenguaje aumentados con herramientas, preimpresión de arXiv arXiv:2205.12255 (2022). 2, 18,
19 [28] B. Zhang, H. Soh, Modelos de lenguaje grandes como

[9] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, L.
Zettlemoyer, Bart: Entrenamiento previo de eliminación de ruido de secuencia a secuencia

modelos humanos de disparo cero para la interacción entre humanos y robots, preimpresión de
arXiv arXiv:2303.03548 (2023). 2, 33

para la generación de lenguaje natural , traducción y comprensión, preimpresión de arXiv
arXiv:1910.13461 (2019). 2 [10] C. Raffel, N. Shazeer, A.
Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, PJ Liu, Explorando los límites del

[29] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, et al., mplug­owl :

aprendizaje por transferencia con un texto unificado. transformador de texto, The Journal

La modularización potencia los grandes modelos de lenguaje con multimodalidad,

of Machine Learning Re­search 21 (1) (2020) 5485–5551. 2, 7, 8, 17, 19, 24, 25, 26, 28,

preimpresión de arXiv arXiv:2304.14178 (2023). 2, 22

30, 31
[30] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo, T. Lu, J. Zhou, Y. Qiao, et al.,
[11] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al­Rfou, A. Siddhant, A. Barua, C. Raffel, mt5:

Visionllm: Large El modelo de lenguaje también es un decodificador abierto para tareas
centradas en la visión, preimpresión de arXiv arXiv:2305.11175 (2023). 2, 22 [31] R. Yang,

Un texto a texto masivamente multilingüe pre­entrenado transformador, preimpresión de

L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, Y.

arXiv arXiv:2010.11934 (2020). 2, 7, 8, 24, 25, 28, 30

Shan, Gpt4tools: Enseñanza de modelos de lenguaje grandes para utilizar herramientas mediante
[12] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi, J. Guan, P. Ke, et al.,

autoinstrucción, preimpresión de arXiv arXiv:2305.18752 (2023). 2, 19, 22 [32] E. Saravia,
Guía de ingeniería rápida, https://github.com/dair­ai/

Cpm­2 : Modelos de lenguaje preentrenados, rentables y a gran escala, AI Open 2 (2021)
216–224. 2, 8, 25 [13] TL Scao, A. Fan, C. Akiki, E. Pavlick, S.
´
Ilic, D. Hesslow, R. Castagné, AS Luccioni, F. Yvon, M. Gallé, et al., Bloom: un modelo de lenguaje

Prompt­Engineering­Guide (12 2022). 2, 7, 17, 33 [33] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M.
Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al., Glm­130b: An open

multilingüe de acceso abierto con parámetros 176b, preimpresión de arXiv arXiv:2211.05100

bilingual pre­trained model, arXiv preprint arXiv:2210.02414 (2022). 2, 10, 22, 23, 25 [34] Y. Wang,

(2022). 2, 4, 9, 11, 22, 23, 24, 25, 30 [14] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M.

H. Le, AD Gotmare, ND Bui, J. Li, SC Hoi, Codet5+: modelos de lenguaje grande de código

Chen, S. Chen, C. Dewan, M. Diab, X. Li, XV Lin, et al., Opt: modelos

abierto para la comprensión y generación de código, preimpresión de arXiv

de lenguaje transformador preentrenados abiertos, preimpresión de arXiv arXiv:2205.01068

arXiv:2305.07922 (2023). 2, 10, 24, 25 [35] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong,

(2022). 2, 9, 11, 23, 24, 25

S. Feng, J. Shang, Y. Zhao, C. Pang, et al., Ernie 3.0 titan: Explorando la capacitación
previa mejorada del conocimiento a mayor escala para la comprensión y
generación del lenguaje, preimpresión de arXiv arXiv:2112.12731 (2021). 2, 8, 23, 25 [36] J.

[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, HW

Rasley, S. Rajbhandari, O. Ruwase, Y. He, Deepspeed: Las optimizaciones del sistema

Chung, C. Sutton, S. Gehrmann, et al., Palm: Scal­ modelado de lenguaje con vías,

permiten entrenar modelos de aprendizaje profundo con más de 100 mil millones de
parámetros, en: Actas de la 26ª ACM Conferencia internacional

preimpresión de arXiv arXiv:2204.02311 (2022). 2, 6, 9, 10, 22, 23, 24, 25 [16] HW Chung,
L. Hou, S. Longpre, B. Zoph, Y. Tay, W.

SIGKDD sobre descubrimiento de conocimientos y minería de datos, 2020, págs. 3505­3506. 2,

Fedus, E. Li, X. Wang, M Dehghani, S. Brahma, et al., Escalamiento de modelos de lenguaje

5

ajustados a la instrucción, preimpresión de arXiv arXiv:2210.11416 (2022). 2, 7, 11, 17, 22,
23, 25, 28, 31 [17] V. Sanh, A. Webson, C. Raffel, SH Bach, L. Sutawika, Z. Alyafeai, A.
Chaffin, A. Stiegler ,
TL Scao, A. Raja, et al., La capacitación impulsada por multitarea permite la generalización de

[37] S. Rajbhandari, J. Rasley, O. Ruwase, Y. He, Zero: Optimizaciones de memoria hacia el

tareas inmediatas, preimpresión de arXiv arXiv:2110.08207 (2021). 2, 11, 25, 28, 31 [18]

entrenamiento de modelos de billones de parámetros, en: SC20: Conferencia internacional

Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, AS

sobre informática, redes, almacenamiento y análisis de alto rendimiento, IEEE, 2020, págs.

Dhanasekaran, A. Arunkumar, D. Stap , et al.,

1­16. 2, 4, 23

Instrucciones sobrenaturales: generalización mediante instrucciones declarativas en más de 1600

[38] J. He, C. Zhou, X. Ma, T. Berg­Kirkpatrick, G. Neubig, Hacia una visión unificada del

tareas de PNL, en: Actas de la Conferencia de 2022 sobre métodos empíricos en el

aprendizaje por transferencia eficiente en parámetros, preimpresión de arXiv
arXiv:2110.04366 (2021). 2, 20, 21

procesamiento del lenguaje natural, 2022, págs. 2, 7, 11, 17, 23, 25, 28, 31

[39] Z. Hu, Y. Lan, L. Wang, W. Xu, E.­P. Lim, RK­W. Lee, L. Bing, S. Po­ria, Llm­adapters: Una
familia de adaptadores para el ajuste eficiente de parámetros de modelos de lenguaje
grandes, preimpresión de arXiv arXiv:2304.01933 (2023). 2, 20
[19] Y. Wang, Y. Kordi, S. Mishra, A. Liu, NA Smith, D. Khashabi, H. Ha­jishirzi, Autoinstrucciones:
[40] B. Lester, R. Al­Rfou, N. Constant, El poder de la escala para un ajuste rápido eficiente en

alineación del modelo de lenguaje con instrucciones autogeneradas, preimpresión de
arXiv arXiv: 2212.10560 (2022). 2, 11, 18, 22, 28 [20] L. Ouyang, J. Wu, X. Jiang, D.

los parámetros, preimpresión de arXiv arXiv:2104.08691 (2021). 2, 8, 20

Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Entrenamiento
de modelos de lenguaje para seguir instrucciones con retroalimentación humana, Avances

[41] XL Li, P. Liang, Ajuste de prefijos: optimización de indicaciones continuas para la generación,

en sistemas de procesamiento de información neuronal 35 (2022) 27730–27744. 2, 7, 11,

preimpresión de arXiv arXiv:2101.00190 (2021). 2, 20 [42] X. Ma, G. Fang,

16,
22

X. Wang, Llm­pruner: Sobre la poda estructural de modelos de lenguaje grandes, preimpresión de
arXiv arXiv:2305.11627 (2023). 2, 21 [43] R. Xu, F. Luo, C. Wang, B. Chang, J. Huang,

[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P.

S. Huang, F. Huang, De denso a escaso: poda contrastiva para mejores tierras previamente

Bhargava, S. Bhosale, et al., Llama 2: Abierto

entrenadas

35

Machine Translated by Google

compresión del modelo de lenguaje, en: Actas de la Conferencia AAAI sobre Inteligencia

los sesgos lineales permiten la extrapolación de la longitud de entrada, en: Conferencia

Artificial, vol. 36, 2022, págs. 11547­11555. 2, 21 [44] G. Xiao, J. Lin, M. Seznec,

Internacional sobre Representaciones del Aprendizaje, 2022.

H. Wu, J. Demouth, S. Han, Smoothquant: Cuantización posterior al entrenamiento precisa y

URL https://openreview.net/forum?id=R8sQPpGCv0 4, 17 [66] J. Su, Y. Lu, S. Pan, A.
Murtadha, B. Wen, Y. Liu, Roformer: Transformador mejorado con incrustación de posición giratoria,

eficiente para modelos de lenguaje grandes, en: ICML, vol. 202 de Actas de investigación
sobre aprendizaje automático, PMLR, 2023, págs. 38087–38099. 2, 20 [45] C. Tao, L. Hou,

preimpresión de arXiv arXiv:2104.09864 (2021). 4, 9, 17 [67] R. Child, S. Gray, A. Radford, I.
Sutskever, Generación de secuencias largas

W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, N. Wong,
Compresión de modelos de lenguaje generativo previamente entrenados mediante cuantificación ,

con transformadores dispersos, preimpresión de arXiv arXiv:1904.10509 (2019). 4, 7, 23

preimpresión de arXiv arXiv:2203.10705 (2022). 2, 20 [46] A. Pal, D. Karkhanis, M. Roberts,
S. Dooley, A. Sundararajan, S. Naidu, Giraffe: Adventures in
[68] T. Dao, D. Fu, S. Ermon, A. Rudra, C. Ré, Flashattention: atención exacta rápida y eficiente en

expanding context lengths in llms, preimpresión de arXiv arXiv:2308.10882 (2023). 2, 17 [47] B.
Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn: Extensión eficiente de ventana de contexto de

memoria con io­awareness, Avances en sistemas de procesamiento de información neuronal

modelos de lenguaje grandes,

35 (2022) 16344–16359 . 4 [69] K. Hornik, M. Stinchcombe, H. White, Las

preimpresión de arXiv arXiv:2309.00071 (2023). 2, 17 [48] M. Guo, J. Ainslie, D. Uthus, S. Ontanon,

redes de avance multicapa son aproximadores universales, Redes neuronales 2 (5) (1989) 359–366.
4 [70] V. Nair, GE Hinton, Las unidades lineales rectificadas mejoran las máquinas Boltz­

J. Ni, Y.­H. Sung, Y. Yang, Longt5: Transformador eficiente de texto a texto para secuencias
largas, preimpresión de arXiv

mann restringidas, en: Actas de la 27ª conferencia internacional sobre aprendizaje automático

arXiv:2112.07916 (2021). 2, 17

(ICML­10), 2010, págs. 4 [71] D. Hendrycks, K. Gimpel, unidades lineales de error gaussiano
(gelus), arXiv
preimpresión arXiv:1606.08415 (2016). 4

[49] S. Chen, S. Wong, L. Chen, Y. Tian, Ampliación de la ventana de contexto de modelos de
lenguaje grandes mediante interpolación posicional, preimpresión de arXiv arXiv:2306.15595

[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Abandono: una forma

(2023). 2, 17

sencilla de evitar el sobreajuste de las redes neuronales, The Journal of Machine Learning

[50] WX Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al.,

Research 15 (1) (2014) 1929– 1958. 4 [73] D. Krueger, T. Maharaj, J. Kramár, M.

Una encuesta de grandes modelos de lenguaje, preimpresión de arXiv arXiv:2303.18223

Pezeshki, N. Ballas, NR

(2023). 2, 3, 7

Ke, A. Goyal, Y. Bengio, A. Courville, C. Pal, Zoneout: Regularización de rnns mediante la

[51] U. Naseem, I. Razzak, SK Khan, M. Prasad, Una encuesta exhaustiva sobre los modelos de

preservación aleatoria de activaciones ocultas, preimpresión de arXiv arXiv:1606.01305
(2016). 4 [74] N. Shazeer, Las

representación de palabras: desde los modelos de lenguaje de representación de palabras

variantes de Glu mejoran el transformador, preimpresión de arXiv arXiv:2002.05202 (2020). 4 [75]

clásicos hasta los más modernos, Transacciones en países asiáticos y de bajos recursos.

YN Dauphin, A. Fan, M. Auli, D.

Procesamiento de información lingüística 20 (5) (2021) 1–35. 2, 3 [52] B. Min, H. Ross,
E. Sulem, APB Veyseh, TH Nguyen, O. Sainz, E. Agirre, I. Heinz, D. Roth, Avances recientes en el

Grangier, Modelado de lenguaje con redes convolucionales cerradas, en: Conferencia internacional

procesamiento del lenguaje natural a través de grandes Modelos de lenguaje previamente

sobre aprendizaje automático, PMLR, 2017, págs. 4

entrenados: una encuesta, preimpresión de arXiv arXiv:2111.01243 (2021). 2, 3 [53] C. Zhou,
Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K.

[76] JL Ba, JR Kiros, GE Hinton, Normalización de capas, preimpresión de arXiv arXiv:1607.06450
(2016). 4 [77] B. Zhang, R. Sennrich,

Zhang, C. Ji, Q. Yan, L. He, et al., Un estudio completo sobre modelos básicos previamente
entrenados: una historia de bert a chatgpt, preimpresión de arXiv arXiv:2302.09419 (2023).

Normalización de la capa cuadrática media, Avances en los sistemas de procesamiento de

2, 3

información neuronal 32 (2019). 4 [78] A. Baevski, M. Auli,
Representaciones de entrada adaptativas para el modelado del lenguaje neuronal, preimpresión de

[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, Z. Sui, Una encuesta para el

arXiv arXiv:1809.10853 (2018). 4 [79] H. Wang, S. Ma, L. Dong, S.
Huang, D. Zhang, F. Wei, Deepnet: Scaling transformadores a 1000 capas, preimpresión de arXiv

aprendizaje en contexto, preimpresión de arXiv arXiv :2301.00234 (2022). 2, 7, 17 [55] J.
Huang, KC­C. Chang, Hacia el razonamiento

arXiv:2203.00555 (2022). 4 [80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B.

en modelos de lenguaje grandes: una encuesta, preimpresión de arXiv arXiv:2212.10403 (2022). 2,

Catanzaro, Megatron­lm: Entrenamiento de modelos de lenguaje de parámetros multimillonarios

7, 17 [56] Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X.

utilizando el paralelismo de modelos, preimpresión de arXiv arXiv:1909.08053 (2019 ). 4, 5
[81] "bmtrain: Entrenamiento eficiente para grandes modelos".

Jiang, Q. Liu, Alineación de modelos de lenguaje grandes con humanos : Una encuesta, preimpresión
de arXiv arXiv:2307.12966 (2023). 2

URL https://github.com/OpenBMB/BMTrain 4, 5 [82] T. Wolf, L. Debut,
[57] X. Zhu, J. Li, Y. Liu, C. Ma, W. Wang, Una encuesta sobre la compresión de modelos para

V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cis­tac, T. Rault, R. Louf, M. Funtowicz, et al. .,

modelos de lenguaje grandes, preimpresión de arXiv arXiv:2308.07633 (2023). 2 [58] S.

Transformers: Procesamiento del lenguaje natural de última generación, en: Actas de la

Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, Una encuesta sobre modelos de lenguajes

conferencia de 2020 sobre métodos empíricos en el procesamiento del lenguaje natural:

grandes multimodales, preimpresión de arXiv arXiv:2306.13549 (2023) . 2, 22

demostraciones del sistema, 2020, págs. 5 [83] J. Bradbury, R. Frostig, P. Hawkins, MJ
Johnson, C. Leary, D. Maclau­rin, G.

[59] JJ Webster, C. Kit, Tokenización como fase inicial en PNL, en: COL­ING 1992 volumen 4: La

Necula, A. Paszke, J. VanderPlas, S. Wanderman­Milne, et al. , Jax: transformaciones componibles
de programas python+ numpy (2018). 5

14ª conferencia internacional sobre lingüística computacional, 1992. 4 [60] T. Kudo,
Regularización de subpalabras:
Mejora de los modelos de traducción de redes neuronales con múltiples subpalabras candidatas, en:
Actas de la 56.ª reunión anual de la Asociación de Lingüística Computacional (Volumen 1:

[84] S. Li, J. Fang, Z. Bian, H. Liu, Y. Liu, H. Huang, B. Wang, Y. You, Colossal­ai: un sistema

artículos extensos), 2018, págs. 4

unificado de aprendizaje profundo para trenes paralelos a gran escala ­ing, preimpresión de
arXiv arXiv:2110.14883 (2021). 5

[61] R. Sennrich, B. Haddow, A. Birch, Traducción automática neuronal de palabras raras con

[85] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, J. Tang, Fastmoe: un sistema rápido de formación mixta

unidades de subpalabras, en: Actas de la 54ª Reunión Anual de la Asociación de Lingüística

de expertos, preimpresión de arXiv arXiv:2103.13262 (2021). 5

Computacional (Volumen 1: Artículos extensos), 2016 , págs. 1715­1725. 4 [62] M. Schuster,
K. Nakajima, Búsqueda de voz en japonés

[86] L. Huawei Technologies Co., Marco de desarrollo de inteligencia artificial de Huawei mindspore,

y coreano, en: Conferencia internacional IEEE de 2012 sobre acústica, habla y procesamiento de

en: Tecnología de inteligencia artificial, Springer, 2022, págs. 137 a 162. 5

señales (ICASSP), IEEE, 2012, págs. 4 [63] SJ Mielke, Z. Alyafeai, E. Salesky, C. Raffel, M.
Dey, M. Gallé, A. Raja, C. Si, WY Lee, B. Sagot, et al.,

[87] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N.

Entre palabras y caracteres ­acters: Una breve historia del modelado de vocabulario abierto y la

Gimelshein, L. Antiga y otros, Pytorch: An Estilo imperativo, biblioteca de aprendizaje

tokenización en PNL, preimpresión de arXiv arXiv:2112.10508 (2021). 4

profundo de alto rendimiento, Avances en sistemas de procesamiento de información
neuronal 32 (2019). 5 [88] M. Abadi, P. Barham, J. Chen,
Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al., Tensorflow: un sistema

[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, Ł. Kaiser, I. Polosukhin,

para el aprendizaje automático a gran escala., en: Osdi, vol. 16, Savannah, GA, EE. UU.,

La atención es todo lo que necesita, Avances en los sistemas de procesamiento de

2016, págs. 265–283. 5 [89] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,

información neuronal 30 (2017). 4, 7
[65] O. Press, N. Smith, M. Lewis, Entrene corto, pruebe largo: atención con

36

Machine Translated by Google

B. Xu, C. Zhang, Z. Zhang, Mxnet: Una biblioteca de aprendizaje automático flexible

[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y. Cen, X. Zou, Z. Yang, J. Tang, Wudaocorpora:

y eficiente para sistemas distribuidos heterogéneos, preimpresión de arXiv

un corpus chino a gran escala para pre ­modelos de lenguaje de entrenamiento, AI

arXiv:1512.01274 (2015). 5 [90]

Open 2 (2021) 65–68. 8, 30

W. Fedus, B. Zoph, N. Shazeer, Transformadores de conmutación: escalado a modelos de

[110] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu,

parámetros de billones con escasez simple y eficiente, The Journal of Machine

et al., Ernie 3.0: El conocimiento a gran escala mejoró la capacitación previa para la

Learning Research 23 (1) (2022) 5232–5270. 5, 9 [91] N. Du, Y. Huang,

comprensión y generación del lenguaje, preimpresión de arXiv arXiv:2107.02137
(2021). 8, 25 [111] Z. Dai, Z. Yang, Y.

AM Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, AW Yu, O. Firat, et al., Glam:
Escalado eficiente de modelos de lenguaje con una combinación de expertos, en:

Yang, J. Carbonell, QV Le, R. Salakhutdinov, Transformer­xl: Modelos de lenguaje atento más

Conferencia internacional sobre aprendizaje automático, PMLR, 2022, págs. 5, 9, 23,

allá de un contexto de longitud fija, preimpresión de arXiv arXiv:1901.02860 (2019).
8 [112] O. Lieber, O. Sharir, B. Lenz, Y. Shoham,
Jurassic­1: Detalles técnicos

25 [92] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P.
Li, X. Zhang, A. Podolskiy, G. Arshinov, et al., Pangu­: Hacia un modelo de lenguaje de

y evaluación, Libro Blanco. Laboratorios AI21 1 (2021). 8, 23, 25

billones de parámetros con computación heterogénea dispersa, preimpresión de arXiv

[113] Y. Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Límites de la eficiencia profunda de

arXiv:2303.10845 (2023). 5, 10, 11, 23, 25 [93] T. Wang, A. Roberts, D. Hesslow, T.
Le Scao, HW Chung, I. Beltagy, J. Launay, C. Raffel, ¿Qué

la autoatención, Avances en los sistemas de procesamiento de información neuronal

arquitectura de modelo de lenguaje y preentrenamiento ­¿El objetivo funciona mejor para la

33 (2020) 22640–22651. 8, 11
[114] B. Kim, H. Kim, S.­W. Lee, G. Lee, D. Kwak, DH Jeon, S. Park, S. Kim, S. Kim, D. Seo,

generalización de tiro cero?, en: Conferencia internacional sobre aprendizaje
automático, PMLR, 2022, págs. 5 [94] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y.

et al., ¿Qué cambios pueden traer los modelos lingüísticos a gran escala? Estudio

Wang, J. Gao, M. Zhou, H.­W. Hon, Entrenamiento previo del modelo de lenguaje

intensivo sobre hiperclova: transformadores preentrenados generativos coreanos a

unificado para la comprensión y generación del lenguaje natural, Avances en sistemas de

escala de miles de millones, preimpresión de arXiv arXiv:2109.04650 (2021). 8, 25
[115] S. Wu, X.

procesamiento de información neuronal 32 (2019). 6 [95] J. Kaplan, S. McCandlish, T.
Henighan, TB Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, D. Amodei,

Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo, L. Xu, et al., Yuan 1.0: modelo

Leyes de escala para modelos

de lenguaje preentrenado a gran escala en aprendizaje de pocas y cero oportunidades,

de lenguaje neuronal, Preimpresión de arXiv arXiv:2001.08361 (2020). 6 [96] J. Hoffmann,

preimpresión de arXiv arXiv:2110.04725 (2021). 8, 23, 25

S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, LA
Hendricks, J. Welbl, A. Clark, et al., Entrenamiento de modelos

[116] JW Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S.

de lenguaje grande con cálculo óptimo, preimpresión de arXiv arXiv:2203.15556 (2022). 6,
9, 25, 29

Henderson, R. Ring, S. Young, et al., Scaling language Modelos: métodos, análisis y
conocimientos de Training Gopher, preimpresión de arXiv arXiv:2112.11446 (2021).
8, 9, 25, 28 [117] S. Smith, M. Patwary, B. Norick, P.
LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, et

[97] S. Iyer, XV Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu,

al., Uso de deepspeed y megatron para entrenar megatron­turing nlg 530b, un modelo

PS Koura y otros, Opt­iml: Scaling Metaaprendizaje de instrucción de modelos de

de lenguaje generativo a gran escala, preimpresión de arXiv arXiv:2201.11990 (2022).

lenguaje a través de la lente de la generalización, preimpresión de arXiv

8, 9, 23, 25

arXiv:2212.12017 (2022). 7, 11, 17, 22, 25, 28
[98] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, C. Gan, Autoalineación

[118] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K.

de modelos de lenguaje basada en principios desde cero con una mínima supervisión

McDonell, J. Phang, et al., Gpt­neox ­20b: Un modelo de lenguaje autorregresivo de

humana , preimpresión de arXiv arXiv:2305.03047 (2023). 7, 16

código abierto, preimpresión de arXiv arXiv:2204.06745 (2022). 9, 22, 23, 24, 25 [119]
W. Ben, K. Aran, Gpt­j­6b:
Un modelo de lenguaje autorregresivo de 6 mil millones de parámetros (2021). 9 [120] P.

[99] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B.
Mann, N. DasSarma, et al., Un lenguaje general asistente como laboratorio de

Micikevicius, S. Narang, J.

alineación, preimpresión de arXiv arXiv:2112.00861 (2021).
7

Alben, G. Diamos, E. Elsen, D. García, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh,
et al., Pre mixto ­entrenamiento de precisión, preimpresión de arXiv arXiv:1710.03740

[100] DM Ziegler, N. Stiennon, J. Wu, TB Brown, A. Radford, D. Amodei, P. Christiano, G.

(2017). 9, 23 [121] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le,
G. Hin­ton, J. Dean, Redes neuronales escandalosamente grandes: la mezcla de expertos

Irving, Ajuste de modelos de lenguaje a partir de preferencias humanas, preimpresión
de arXiv arXiv:1909.08593 ( 2019). 7 [101] S. Kim, SJ Joo, D.

escasamente cerrada capa, preimpresión de arXiv arXiv:1701.06538 (2017). 9, 23

Kim, J. Jang, S. Ye, J. Shin, M. Seo, The cot collection: Mejora del aprendizaje de modelos

[122] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H. Khan, C.
Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, et al., Alex­atm 20b: Aprendizaje en pocas

lingüísticos de tiro cero y de pocos tiros mediante Ajuste de la cadena de pensamiento,
preimpresión de arXiv arXiv:2305.14045 (2023). 7, 11

oportunidades utilizando un modelo seq2seq multilingüe a gran escala, preimpresión
de arXiv arXiv:2208.01448 (2022). 9, 22, 23, 24, 25 [123] R. Anil, AM Dai, O. Firat, M.
Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen ,

[102] Q. Liu, F. Zhou, Z. Jiang, L. Dou, M. Lin, De cero a héroe: examen del poder de las
tareas simbólicas en el ajuste de instrucciones, preimpresión de arXiv arXiv:2304.07995
(2023). 7, 11 [103] J. Wei, X. Wang,

et al., informe técnico de Palm 2, preimpresión de arXiv arXiv:2305.10403 (2023). 9, 25 [124]
Y. Tay, J. Wei, HW Chung, VQ Tran, DR So, S. Shakeri, X. García, HS Zheng, J. Rao,
A. Chowdhery, et al., Trascender las leyes de escala

D. Schuurmans, M. Bosma, F. Xia, E. Chi, QV Le, D. Zhou, et al., La cadena de pensamiento
que incita al razonamiento en modelos de lenguaje grandes, Avances en sistemas de

con 0,1 % de cálculo adicional, preimpresión de arXiv arXiv:2210.11399 (2022). 9, 23, 25

procesamiento de información neuronal 35 (2022) 24824–24837. 7, 19, 22
[104] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd­hery, D. Zhou,
La autoconsistencia mejora el razonamiento en cadena de pensamiento en modelos

[125] Y. Tay, M. Dehghani, VQ Tran, X. García, J. Wei, X. Wang, HW

de lenguaje, arXiv preimpresión arXiv:2203.11171 (2022). 7, 19 [105] S. Yao,
D. Yu, J. Zhao, I. Shafran, TL Griffiths, Y. Cao, K. Narasimhan, Árbol de pensamientos:

Chung, D. Bahri, T. Schuster, S. Zheng, et al., Ul2: Unificando paradigmas de
aprendizaje de idiomas, en: The Undécima Conferencia Internacional sobre

resolución deliberada de problemas con modelos de lenguaje grandes, preimpresión

Representaciones del Aprendizaje, 2022. 9, 10, 23, 24, 25

de arXiv arXiv :2305.10601 (2023). 7, 19 [106] N. Houlsby,

[126 ] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, J. Tang, Glm: Entrenamiento previo

A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, S.

del modelo de lenguaje general con relleno autorregresivo en blanco, en: Actas de la

Gelly, Aprendizaje por transferencia eficiente de parámetros para PNL, en :

60ª Reunión Anual de la Asociación de Lingüística Computacional (Volumen 1:

Conferencia internacional sobre aprendizaje automático, PMLR, 2019, págs. 2790–

Artículos extensos), 2022, págs. 320–335. 10 [127] H. Touvron, T. Lavril, G.
Izacard, X. Martinet, M.­A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et

2799. 7, 20 [107] S. McCandlish,
J. Kaplan, D. Amodei, OD Team, Un modelo empírico de entrenamiento de lotes grandes,

al., Llama: Modelos lingüísticos básicos abiertos y eficientes, preimpresión de arXiv

preimpresión de arXiv arXiv:1812.06162 (2018). 7 [108] W. Zeng, X. Ren, T.

arXiv:2302.13971 (2023). 10, 22, 25

Su, H. Wang, Y. Liao, Z. Wang, X. Jiang, Z. Yang, K. Wang, X. Zhang, et al., Pangu­ α:
Modelos de idioma chino autorregresivos a gran escala previamente entrenados con

[128] MN Rabe, C. Staats, La autoatención no necesita memoria o(n2), arXiv
preimpresión arXiv:2112.05682 (2021). 10

cálculo automático paralelo, preimpresión de arXiv arXiv:2104.12369 (2021). 8, 22,
23, 25

[129] VA Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch,

37

Machine Translated by Google

arXiv:2304.06975 (2023). 16

M. Shoeybi, B. Catanzaro, Reducción del recálculo de activación en modelos de
transformadores grandes, Proceedings of Machine Learning and Systems 5 (2023). 10

[153] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, D. Jiang, Wizardlm:
Empoderamiento de modelos de lenguaje grandes para seguir instrucciones complejas,
preimpresión de arXiv arXiv:2304.12244 (2023). 16 [154] Z. Luo, C.

[130] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, C. Xiong, Codegen:
Un modelo de lenguaje grande abierto para código con síntesis de programas de

Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, D. Jiang, Wizardcoder: Código

múltiples turnos , preimpresión de arXiv arXiv:2203.13474 (2022). 10, 22, 25, 28

potenciador del lenguaje grande modelos con evol­instruct, preimpresión de arXiv

[131] M. Chen, J. Tworek, H. Jun, Q. Yuan, HP d. O. Pinto, J. Kaplan, H. Ed­wards, Y. Burda, N.

[155] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M. Chadwick, M. Glaese, S. Young,

arXiv:2306.08568 (2023). 16, 25
Joseph, G. Brockman, et al., Evaluación de modelos de lenguaje grandes entrenados

L. Campbell­Gillingham, G. Irving y otros, Teach ­ing modelos de lenguaje para respaldar

en código, preimpresión de arXiv arXiv:2107.03374 (2021). 10, 25, 29

respuestas con citas verificadas, preimpresión de arXiv arXiv:2203.11147 (2022). 16

[132] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F.

[156] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W.

Gimeno, A. Dal Lago, et al., Competencia­ generación de código de nivel con código

Saunders, et al., Webgpt: Navegador ­Respuesta de preguntas asistida con

alfabético, Science 378 (6624) (2022) 1092–1097. 10, 23, 25, 29 [133] N. Shazeer,

retroalimentación humana, preimpresión de arXiv arXiv:2112.09332 (2021). 16, 18, 19,

Decodificación

25, 31 [157] A. Glaese, N. McAleese, M. Tr˛ebacz, J.
Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al., Mejora

rápida del transformador: un cabezal de escritura es todo lo que necesita, preimpresión de arXiv
arXiv:1911.02150 (2019). 10 [134] RY Pang, H. He,

de la alineación de los agentes de diálogo mediante juicios humanos específicos,

Generación de texto mediante el aprendizaje de demostraciones, preimpresión de arXiv

preimpresión de arXiv arXiv:2209.14375 (2022). 16, 19, 25 [158] R. Rafailov, A. Sharma,

arXiv:2009.07839 (2020). 10 [135] R. Dabre, A. Fujita,

E. Mitchell, S. Ermon, CD Manning, C. Finn, Optimización

Templado Softmax para el entrenamiento de modelos de traducción automática neuronal,

de preferencias directas: su modelo de lenguaje es secretamente un modelo de recompensa,

preimpresión de arXiv arXiv:2009.09372 (2020). 10 [136] Y. Wang, W. Wang,

preimpresión de arXiv arXiv: 2305.18290 (2023). 16

S. Joty, SC Hoi, Codet5: Modelos de codificador­decodificador preentrenados unificados con
reconocimiento de identificador para la comprensión y generación de código,

[159] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, T. Zhang, Raft: Ajuste

preimpresión de arXiv arXiv:2109.00859 (2021). 10 [137] R.

fino clasificado como recompensa para la alineación del modelo de base generativa,

Li, LB Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al.,

preimpresión de arXiv arXiv: 2304.06767 (2023). 16 [160] Z. Yuan, H. Yuan,

Starcoder: mayo ¡La fuente esté contigo!, preimpresión de arXiv arXiv:2305.06161 (2023).

C. Tan, W. Wang, S. Huang, F. Huang, Rrhf: Clasificar las respuestas para alinear los modelos

10, 25

de lenguaje con la retroalimentación humana sin lágrimas, preimpresión de arXiv
arXiv:2304.05302 (2023) . 16 [161] F. Song, B. Yu, M.

[138] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V.
Kerkez, R. Stojnic, Galactica: A Large Language Model for Science, preimpresión de

Li, H. Yu, F. Huang, Y. Li, H. Wang, Optimización de clasificación de preferencias para la

arXiv arXiv:2211.09085 (2022). 10, 23, 25, 29 [139] Autores de FairScale,

alineación humana, preimpresión de arXiv arXiv:2306.17492 (2023). 16

Fairscale: una biblioteca pytorch modular de propósito general para alto rendimiento y capacitación
[162] H. Liu, C. Sferrazza, P. Abbeel, Los idiomas son recompensas: ajuste en retrospectiva

a gran escala, https://github.com/ facebookresearch/fairscale (2021). 10 [140] R.
Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A.

utilizando la retroalimentación humana, preimpresión de arXiv arXiv:2302.02676 (2023).
16

Kulshreshtha, H.­T.

[163] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A.

Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al., Lamda: Modelos de lenguaje para

Mirhoseini, C. McKinnon, et al., Constitucional ai: Inocuidad de la retroalimentación de

aplicaciones de diálogo, preimpresión de arXiv arXiv:2201.08239 (2022). 11, 25 [141] S.

la IA, preimpresión de arXiv arXiv:2212.08073 (2022). 16 [164] Y. Dubois, X. Li, R. Taori,

Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, G.

T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, TB Hashimoto, Alpacafarm: un marco de

Mann, Bloomberggpt: Un modelo de lenguaje grande para las finanzas , preimpresión de
arXiv arXiv:2303.17564 (2023). 11, 25, 32 [142] X. Zhang, Q. Yang, D. Xu, Xuanyuan

simulación para métodos que aprenden a partir de comentarios humanos, preimpresión
de arXiv arXiv:2305.14387 (2023). 16

2.0: Un gran modelo de chat financiero chino con cientos de miles de millones de parámetros,
preimpresión de arXiv arXiv:2305.12002 (2023). 11, 16, 25

[165] C. Si, Z. Gan, Z. Yang, S. Wang, J. Wang, J. Boyd­Graber, L. Wang, Incitando a gpt­3 a ser
[143] W. Ben, Mesh­transformer­jax: Implementación paralela del modelo del modelo de lenguaje

confiable, preimpresión de arXiv arXiv:2210.09150 (2022). 16

transformador con jax (2021). 12, 23 [144] N. Muennighoff,

˙
[166] D. Ganguli, A. Askell, N. Schiefer, T. Liao, K. Lukošiut¯ e, A. Chen, A. Goldie, A. Mirhoseini,

T. Wang, L. Sutawika, A. Roberts, S. Biderman, TL Scao, MS Bari, S. Shen, Z.­X. Yong, H.
Schoelkopf, et al., Generalización multilingüe mediante ajuste multitarea, preimpresión

C. Olsson, D. Hernandez, et al., The capacidad de autocorrección moral en modelos de

de arXiv arXiv:2211.01786 (2022). 11, 25, 28, 31

lenguaje grandes, preimpresión de arXiv arXiv:2302.07459 (2023). 16

[145] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.­W. Chang, Dynosaur: Un paradigma

[167] A. Wei, N. Haghtalab, J. Steinhardt, Jailbroken: ¿Cómo falla el entrenamiento de seguridad

de crecimiento dinámico para la curación de datos de ajuste de instrucciones,

en películas?, preimpresión de arXiv arXiv:2307.02483 (2023). 16

preimpresión de arXiv arXiv:2305.14327 (2023). 16 [146] P.

[168] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N.

Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu, C. He, X. Yue, et al., Llama­

Schiefer, K. Ndousse, et al., Red teaming lan ­Modelos de lenguaje para reducir los

adaptador v2: modelo de instrucción visual eficiente en parámetros, preimpresión de

daños: métodos, comportamientos de escalamiento y lecciones aprendidas, preimpresión

arXiv arXiv:2304.15010 (2023). 16, 24 [147] Abreai. Informe técnico gpt­4 (2023).

de arXiv arXiv:2209.07858 (2022). 16, 28
[169] S. Casper, J. Lin, J. Kwon, G. Culp, D. Hadfield­Menell, Explorar, establecer, explotar:

16, 34 [148] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C.
Guestrin, P. Liang, TB Hashimoto, Stanford alpaca: un modelo de llama que sigue instrucciones,

modelos de lenguaje de equipos rojos desde cero, preimpresión de arXiv arXiv:2306.09442
(2023). 16

https:/ /github.com/tatsu­lab/stanford_alpaca (2023). 16, 25, 28 [149] W.­L. Chiang, Z. Li,
Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, JE González, I.

[170] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, G.

Stoica, EP Xing,

Irving, Red combinando modelos de lenguaje con modelos de lenguaje, preimpresión de
arXiv arXiv :2202.03286 (2022). 16 [171] T. Scialom, T.

Vicuña: un código abierto chatbot impresiona a gpt­4 con una calidad de chatgpt del 90%* (Marzo
de 2023).

Chakrabarty, S. Muresan, Los modelos de lenguaje perfeccionados aprenden continuamente, en:
Actas de la Conferencia de 2022 sobre métodos empíricos en el procesamiento del
lenguaje natural, 2022, págs. 16 [172] Z. Shi, A. Lipani, ¿No dejas de entrenar
previamente? Haga que el ajuste fino basado en indicaciones sea un alumno potente,

URL https://lmsys.org/blog/2023­03­30­vicuña/ 16, 22, 25, 28

preimpresión de arXiv arXiv:2305.01711 (2023). 17 [173] H. Gupta, SA Sawant, S.
[150] B. Peng, C. Li, P. He, M. Galley, J. Gao, Ajuste de instrucciones con gpt­4, preimpresión de

Mishra, M. Nakamura, A. Mitra, S. Mashetty, C. Baral, Los modelos adaptados a las instrucciones

arXiv arXiv:2304.03277 (2023). 16, 28 [151] T. Liu, BKH

aprenden rápido, preimpresión de arXiv arXiv:2306.05539 (2023). 17

Low, Cabra: La llama afinada supera al gpt­4 en
tareas aritméticas, preimpresión de arXiv arXiv:2305.14201 (2023). 16 [152]

[174] H. Chen, Y. Zhang, Q. Zhang, H. Yang, X. Hu, X. Ma, Y. Yanggong, J. Zhao, Quizás solo

H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo: Tuning llama model with Chinese

se necesite un 0,5% de datos: una exploración preliminar de datos de entrenamiento

Medical Knowledge, preimpresión de arXiv

bajos ajuste de instrucciones, preimpresión de arXiv arXiv:2305.09246

38

Machine Translated by Google

(2023). 17

[197] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, W. Chen, ¿Qué constituye buenos ejemplos en

[175] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al., Lima: Menos

contexto para gpt­3?, preimpresión de arXiv arXiv:2101.06804 (2021). 18

es más para alineación, preimpresión de arXiv arXiv:2305.11206 (2023). 17, 25, 28
[198] O. Rubin, J. Herzig, J. Berant, Aprender a recuperar indicaciones para el aprendizaje en
[176] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, S. Wang, Lm­infinite: generalización de longitud

contexto, preimpresión de arXiv arXiv:2112.08633 (2021). 18 [199] W. Shi, S.

simple sobre la marcha para modelos de lenguaje grandes, preimpresión de arXiv arXiv:

Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle­moyer, W.­t. Yih, Replug: modelos de

2308.16137 (2023). 17

lenguaje de caja negra con recuperación aumentada, preimpresión de arXiv arXiv:2301.12652
(2023). 18 [200] O. Rubin, J. Berant, Modelado de lenguaje de largo

[177] J. Ainslie, T. Lei, M. de Jong, S. Ontañón, S. Brahma, Y. Zemlyan­skiy, D. Uthus, M. Guo, J.
Lee­Thorp, Y. Tay, et al. ., Colt5: Transformadores de largo alcance más rápidos con cálculo

alcance con autorrecuperación, preimpresión de arXiv arXiv:2306.13421 (2023). 18 [201] K. Guu, K.
Lee, Z. Tung, P. Pasupat, M. Chang, Entrenamiento

condicional, preimpresión de arXiv arXiv:2303.09752 (2023). 17

previo del modelo de lenguaje aumentado de recuperación, en: Conferencia internacional sobre
[178] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, F. Wei, Longnet: Escalado de

aprendizaje automático, PMLR, 2020, págs. 18 [202] S. Hofstätter, J. Chen, K. Raman, H.

transformadores a 1.000.000.000 de tokens, preimpresión de arXiv arXiv:2307.02486 (2023).

Zamani, Fid­light: Generación de texto aumentada de

17

recuperación eficiente y eficaz, en: Actas de la 46ª Conferencia Internacional ACM SIGIR sobre

[179] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, J. Jia, Longlora: Ajuste eficiente de modelos

Investigación y Desarrollo en Recuperación de información, 2023, págs. 1437­1447. 18

de lenguaje grande de contexto largo, preimpresión de arXiv arXiv :2309.12307 (2023). 17

[203] M. Komeili, K. Shuster, J. Weston, generación de diálogo aumentada por Internet,
preimpresión de arXiv arXiv:2107.07566 (2021). 18 [204] A.

[180] N. Ratner, Y. Levine, Y. Belinkov, O. Ram, I. Magar, O. Abend, E. Karpas, A. Shashua, K.

Lazaridou, E. Gribovskaya, W. Stokowiec, N. Grigorev, Modelos de lenguaje aumentados por
Internet mediante indicaciones de pocas tomas para responder

Leyton­Brown, Y. Shoham, ventanas de contexto paralelo para grandes modelos de
lenguaje, en: Actas de la 61ª Reunión Anual de la Asociación de Lingüística Computacional

preguntas en dominio abierto, preimpresión de arXiv arXiv:2203.05115 (2022). 18

(Volumen 1: Artículos extensos), 2023, págs. 17
[181] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei, Aumento de modelos de lenguaje
[205] D. Gao, L. Ji, L. Zhou, KQ Lin, J. Chen, Z. Fan, MZ Shou, Assist­gpt: un asistente multimodal

con memoria a largo plazo, preimpresión de arXiv arXiv:2306.07174 (2023). 17

general que puede planificar, ejecutar, inspeccionar y aprender. Preimpresión de arXiv
arXiv:2306.08640 (2023). 18, 19 [206] P. Lu, B. Peng, H. Cheng, M.

[182] X. Xu, Z. Gou, W. Wu, Z.­Y. Niu, H. Wu, H. Wang, S. Wang, ¡Cuánto tiempo sin verte!
conversación de dominio abierto con memoria personal a largo plazo, preimpresión de arXiv

Galley, K.­W. Chang, YN Wu, S.­C. Zhu, J. Gao, Chameleon: razonamiento compositivo plug­and­

arXiv:2203.05797 (2022). 17 [183] S. Borgeaud, A.

play con modelos de lenguaje grandes, preimpresión de arXiv arXiv:2304.09842 (2023). 18,

Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli­can, GB Van Den Driessche, J.­B. Lespiau, B.

19, 22 [207] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, MT

Damoc, A. Clark, et al., Mejora de los modelos de lenguaje mediante la recuperación de
billones de tokens, en: Conferencia internacional sobre aprendizaje automático, PMLR,

Ribeiro, Arte: Razonamiento automático de varios pasos y uso de herramientas para

2022, págs. 2206–2240. 17, 18, 33 [184] W. Zhong, L. Guo, Q. Gao, Y. Wang, Memorybank:

modelos de lenguaje grandes, preimpresión de arXiv arXiv:2303.09014 (2023). 18

Mejora de modelos

[208] C.­Y. Hsieh, S.­A. Chen, C.­L. Li, Y. Fujii, A. Ratner, C.­Y. Lee, R. Kr­ishna, T. Pfister, La

de lenguaje grandes con memoria a largo plazo, preimpresión de arXiv arXiv:2305.10250 (2023). 17

documentación de herramientas permite el uso de herramientas sin complicaciones con
modelos de lenguaje grandes, preimpresión de arXiv arXiv:2308.00675 (2023). 18 [209]
Y. Song, W. Xiong, D. Zhu, C. Li, K. Wang, Y. Tian, S. Li, Restgpt: Conexión de modelos de lenguaje

[185] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao, Reflexion: Agentes

grandes con aplicaciones del mundo real a través de API rest­ful, arXiv preimpresión
arXiv:2306.06624 (2023). 18 [210] S. Hao, T. Liu, Z. Wang, Z. Hu,

lingüísticos con aprendizaje por refuerzo verbal, preimpresión de arXiv arXiv:2303.11366 14
(2023). 17, 19 [186] C. Hu, J. Fu, C. Du, S. Luo, J. Zhao,

Toolkengpt: Aumento de modelos de lenguaje congelados con herramientas masivas mediante

H. Zhao, Chatdb: Aumento de películas con bases de datos como memoria simbólica, preimpresión

incrustaciones de herramientas, preimpresión de arXiv arXiv:2305.11554 (2023). 18

de arXiv arXiv:2306.03901 (2023 ). 17
[211] SG Patil, T. Zhang, X. Wang, JE González, Gorilla: modelo de lenguaje grande conectado con
apis masivas, preimpresión de arXiv arXiv:2305.15334 (2023). 18

[187] Z. Jiang, FF Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi­Yu, Y. Yang, J. Callan, G. Neubig, generación
aumentada de recuperación activa, preimpresión de arXiv arXiv:2305.06983 (2023). 17, 18
[188] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay,

[212] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, J. Zhang, Sobre la capacidad de manipulación de

A. Shashua, K. Leyton­Brown, Y. Shoham, Modelos de lenguaje aumentados de recuperación en

herramientas de modelos de lenguaje grandes de código abierto, preimpresión de arXiv
arXiv:2305.16504 (2023) . 18

contexto, preimpresión de arXiv arXiv:2302.00083 (2023). 17, 18, 33 [189] X. Li, X. Qiu, Mot:
El pensamiento previo y el recuerdo permiten que chatgpt se

[213] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, et al., Toolllm:

automejore con la memoria de los pensamientos, preimpresión de arXiv arXiv:2305.05181 (2023).

Facilitating modelos de lenguaje grandes para dominar más de 16000 API del mundo real,
preimpresión de arXiv arXiv:2307.16789 (2023). 18, 19

17
[190] D. Schuurmans, Los modelos de lenguaje grande con memoria aumentada son

[214] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, Y. Zhuang, Hugginggpt: Solv­ing ai task with chatgpt

computacionalmente universales, preimpresión de arXiv arXiv:2301.04589 (2023).

and its friends in huggingface, preimpresión de arXiv arXiv:2303.17580 (2023) . 19, 33 [215]
Y. Liang, C. Wu, T. Song, W. Wu, Y. Xia,

17 [191] A. Modarressi, A. Imani, M. Fayyaz, H. Schütze, Ret­llm: Hacia una memoria general de
lectura y escritura para modelos de lenguaje grandes, preimpresión de arXiv arXiv:2305.14322
(2023). 17

Y. Liu, Y. Ou, S. Lu, L. Ji, S. Mao, et al., Matriz de tareas. ai: completar tareas conectando modelos
básicos con millones de API, preimpresión de arXiv arXiv:2303.16434 (2023). 19

[192] S. Robertson, H. Zaragoza, et al., El marco de relevancia probabilística: Bm25 y más allá,
Foundations and Trends® in Information Re­trieval 3 (4) (2009) 333–389. 18 [193] X. Wang,
J. Wei, D. Schuurmans, Q. Le, E. Chi, D.

[216] D. Surís, S. Menon, C. Vondrick, Vipergpt: Inferencia visual mediante ejecución en Python para

Zhou, Conjuntos racionales aumentados en modelos lingüísticos, preimpresión de arXiv

el razonamiento, preimpresión de arXiv arXiv:2303.08128 (2023). 19 [217] A. Maedche,
S. Morana, S. Schacht, D. Werth, J. Krumeich, Sistemas avanzados de asistencia al usuario,

arXiv:2207.00747 (2022). 18

Ingeniería de sistemas de información y negocios 58 (2016) 367–370. 19 [218] M. Campbell,
AJ Hoane Jr, F.­h. Hsu, Azul

[194] F. Zhang, B. Chen, Y. Zhang, J. Liu, D. Zan, Y. Mao, J.­G. Lou, W. Chen, Repocoder:
finalización de código a nivel de repositorio mediante recuperación y generación iterativa,

profundo, Inteligencia artificial 134 (1­2) (2002) 57–83. 19 [219] S. Hong, X. Zheng, J. Chen, Y.
Cheng, J. Wang, C. Zhang, Z.

preimpresión de arXiv arXiv:2303.12570 (2023). 18
[195] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong, O. Kuchaiev, B. Li, C. Xiao,

Wang, SKS Yau, Z. Lin, L. Zhou, et al., Metagpt: Meta programación para un marco colaborativo

et al., ¿Debemos entrenar previamente? ¿Modelos de lenguaje autorregresivos con

multiagente, preimpresión de arXiv arXiv:2308.00352 (2023). 19

recuperación? un estudio completo, preimpresión de arXiv arXiv:2304.06762 (2023). 18
[196] L. Wang, N. Yang, F. Wei, Aprendiendo a recuperar ejemplos en contexto para modelos de

[220] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou, et al., El

lenguaje grandes, preimpresión de arXiv arXiv:2307.07164 (2023). 18

ascenso y potencial del modelo de lenguaje grande

39

Machine Translated by Google

agentes basados: una encuesta, preimpresión de arXiv arXiv:2309.07864 (2023). 19

actas de la 60.a reunión anual de la Asociación de Lingüística Computacional (Volumen

[221] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, et

2: artículos breves), 2022, págs. 20 [242] A. Razdaibiedina, Y. Mao, R. Hou, M.
Khabsa, M. Lewis, A. Almahairi, Indicaciones progresivas: aprendizaje continuo de modelos

al., Una encuesta sobre agentes autónomos basados en modelos de lenguaje grande,
preimpresión de arXiv arXiv:2308.11432 (2023). 19 [222] W. Huang, P. Abbeel,

lingüísticos, preimpresión de arXiv arXiv:2301.12314 (2023). 20

D. Pathak, I. Mordatch, Modelos de lenguaje como planificadores de tiro cero: extracción de
conocimiento procesable para agentes incorporados, en: Conferencia internacional sobre

[243] Z.­R. Zhang, C. Tan, H. Xu, C. Wang, J. Huang, S. Huang, Hacia el ajuste adaptativo de

aprendizaje automático, PMLR, 2022, págs.9118 –9147. 19

prefijos para un ajuste fino del modelo de lenguaje eficiente en parámetros, preimpresión
de arXiv arXiv:2305.15212 (2023). 20 [244] EB Zaken, S. Ravfogel, Y.

[223] S. Hao, Y. Gu, H. Ma, JJ Hong, Z. Wang, DZ Wang, Z. Hu, Razonar con el modelo de

Goldberg, Bitfit: Ajuste simple y eficiente de parámetros para modelos de lenguaje enmascarados

lenguaje es planificar con el modelo mundial, preimpresión de arXiv arXiv:2305.14992

basados en transformadores, preimpresión de arXiv arXiv:2106.10199 (2021). 20 [245] T.

(2023). 19, 33 [224] W. Yao, S. Heinecke,

Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, Llm.

JC Niebles, Z. Liu, Y. Feng, L. Xue, R. Murthy, Z. Chen, J. Zhang, D. Arpit, et al., Retroformer :

int8 (): multiplicación de matrices de 8 bits para transformadores a escala, preimpresión de arXiv

Agentes de lenguaje grandes retrospectivos con optimización de gradiente de políticas,

arXiv:2208.07339 (2022). 20, 21 [246] E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh,

preimpresión de arXiv arXiv:2308.02151 (2023). 19, 33 [225] W. Huang, F. Xia, T. Xiao, H.

Gptq: Cuantización precisa posterior al

Chan, J. Liang, P. Florence, A. Zeng, J.

entrenamiento para transformadores generativos preentrenados, preimpresión de arXiv

Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, T. Jackson, N. Brown, L. Luu, S. Levine, K.

arXiv:2210.17323 (2022). 20

Hausman, Brian Ichter, Monólogo interior : razonamiento incorporado a través de la
planificación con modelos de lenguaje, en: Sexta Conferencia Anual sobre Aprendizaje

[247] X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, X. Liu, Supresión de valores atípicos+:

de Robots, 2022.

Cuantización precisa de modelos de lenguaje grandes mediante cambios y cambios
equivalentes y óptimos. escalado, preimpresión de arXiv arXiv:2304.09145 (2023). 20

URL https://openreview.net/forum?id=3R3Pz5i0tye 19 [226] C. Jin, W. Tan, J. Yang,
[248] E. Frantar, D. Alistarh, Compresión cerebral óptima: un marco para una cuantificación y poda

B. Liu, R. Song, L. Wang, J. Fu, Alphablock: ajuste fino incorporado para el razonamiento visión­
lenguaje en la manipulación de robots, preimpresión de arXiv arXiv:2305.18898 (2023).

precisas posteriores al entrenamiento, Avances en los sistemas de procesamiento de

19, 20, 33 [227] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J.

información neuronal 35 (2022) 4475–4488. 20
[249] C. Lee, J. Jin, T. Kim, H. Kim, E. Park, Owq: Lecciones aprendidas de valores atípicos de

Tremblay, D. Fox, J. Thomason, A. Garg, Progprompt: Generación de tarea de robot situada planes
que utilizan modelos de lenguaje grandes, en: Conferencia internacional IEEE sobre

activación para la cuantificación de peso en modelos de lenguaje grandes, preimpresión

robótica y automatización (ICRA) de 2023, IEEE, 2023, págs. 19, 33

de arXiv arXiv:2306.02272 (2023). 21
[250] SJ Kwon, J. Kim, J. Bae, KM Yoo, J.­H. Kim, B. Park, B. Kim, J.­ W. Ha, N. Sung, D. Lee,
Alphatuning: adaptación eficiente de parámetros con reconocimiento de cuantificación de

[228] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.­H. Lee, MG Arenas, H.­TL

modelos de lenguaje preentrenados a gran escala, preimpresión de arXiv arXiv:2210.03858
( 2022). 21

Chiang, T. Erez, L. Hasenclever, J. Humplik, et al., Lenguaje para recompensas por la
síntesis de habilidades robóticas, preimpresión de arXiv arXiv:2306.08647 (2023). 19

[251] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Ajuste eficiente de películas

[229] X. Tang, A. Zou, Z. Zhang, Y. Zhao, X. Zhang, A. Cohan, M. Gerstein, Medagents: modelos

cuantificadas, preimpresión de arXiv arXiv:2305.14314 (2023). 21

de lenguaje grandes como colaboradores para el razonamiento médico de disparo cero,
preimpresión de arXiv arXiv:2311.10537 (2023). 19

[252] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Kr­ishnamoorthi, V.
Chandra, Llm­qat: consciente de la cuantificación sin datos formación para modelos de

[230] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang,
R. Julian y otros, Do as i puedo, no como digo: Fundamento del lenguaje en las

lenguaje grandes, preimpresión de arXiv arXiv:2305.17888 (2023). 21

posibilidades de la robótica, en: Conferencia sobre aprendizaje de robots, PMLR, 2023,
págs. 19, 33 [231] H. Ha, P. Florence, S. Song,

[253] Y. Guo, A. Yao, H. Zhao, Y. Chen, Bosquejo de redes: explotación de estructura binaria en

Ampliación y destilación: adquisición de habilidades de robot guiada por el lenguaje, preimpresión

cnns profundos, en: Actas de la Conferencia IEEE sobre visión por computadora y

de arXiv arXiv:2307.14535 (2023). 20

reconocimiento de patrones, 2017, págs.5955 –5963. 21 [254] J. Kim, JH Lee, S. Kim,
J. Park, KM Yoo, SJ Kwon, D. Lee, Ajuste fino de memoria eficiente de modelos de lenguaje

[232] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.­P. Chiu, A. Velasquez, Say­nav: Fundamentación

grandes comprimidos mediante cuantificación de enteros de menos de 4 bits, arXiv

de modelos de lenguaje grandes para la planificación dinámica de la navegación en

preimpresión arXiv:2305.14152 (2023). 21

nuevos entornos, preimpresión de arXiv arXiv:2309.04077 (2023). 20
[255] M. Sun, Z. Liu, A. Bair, JZ Kolter, Un enfoque de poda simple y eficaz para modelos de
[233] CH Song, J. Wu, C. Washington, BM Sadler, W.­L. Chao, Y. Su, Llm­planner: Planificación

lenguaje grandes, preimpresión de arXiv arXiv:2306.11695 (2023). 21

fundamentada de pocas tomas para agentes incorporados con modelos de lenguaje
grandes, preimpresión de arXiv arXiv:2212.04088 (2022). 20 [234] VS Dorbala, JF

[256] Z. Wang, J. Wohlwend, T. Lei, Poda estructurada de modelos de lenguaje grandes,
preimpresión de arXiv arXiv:1910.04732 (2019). 21 [257] L. Yin,

Mullen Jr, D. Manocha, ¿Puede un agente encarnado encontrar su "taza con forma de gato"?
Navegación de objetos de disparo cero basada en llm, preimpresión de arXiv

Y. Wu, Z. Zhang, C.­Y. Hsieh, Y. Wang, Y. Jia, M. Pechenizkiy, Y. Liang, Z. Wang, S. Liu, Valor

arXiv:2303.03480 (2023). 20

atípico de escasez por capas ponderada (búho): una salsa secreta que falta para podar

[235] C. Huang, O. Mees, A. Zeng, W. Burgard, Mapas de lenguaje visual para la navegación de

películas hasta lograr una escasez alta, preimpresión de arXiv arXiv:2310.05175 (2023). 21

robots, en: Conferencia internacional IEEE sobre robótica y automatización (ICRA) de
2023, IEEE, 2023, págs. 20

[258] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P. Luo, N. Wong, Poda estructurada para

[236] Y. Ding, X. Zhang, C. Paxton, S. Zhang, Planificación de tareas y movimientos con modelos

modelos de lenguaje generativo preentrenados eficientes, en: Hallazgos de Asociación de

de lenguaje grandes para la reorganización de objetos, preimpresión de arXiv

Lingüística Computacional: ACL 2023, 2023, págs. 10880–10895. 21 [259] J.­B. Alayrac,

arXiv:2303.06247 (2023). 20, 33 [237] X.

J. Donahue, P. Luc, A. Miech, I. Barr, Y.

Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, J. Tang, Gpt también entiende, preimpresión de

Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al., Flamingo: un modelo de lenguaje

arXiv arXiv:2103.10385 (2021). 20 [238] G. Chen, F. Liu, Z. Meng, S.

visual para el aprendizaje en pocas oportunidades, Advances in Neural Information

Liang, Revisando el ajuste eficiente de parámetros: ¿Realmente ya llegamos a ese punto?,

Processing Systems 35 (2022) 23716–23736. 21, 22 [260] J. Li, D. Li, S. Savarese, S.

preimpresión de arXiv arXiv:2202.07962 (2022). 20

Hoi, Blip­2: Entrenamiento previo de imágenes de lenguaje
con codificadores de imágenes congeladas y modelos de lenguaje grandes, preimpresión de arXiv

[239] Y. Wang, S. Mukherjee, X. Liu, J. Gao, AH Awadallah, J. Gao, Adamix: Mezcla de adaptadores

arXiv:2301.12597 (2023) . 21, 22 [261] H. Liu, C. Li, Q. Wu, YJ Lee, Ajuste de instrucciones
visuales, preimpresión de arXiv arXiv:2304.08485 (2023).

para el ajuste eficiente de parámetros de modelos de lenguaje grandes, preimpresión de
arXiv arXiv:2205.12410 1 (2) (2022) 4. 20 [240] EJ Hu, Y. Shen, P. Wallis, Z. Allen­

21, 22 [262] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, Y. Qiao, Videochat:
comprensión de vídeo centrada en el

Zhu, Y. Li, S. Wang, L. Wang, W. Chen, Lora: Baja­ Adaptación de rango de modelos de lenguaje
grandes, preimpresión de arXiv arXiv:2106.09685 (2021). 20, 21, 22 [241] X. Liu, K. Ji, Y.

chat, Preimpresión de arXiv arXiv:2305.06355 (2023). 21, 22 [263] M. Maaz, H. Rasheed, S. Khan,

Fu, W. Tam, Z. Du, Z. Yang, J. Tang, P­tuning: el ajuste

FS Khan, Video­chatgpt: Hacia el desarrollo

rápido puede ser comparable al ajuste fino escalas y tareas, en: Pro­

40

Machine Translated by Google

[285] Z. Yu, J. Yu, Y. Cui, D. Tao, Q. Tian, Redes de coatención modulares profundas para la

Comprensión de video con cola a través de modelos de lenguaje y visión grandes,
preimpresión de arXiv arXiv:2306.05424 (2023). 21

respuesta visual a preguntas, en: Actas de la conferencia IEEE/CVF sobre visión y patrones

[264] H. Zhang, X. Li, L. Bing, Video­llama: un modelo de lenguaje audiovisual adaptado a

por computadora. reconocimiento, 2019, págs. 6281–6290. 22

instrucciones para la comprensión de videos, preimpresión de arXiv arXiv:2306.02858
(2023). 21

[286] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, HA Ayyubi, K.­ W. Chang, S.­F. Chang, Idealgpt:

[265] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, MD Plumbley, Y. Zou, W. Wang, Wavcaps:

Descomposición iterativa del razonamiento de la visión y el lenguaje mediante modelos de

un audio con etiqueta débil asistido por chatgpt Conjunto de datos de subtítulos para la

lenguaje grandes, preimpresión de arXiv arXiv:2305.14985 (2023). 22

investigación multimodal de audiolenguaje, preimpresión de arXiv arXiv:2303.17395 (2023).
21

[287] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y. Qiao, P. Gao, H. Li, Solicitar, generar y luego

[266] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, Z. Tu, Macaw­llm: modelado de

almacenar en caché: la cascada de modelos básicos genera pocas fortalezas. estudiantes

lenguaje multimodal con imagen, audio y video e integración de texto, preimpresión de

de tiro, en: Actas de la Conferencia IEEE/CVF sobre visión por computadora y

arXiv arXiv:2306.09093 (2023). 21 [267] D. Zhu, J. Chen, X. Shen, X.

reconocimiento de patrones, 2023, págs.
22

Li, M. Elhoseiny, Minigpt­4: Mejora de la comprensión del lenguaje visual con modelos avanzados
de lenguaje grande, preimpresión de arXiv arXiv:2304.10592 (2023). 22 [268] A. Dosovitskiy,

[288] TQ Nguyen, J. Salazar, Transformers sin lágrimas: Mejorando la normalización de la
autoatención, CoRR abs/1910.05895 (2019). 23 [289] Y. Liu, M. Ott, N. Goyal, J.

L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T.
Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., Una imagen vale 16x16 palabras:

Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov, Roberta: un bert sólidamente

Transformadores para el reconocimiento de imágenes a escala, preimpresión de arXiv

optimizado enfoque de preentrenamiento, preimpresión de arXiv arXiv:1907.11692 (2019).

arXiv:2010.11929 (2020). 22 [269] W. Dai, J. Li, D. Li, AMH Tiong, J. Zhao, W. Wang, B. Li,

24, 30 [290] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, D.

P. Fung, S. Hoi, Instructblip: Hacia modelos de visión y lenguaje

Song, Koala: un modelo de diálogo para la investigación académica, Publicación de blog (abril de

de propósito general con ajuste de instrucciones, preimpresión de arXiv arXiv:2305.06500 (2023).

2023).

22 [270] Z. Xu, Y. Shen, L. Huang, Multiinstruct: Mejorando el aprendizaje multimodal de
disparo cero mediante el ajuste de instrucciones, preimpresión de arXiv

URL https://bair.berkeley.edu/blog/2023/04/03/koala/ 25

arXiv:2212.10773 (2022). 22
[291] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N.
Nabeshima, et al., La pila: Un conjunto de datos de 800 GB de texto diverso para modelado
[271] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, J. Liu, Chatbridge: Modalidades

de lenguaje, preimpresión de arXiv arXiv:2101.00027 (2020). 28, 30 [292] H. Laurençon, L.
Saulnier, T. Wang, C. Akiki, A. Villanova

puente con un modelo de lenguaje grande como catalizador del lenguaje, Preimpresión
de arXiv arXiv:2305.16103 (2023). 22

del Moral, T. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada, H. Nguyen , et al., The

[272] L. Li, Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu, X. Sun, et al., M3 it: Un

bigscience root corpus: un conjunto de datos multilingüe compuesto de 1,6 tb, Advances in

conjunto de datos a gran escala hacia el ajuste de instrucciones multimodal y multilingüe,

Neural Information Processing Systems 35 (2022) 31809–31826. 28

preimpresión de arXiv arXiv:2306.04387 (2023). 22 [273] R. Pi, J. Gao, S. Diao, R. Pan,
H. Dong, J. Zhang, L. Yao, J. Han, H. Xu, LKT Zhang, Detgpt: Detecte lo que necesita a través de
[293] Wikipedia.

razonamiento, preimpresión de arXiv arXiv:2305.14167 (2023). 22 [274] G. Luo, Y. Zhou,
T. Ren, S. Chen, X. Sun, R. Ji, Barato y rápido: ajuste

URL https://en.wikipedia.org/wiki/Main_Page 28

eficiente de la instrucción visión­lenguaje para modelos de lenguaje grandes, preimpresión de arXiv

[294] Together Computer, Redpajama: una receta de código abierto para reproducir

arXiv:2305.15023 (2023 ). 22 [275] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li,

conjunto de datos de entrenamiento de llamas (abril de 2023).

P. Gao, Y. Qiao, Adaptador de llama: ajuste eficiente

URL https://github.com/togethercomputer/ RedPajama­Data 28 [295] O. Honovich, T.

del lenguaje modelos con atención de inicio cero, preimpresión de arXiv arXiv:2303.16199 (2023).

Scialom, O. Levy, T. Schick,

22 [276] A. Radford, JW Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever,

Instrucciones antinaturales: ajuste de modelos de lenguaje sin (casi) ninguna mano de obra

Reconocimiento de voz robusto mediante supervisión débil a gran

humana, preimpresión de arXiv arXiv:2212.09689 (2022). 28

escala, en: Conferencia internacional sobre aprendizaje automático, PMLR, 2023 , págs . 28492–
[296] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,

28518.22

T. Henighan, et al., Capacitación de un útil y asistente inofensivo con aprendizaje reforzado
a partir de comentarios humanos, preimpresión de arXiv arXiv:2204.05862 (2022). 28 [297]
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika,

[277] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, A. Smola, Razonamiento de cadena de
pensamiento multimodal en modelos de lenguaje, preimpresión de arXiv arXiv:2302.00923

D. Song, J. Steinhardt, Medición de la comprensión masiva del lenguaje multitarea, preimpresión

(2023). 22

de arXiv arXiv:2009.03300 (2020). 24, 29 [298] A. Srivastava, A. Rastogi, A. Rao, AAM
Shoeb, A. Abid, A. Fisch, AR Brown, A. Santoro, A. Gupta,

[278] J. Ge, H. Luo, S. Qian, Y. Gan, J. Fu, S. Zhan, Sintonización de la cadena de pensamiento en
modelos de lenguaje visual, preimpresión de arXiv arXiv:2304.07919 (2023). 22

A. Garriga­Alonso, et al., Más allá del juego de imitación: cuantificación y extrapolación de las
capacidades de los modelos lingüísticos, preimpresión de arXiv arXiv:2206.04615 (2022).

[279] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual chatgpt: hablar, dibujar y editar con

24, 29 [299] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, SR Bowman, Glue: una

modelos de base visual, preimpresión de arXiv arXiv:2303.04671 (2023) . 22

plataforma de análisis y referencia de múltiples tareas para la comprensión del
lenguaje natural, preimpresión de arXiv arXiv: 1804.07461 (2018). 24, 29

[280] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu, M. Zeng, L. Wang, Mm­
react: Incitar chatgpt para acción y razonamiento multimodal, preimpresión de arXiv
arXiv:2303.11381 (2023). 22 [281] T. Wang, J. Zhang, J. Fei, Y. Ge, H. Zheng,

[300] Y. Yao, Q. Dong, J. Guan, B. Cao, Z. Zhang, C. Xiao, X. Wang, F. Qi, J. Bao, J. Nie, et al.,

Y. Tang, Z. Li, M. Gao, S. Zhao, Y. Shan, et al., Título de cualquier cosa : Descripción de imagen

Cuge: A Punto de referencia de evaluación de generación y comprensión del idioma chino,
preimpresión de arXiv arXiv:2112.13610 (2021). 29

interactiva con diversos controles multimodales, preimpresión de arXiv arXiv:2305.02677
(2023). 22

[301] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu, C. Yu, et al., Pista: A Punto
[282] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, P. Gao, Pointclip v2: Adaptación de clip para un

de referencia de evaluación de la comprensión del idioma chino, preimpresión de arXiv
arXiv:2004.05986 (2020). 29 [302] L. Xu, X. Lu, C. Yuan, X.

potente aprendizaje en 3D en mundo abierto, preimpresión de arXiv arXiv:2211.11682
(2022). 22

Zhang, H. Xu, H. Yuan, G. Wei, X. Pan, X. Tian, L. Qin, et al., Fewclue: Un punto de referencia

[283] T. Gupta, A. Kembhavi, Programación visual: razonamiento visual compositivo sin formación,

chino de evaluación del aprendizaje de pocas oportunidades, preimpresión de arXiv
arXiv:2107.07498 (2021). 29

en: Actas de la Conferencia IEEE/CVF sobre visión por computadora y reconocimiento de

[303] EM Smith, M. Williamson, K. Shuster, J. Weston, Y.­L. Boureau, ¿Puedes ponerlo todo junto?

patrones, 2023, págs. 22

Evaluación de la capacidad de los agentes conversacionales para combinar habilidades,
preimpresión de arXiv arXiv:2004.08449 (2020). 29 [304] P.

[284] P. Gao, Z. Jiang, H. You, P. Lu, SC Hoi, X. Wang, H. Li, Fusión dinámica con flujo de atención
intra e intermodal para la respuesta visual a preguntas, en: Actas de la conferencia IEEE/

Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu,

CVF sobre visión por computadora y reconocimiento de patrones, 2019, págs. 6639–6648.

A. Kumar, et al., Evaluación holística de modelos de lenguaje, preimpresión de arXiv

22

arXiv:2211.09110 (2022). 29

41

Machine Translated by Google

arXiv:1908.06605 (2019). 29

[305] S. Park, J. Moon, S. Kim, WI Cho, J. Han, J. Park, C. Song, J. Kim, Y. Song, T. Oh, et al., Klue:
idioma coreano comprensión de la evaluación, preimpresión de arXiv arXiv:2105.09680 (2021).

[328] J. Novikova, O. Dušek, V. Rieser, El conjunto de datos e2e: nuevos desafíos para la generación de

29 [306] S. Reddy, D. Chen, CD Manning, Coqa: Un desafío

un extremo a otro, preimpresión de arXiv arXiv:1706.09254 (2017). 29 [329] C. Zheng, M.

de respuesta a preguntas conversacionales, Transactions of the Association for Computational Linguistics

Huang, A. Sun, Chid: un conjunto de datos idiomáticos chinos a gran escala
para la prueba de cierre, preimpresión de arXiv arXiv:1906.01265 (2019). 29

7 (2019) 249–266. 25, 29

[330] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al., Piqa: Reasoning about physi­ical commonsense in
[307] MT Pilehvar, J. Camacho­Collados, Wic: 10,000 pares de ejemplos para evaluar representaciones

natural language, en: Actas de la conferencia AAAI sobre inteligencia artificial, vol. 34, 2020,

sensibles al contexto, preimpresión de arXiv arXiv:1808.09121 6 (2018). 25, 29

págs. 7432–7439. 26, 29

[308] S. Merity, C. Xiong, J. Bradbury, R. Socher, modelos de mezcla Pointer Sentinel, preimpresión de

[331] M. Joshi, E. Choi, DS Weld, L. Zettlemoyer, Triviaqa: un conjunto de datos de desafío a gran escala

arXiv arXiv:1609.07843 (2016). 25, 29 [309] JW Rae, A. Potapenko, SM

supervisado a distancia para la comprensión lectora, preimpresión de arXiv arXiv:1705.03551
(2017). 26, 29, 31 [332] P. Clark, I. Cowhey, O. Etzioni, T.

Jayakumar, TP Lillicrap, Transformadores de compresión para modelado de secuencias de largo
alcance, preimpresión de arXiv arXiv:1911.05507 (2019). 25, 29 [310] X. Liu, Q. Chen, C. Deng,

Khot, A. Sabharwal, C. Schoenick, O. Tafjord, ¿Crees que has resuelto la respuesta a preguntas? Pruebe

H. Zeng, J. Chen, D. Li, B. Tang, Lcqmc: Un

Arc, el desafío de razonamiento ai2, preimpresión de arXiv arXiv:1803.05457 (2018). 26, 29, 31

corpus de coincidencia de preguntas chinas a gran escala, en: Proceedings of the 27.a conferencia
internacional sobre lingüística computacional, 2018, págs. 1952­1962. 26, 29
[333] S. Aroca­Ouellette, C. Paik, A. Roncone, K. Kann, Prost: Razonamiento físico de objetos a través
del espacio y el tiempo, preimpresión de arXiv arXiv:2106.03634 (2021). 29
[311] S. Iyer, N. Dandekar, K. Csernai, pares de preguntas del primer conjunto de datos de quora, https://
[334] T. Mihaylov, P. Clark, T. Khot, A. Sabharwal, ¿Puede una armadura conducir electricidad? un

quoradata.quora.com/
arrendamiento: Primeros­pares­de­preguntas­de­liberación­del­conjunto­de­datos­de­Quora. 29

nuevo conjunto de datos para responder preguntas de libro abierto, preimpresión de arXiv
arXiv:1809.02789 (2018). 29

[312] R. Rudinger, J. Naradowsky, B. Leonard, B. Van Durme, Sesgo de género en la resolución de
correferencia, preimpresión de arXiv arXiv:1804.09301 (2018). 29 [313] M.­C. De Marneffe,

[335] TC Ferreira, C. Gardent, N. Ilinykh, C. Van Der Lee, S. Mille, D. Moussallem, A. Shimorina, Resumen

M. Simons, J. Tonhauser, El banco de compromiso: investigación de la proyección en el discurso natural,

de tareas compartidas y resultados de evaluación de webnlg+ bilingüe y bidireccional de 2020

en: actas de Sinn und Bedeutung, vol. 23, 2019, págs. 107­124. 29 [314] Z. Li, N. Ding, Z. Liu,

(webnlg+ 2020) , en: Actas del 3er Taller Internacional sobre Generación de Lenguaje Natural a

H. Zheng, Y. Shen, Extracción de relaciones chinas con información

partir de la Web Semántica (WebNLG+), 2020. 29

multigrano y conocimiento lingüístico externo, en: Actas de la 57ª Reunión Anual de la Asociación de
Compu ­Lingüística nacional, 2019, págs. 4377–4386. 29 [315] J. Xu, J. Wen, X. Sun, Q. Su, Un

[336] C. Xu, W. Zhou, T. Ge, K. Xu, J. McAuley, F. Wei, Toca el silbato para perros: un conjunto de datos

conjunto de datos de extracción de relaciones y reconocimiento de entidades con nombre a

chino para la comprensión de la hipocresía con sentido común y conocimiento del mundo,

nivel de discurso para textos de literatura china, preimpresión

preimpresión de arXiv arXiv:2104.02704 (2021). 29 [337] G. Lai, Q. Xie, H.

de arXiv arXiv:1711.07010 (2017). 29

Liu, Y. Yang, E. Hovy, Race: conjunto de datos de comprensión lectora a gran escala a partir de
exámenes, preimpresión de arXiv arXiv:1704.04683 (2017). 26, 29 [338] E. Choi, H. He, M. Iyyer,
M. Yatskar, W.­t. Yih, Y. Choi, P. Liang, L.

[316] J. Chen, Q. Chen, X. Liu, H. Yang, D. Lu, B. Tang, The bq corpus: Un corpus chino específico de

Zettlemoyer, Quac: Respuesta a preguntas en contexto, preimpresión de arXiv arXiv:1808.07036 (2018).

dominio a gran escala para la identificación de equivalencias semánticas de oraciones, en:

27, 29

Actas de la conferencia de 2018 sobre métodos empíricos en el procesamiento del lenguaje
natural, 2018, págs. 4946–4951. 29 [317] B. Liu, D. Niu, H. Wei, J. Lin, Y. He, K. Lai, Y. Xu,

[339] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, J. Berant, ¿Aristóteles usó una computadora

Emparejamiento de pares de artículos con descomposición gráfica y convoluciones, preimpresión de

portátil? un punto de referencia de respuesta a preguntas con estrategias de razonamiento

arXiv arXiv:1802.07459 (2018 ). 29

implícitas, Transactions of the Association for Computational Linguistics 9 (2021) 346–361. 29

[318] P. Li, W. Li, Z. He, X. Wang, Y. Cao, J. Zhou, W. Xu, Conjunto de datos y modelo de etiquetado de

[340] J. Boyd­Graber, B. Satinoff, H. He, H. Daumé III, Besting the quiz master: Crowdsourcing de juegos

secuencia recurrente neuronal para respuesta a preguntas factoides de dominio abierto,

de clasificación incremental, en: Actas de la conferencia conjunta de 2012 sobre métodos

preimpresión de arXiv arXiv:1607.06275 (2016). 29 [319] N. Peng, M.

empíricos en procesos de lenguaje natural. Proceso y aprendizaje computacional del lenguaje

Dredze, Reconocimiento de entidad nombrada para redes sociales chinas con incrustaciones entrenadas

natural, 2012, págs. 1290­1301. 29

conjuntamente, en: Actas de la conferencia de 2015 sobre métodos empíricos en el procesamiento
del lenguaje natural, 2015, págs. 548–554. 29

[341] S. Zhang, X. Zhang, H. Wang, J. Cheng, P. Li, Z. Ding, Coincidencia de respuestas a preguntas
médicas chinas utilizando cnns multiescala de nivel de caracteres de extremo a extremo,
Ciencias Aplicadas 7 ( 8) (2017) 767. 29 [342] S. Zhang, X.

[320] W. Ling, D. Yogatama, C. Dyer, P. Blunsom, Inducción de programas por generación racional:
aprender a resolver y explicar problemas planteados algebraicos, preimpresión de arXiv

Zhang, H. Wang, L. Guo, S. Liu, Redes de interacción atenta a múltiples escalas para la selección de

arXiv:1705.04146 (2017). 29 [321] R. Weischedel, S.

respuestas a preguntas médicas chinas, IEEE Access 6 ( 2018) 74061–74071. 29

Pradhan, L. Ramshaw, M. Palmer, N. Xue, M. Mar­cus, A. Taylor, C. Greenberg, E. Hovy, R. Belvin, et
al., Ontonotes versión 4.0, LDC2011T03, Filadelfia, Pensilvania: Linguistic Data Consor­tium

[343] C. Xu, J. Pei, H. Wu, Y. Liu, C. Li, Matinf: Un conjunto de datos a gran escala etiquetado

(2011). 29 [322] D. Vilares, C. Gómez­Rodríguez, Head­qa: Un conjunto de datos sanitarios

conjuntamente para clasificación, respuesta a preguntas y resumen, preimpresión de arXiv

para el razonamiento

arXiv:2004.12302 (2020). 29

complejo, preimpresión de arXiv arXiv:1906.04701 (2019). 29 [323] SL Blodgett, L. Green, B. O'Connor,

[344] K. Sakaguchi, RL Bras, C. Bhagavatula, Y. Choi, Winogrande: Un desafío contradictorio del esquema

Variación dialectal demográfica en las redes sociales: un estudio de caso del inglés

de Winograd a escala, Communications of the ACM 64 (9) (2021) 99–106. 25, 29

afroamericano, preimpresión de arXiv arXiv:1608.08868 (2016). 29
[345] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, Y. Choi, Hellaswag: ¿Puede una máquina realmente
terminar su oración?, preimpresión de arXiv arXiv:1905.07830 (2019). 27, 29 [346] M. Roemmele,
[324] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Van­derwende, P. Kohli, J. Allen, Un

CA Bejan, AS

corpus y marco de evaluación para una comprensión más profunda de las historias de sentido

Gordon, Elección de alternativas plausibles: una evaluación del razonamiento causal de sentido común.,

común, arXiv preimpresión arXiv:1604.01696 (2016). 26, 29 [325] D. Paperno, G. Kruszewski,

en: Simposio de primavera de AAAI: formalizaciones lógicas del razonamiento de sentido común,

A. Lazaridou, QN Pham, R. Bernardi, S.

2011, págs. . 29

Pezzelle, M. Baroni, G. Boleda, R. Fernández, El conjunto de datos lambada: predicción de palabras que
requiere una amplia contexto del discurso, preimpresión de arXiv arXiv:1606.06031 (2016). 26,

[347] H. Levesque, E. Davis, L. Morgenstern, El desafío del esquema de Winograd, en: Decimotercera

29 [326] B. Hu, Q. Chen, F. Zhu, Lcsts: Un resumen breve de texto chino a gran escala.

conferencia internacional sobre los principios de representación y razonamiento del
conocimiento, 2012. 25, 27, 29 [348] A. Talmor, J. Herzig, N. Lourie,
J. Berant, Commonsenseqa: Un desafío de respuesta a preguntas dirigido al conocimiento de sentido

conjunto de datos de rización, preimpresión de arXiv arXiv:1506.05865 (2015). 29

común, preimpresión de arXiv arXiv:1811.00937 (2018). 27, 29 [349] M. Sap, H. Rashkin, D.
Chen, R. LeBras, Y. Choi, Socialiqa:

[327] Z. Shao, M. Huang, J. Wen, W. Xu, X. Zhu, Generación de texto largo y diverso con modelo
variacional jerárquico basado en planificación, preimpresión de arXiv

42

Machine Translated by Google

Razonamiento de sentido común sobre las interacciones sociales, preimpresión de arXiv

actas 4, Springer, 2013, págs. 303–320. 29 [370] S.

arXiv:1904.09728 (2019). 29

Lim, M. Kim, J. Lee, Korquad1. 0: conjunto de datos qa coreano para comprensión de lectura

[350] K. Sun, D. Yu, D. Yu, C. Cardie, Investigación del conocimiento previo para desafiar la

automática, preimpresión de arXiv arXiv:1909.07005 (2019). 29 [371] C. Xiao, H.

comprensión de la lectura automática china, Transactions of the Association for

Zhong, Z. Guo, C. Tu, Z. Liu, M. Sun, Y. Feng, X. Han, Z. Hu, H. Wang, et al., Cail2018: Un

Computational Linguistics 8 (2020) 141–155. 29 [351] S. Zhang, X. Liu, J. Liu,

conjunto de datos legales a gran escala para la predicción de sentencias, preimpresión
de arXiv arXiv:1807.02478 (2018). 29

J. Gao, K. Duh, B. Van Durme, Registro: Brindando la brecha entre la comprensión lectora de
sentido común humano y automático, preimpresión de arXiv arXiv: 1810.12885 (2018).

[372] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik,

29 [352] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad:

H. He, D. Song, et al., Desafío de codificación de medición competencia con aplicaciones,
preimpresión de arXiv arXiv:2105.09938 (2021). 28, 29 [373] Y. Wang, X. Liu, S. Shi,

más de 100.000 preguntas para la comprensión automática de texto, preimpresión de arXiv
arXiv:1606.05250 (2016). 28, 29 [353] C. Clark, K. Lee, M.­W. Chang, T. Kwiatkowski,

Solucionador neuronal profundo para problemas matemáticos planteados, en: Actas de la

M. Collins, K.

conferencia de 2017 sobre métodos empíricos en el procesamiento del lenguaje natural,

Toutanova, Boolq: Explorando la sorprendente dificultad de las preguntas naturales de sí/no,

2017, págs. 28, 29 [374] K. Cobbe, V. Kosaraju, M.

preimpresión de arXiv arXiv:1905.10044 (2019). 28, 29 [354] P. Rajpurkar, R. Jia, P.

Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano y otros,

Liang, Sepa lo que no sabe: preguntas sin respuesta para el equipo, preimpresión

Capacitación de verificadores para resolver problemas matemáticos planteados,
preimpresión de arXiv arXiv:2110.14168 (2021). 29

de arXiv arXiv:1806.03822 (2018). 28, 29

[375] J. Austin, A. Odena, MI Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, CJ Cai, M.
[355] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, M. Gardner, Drop: Un punto de

Terry, QV Le, C. Sutton, Síntesis de programas con modelos de lenguaje grandes ,

referencia de comprensión lectora que requiere razonamiento discreto sobre párrafos,

CoRRabs/2108.07732 (2021). 29 [376] F. Shi, M. Suzgun, M. Freitag,

preimpresión de arXiv arXiv:1903.00161 (2019). 28, 29

X. Wang, S. Srivats, S. Vosoughi, HW

[356] I. Dagan, O. Glickman, B. Magnini, El desafío de la vinculación textual que reconoce

Chung, Y. Tay, S. Ruder, D. Zhou, et al., Los modelos lingüísticos son razonadores de

Pascal, en: Taller de desafíos de aprendizaje automático, Springer, 2005, págs. 28, 29

cadena de pensamiento multilingües, preimpresión de arXiv arXiv:2210.03057 (2022).
29

[357] Y. Chang, M. Narang, H.
Suzuki, G. Cao, J. Gao, Y. Bisk, Webqa: Mul­tihop y multimodal qa, en: Actas de la Conferencia

[377] S. Roy, D. Roth, Resolución de problemas verbales de aritmética general, arXiv
preimpresión arXiv:1608.01413 (2016). 29

IEEE/CVF sobre Computación Reconocimiento de visión y patrones, 2022, págs. 16495–
16504. 28, 29

[378] S.­Y. Miao, C.­C. Liang, K.­Y. Su, Un corpus diverso para evaluar y desarrollar
solucionadores de problemas verbales de matemáticas en inglés, preimpresión de arXiv
arXiv:2106.15772 (2021). 29

[358] Y. Cui, T. Liu, Z. Chen, W. Ma, S. Wang, G. Hu, Conjunto de datos para la primera
evaluación sobre comprensión de lectura automática china, preimpresión de arXiv

[379] R. Koncel­Kedziorski, S. Roy, A. Amini, N. Kushman, H. Hajishirzi, Mawps: A Math Word

arXiv:1709.08299 (2017). 29

Problem Repository, en: Actas de la conferencia de 2016 del capítulo norteamericano
de la Asociación de Computación. Lingüística nacional: tecnologías del lenguaje

[359] Y. Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, G. Hu, Un conjunto de datos de
extracción de intervalos para la comprensión de lectura automática china, preimpresión

humano, 2016, págs. 1152­1157. 29

de arXiv arXiv: 1810.07366 (2018). 28, 29 [360] Y. Cui, T.
Liu, Z. Yang, Z. Chen, W. Ma, W. Che, S. Wang, G. Hu, Un conjunto de datos de cierre de

[380] A. Patel, S. Bhattamishra, N. Goyal, ¿Los modelos de PNL son realmente capaces de

oraciones para la comprensión de lectura automática china, preimpresión de arXiv

resolver problemas matemáticos simples?, preimpresión de arXiv arXiv:2103.07191
(2021). 29

arXiv:2004.03116 (2020). 29 [361] Y. Li, T. Liu, D. Li,
Q. Li, J. Shi, Y. Wang, Bilstm­crf basado en caracteres que incorpora pos y diccionarios para la

[381] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.­t. Yih, D. Fried, S. Wang, T.
Yu, Ds­1000: Un punto de referencia natural y confiable para la generación de código

extracción de objetivos de opinión chinos, en: Asian Conference on Machine Aprendizaje,
PMLR, 2018, págs. 518–533. 29

de ciencia de datos, en: Conferencia internacional sobre aprendizaje automático, PMLR,
2023, págs. 29 [382] J. Austin, A. Odena, M. Nye, M.

[362] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, D. Roth, Mirando más allá de la

Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al., Síntesis del programa

superficie: un desafío para la comprensión lectora en múltiples oraciones, en: Actas de

con modelos de lenguaje grandes, preimpresión de arXiv arXiv:2108.07732 (2021). 29

la Conferencia del Norte de 2018 Capítulo americano de la Asociación de Lingüística

[383] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, D. Kiela, Advers­
sarial nli: Un nuevo punto de referencia para la comprensión del lenguaje natural, preimpresión

Computacional: Tecnologías del lenguaje humano, volumen 1 (artículos extensos),
2018, págs. 29 [363] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C.

de arXiv arXiv:1910.14599 (2019). 29

Al­berti, D. Epstein,
I. Polosukhin, J. Devlin, K. Lee, et al., Preguntas naturales: un punto de referencia para la

[384] A. Williams, N. Nangia, SR Bowman, Un corpus de desafío de amplia cobertura para la

investigación de respuesta a preguntas, Transactions of the Association for

comprensión de oraciones a través de la inferencia, preimpresión de arXiv
arXiv:1704.05426 (2017). 29

Computational Linguistics 7 (2019) 453–466. 29 [364] CC Shao, T. Liu, Y. Lai, Y. Tseng,
S. Tsai, Drcd: Un conjunto de datos de comprensión de lectura de máquinas

[385] RT McCoy, E. Pavlick, T. Linzen, Correcto por razones equivocadas: heurística sintáctica

chinas, preimpresión de arXiv arXiv:1806.00920 (2018). 29

de diagnóstico en la inferencia del lenguaje natural, preimpresión de arXiv
arXiv:1902.01007 (2019). 29
[386] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, Y. Zhang, Logiqa: Un conjunto de datos

[365] W. He, K. Liu, J. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu, Y. Wang, H. Wu, Q. She, et al.,

desafiante para la comprensión de la lectura automática con razonamiento lógico,

Dureader: a Conjunto de datos de comprensión de lectura automática china de

preimpresión de arXiv arXiv:2007.08124 ( 2020). 29 [387]
˘
P. Lewis, B. Oguz, R. Rinott, S. Riedel, H. Schwenk, Mlqa: Evaluación de la respuesta a

aplicaciones del mundo real, preimpresión de arXiv arXiv:1711.05073 (2017). 29

preguntas extractivas en varios idiomas, preimpresión de arXiv arXiv:1910.07475 (2019).
29

[366] H. Tang, J. Liu, H. Li, Y. Hong, H. Wu, H. Wang, Dureaderrobust: Un conjunto de datos

[388] A. Conneau, G. Lample, R. Rinott, A. Williams, SR Bowman, H. Schwenk, V. Stoyanov,

chino para evaluar la solidez de los modelos de comprensión de lectura automática,
preimpresión de arXiv arXiv:2004.11142 (2020) . 29 [367] J. Welbl, NF Liu, M.

Xnli: Evaluación de representaciones de oraciones en varios idiomas, preimpresión de
arXiv arXiv:1809.05053 (2018). 29

Gardner, Preguntas científicas de opción múltiple mediante crowdsourcing, preimpresión de
arXiv arXiv:1707.06209 (2017). 29 [368] C. Xiong, Z. Dai, J. Callan,

[389] Y. Yang, Y. Zhang, C. Tar, J. Baldridge, Paws­x: Un conjunto de datos contradictorios en

Z. Liu, R. Power, Clasificación neuronal ad­hoc de extremo a extremo con agrupación de núcleos,

varios idiomas para la identificación de paráfrasis, preimpresión de arXiv arXiv:1908.11828
(2019). 29

en: Actas de la 40.ª conferencia internacional ACM SIGIR sobre investigación y
desarrollo en la recuperación de información, 2017, págs. 55–64. 29

[390] S. Narayan, SB Cohen, M. Lapata, ¡No me den los detalles, solo el resumen!, Redes
neuronales convolucionales temáticas para un resumen extremo. ArXiv, abdominales
(1808). 29

[369] A. Peñas, E. Hovy, P. Forner, Á. Rodrigo, R. Sutcliffe, R. Morante, Qa4mre 2011­2013:

´
[391] EM Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vulic, A. Korhonen, Xcopa: Un conjunto de

Descripción general de la respuesta a preguntas para la evaluación de lectura
automática, en: Evaluación de acceso a la información. Multilingüismo, multimodalidad

datos multilingüe para el razonamiento causal de sentido común, preimpresión de arXiv

y visualización: 4ª Conferencia Internacional de la Iniciativa CLEF, CLEF 2013, Valencia,

arXiv:2005.00333 (2020). 27, 29 [392] A. Tikhonov,

España, 23 al 26 de septiembre de 2013. Pro­

M. Ryabinin, Todo está en la cabeza: usar la atención

43

Machine Translated by Google

como base para la transferencia interlingüística en el razonamiento de sentido común, arXiv

[414] A. Fan, Y. Jernite, E. Pérez, D. Grangier, J. Weston, M. Auli, Eli5: Long

preimpresión arXiv:2106.12066 (2021). 29

formulario de respuesta a preguntas, preimpresión de arXiv arXiv:1907.09190 (2019). 31

[393] JH Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V. Niko­laev, J. Palomaki, Tydi qa:

[415] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,

Un punto de referencia para la respuesta a preguntas de búsqueda de información en

A. Arunkumar, A. Ashok, AS Dhanasekaran, A. Naik, D. Stap, et al.,

lenguajes tipológicamente diversos, transacciones de la

Generalización de evaluaciones comparativas a través de instrucciones en contexto en más de

Asociación de Lingüística Computacional 8 (2020) 454–470. 29

1600 tareas de lenguaje, preimpresión de arXiv arXiv:2204.07705 (2022). 31

[394] T. Scialom, P.­A. Dray, S. Lamprier, B. Piwowarski, J. Staiano,

[416] T. Xie, CH Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.­S. Wu,

Mlsum: el corpus de resumen multilingüe, preimpresión de arXiv

M. Zhong, P. Yin, SI Wang, et al., Unifiedskg: Unificación y base de conocimiento

arXiv:2004.14900 (2020). 29

estructurado multitarea con modelos de lenguaje de texto a texto, preimpresión de arXiv
arXiv:2201.05966 (2022). 31

[395] S. Lin, J. Hilton, O. Evans, Truthfulqa: Medir cómo los modelos imitan
falsedades humanas, preimpresión de arXiv arXiv:2109.07958 (2021). 29

[417] P. Ye, BY Lin, X. Ren, Crossfit: un desafío de aprendizaje de unos pocos intentos

[396] I. Augenstein, C. Lioma, D. Wang, LC Lima, C. Hansen,

para generalización de tareas cruzadas en PNL, preimpresión de arXiv arXiv:2104.08835

C. Hansen, JG Simonsen, Multifc: un multidominio del mundo real

(2021). 31
[418] V. Aribandi, Y. Tay, T. Schuster, J. Rao, HS Zheng, SV Mehta,

conjunto de datos para la verificación de hechos de afirmaciones basada en evidencia, preimpresión de arXiv

arXiv:1909.03242 (2019). 29

H. Zhuang, VQ Tran, D. Bahri, J. Ni, et al., Ext5: Hacia el extremo

[397] J. Thorne, A. Vlachos, C. Christodoulopoulos, A. Mittal, Fiebre: a

escalado de tareas múltiples para el aprendizaje por transferencia, preimpresión de arXiv arXiv:2111.10952

(2021). 31

conjunto de datos a gran escala para extracción y verificación de hechos, preimpresión de arXiv

arXiv:1803.05355 (2018). 29

[419] A. Williams, N. Nangia, S. Bowman, Un corpus de desafío de amplia cobertura para la

[398] I. Mollas, Z. Chrysopoulou, S. Karlos, G. Tsoumakas, Ethos: una versión en línea

comprensión de oraciones a través de la inferencia, en: Actas de
la Conferencia de 2018 del Capítulo Norteamericano de la Asociación de Lingüística

Conjunto de datos de detección de discursos de odio, preimpresión de arXiv arXiv:2006.08328 (2020).

29, 31

Computacional: Tecnologías del Lenguaje Humano,

[399] M. Nadeem, A. Bethke, S. Reddy, Stereoset: medición de estereotipos

Volumen 1 (artículos extensos), Asociación de Lingüística Computacional,
Nueva Orleans, Luisiana, 2018, págs. 1112­1122. doi:10.18653/v1/
N18­1101.

sesgo en modelos de lenguaje previamente entrenados, preimpresión de arXiv arXiv:2004.09456

(2020). 29, 31
[400] A. Parrish, A. Chen, N. Nangia, V. Padmakumar, J. Phang, J. Thomp­son, PM Htut, SR

URL https://aclanthology.org/N18­1101 29

Bowman, Bbq: Un punto de referencia de sesgo hecho a mano para

[420] Y. Zhang, J. Baldridge, L. He, PAWS: Parafrasea a los adversarios de la palabra

respuesta a preguntas, preimpresión de arXiv arXiv:2110.08193 (2021). 29

pelea, en: Actas de la Conferencia de 2019 del Capítulo Norteamericano de la Asociación

[401] J. Zhao, T. Wang, M. Yatskar, V. Ordóñez, K.­W. Chang, sesgo de género

de Lingüística Computacional: Humano

en resolución de correferencia: métodos de evaluación y eliminación de sesgos, arXiv

Tecnologías del lenguaje, volumen 1 (artículos largos y breves), Asociación de Lingüística

preimpresión arXiv:1804.06876 (2018). 29

Computacional, Minneapolis, Minnesota, 2019, págs.
1298­1308. doi:10.18653/v1/N19­1131.

[402] N. Nangia, C. Vania, R. Bhalerao, SR Bowman, Pares de cuervos: un conjunto de datos
desafiante para medir los sesgos sociales en modelos de lenguaje enmascarado,

URL https://aclanthology.org/N19­1131 29

Preimpresión de arXiv arXiv:2010.00133 (2020). 29

[421] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, D. Yang, ¿Es el chat ­GPT un solucionador

[403] S. Gehman, S. Gururangan, M. Sap, Y. Choi, NA Smith, Realtoxic­ityprompts: Evaluación de

de tareas de procesamiento del lenguaje natural de propósito general? en el

la degeneración tóxica neuronal en modelos de lenguaje,

2023 Conferencia sobre métodos empíricos en el procesamiento del lenguaje natural, 2023.

Preimpresión de arXiv arXiv:2009.11462 (2020). 29
[404] D. Borkan, L. Dixon, J. Sorensen, N. Thain, L. Vasserman, Matizado

URL https://openreview.net/forum?id=u03xn1COsO 31
[422] MU Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, MB Shaikh,

métricas para medir sesgos no deseados con datos reales para la clasificación de textos,
en: Actas complementarias de la conferencia mundial web de 2019, 2019, págs. 29

N. Akhtar, J. Wu, S. Mirjalili, et al., Grandes modelos de lenguaje: un estudio completo de
sus aplicaciones, desafíos, limitaciones y futuro.
perspectivas, TechRxiv (2023). 31

[405] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow,
M. Huck, AJ Yepes, P. Koehn, V. Logacheva, C. Monz, et al., Hallazgos de la conferencia

[423] XL Dong, S. Moon, YE Xu, K. Malik, Z. Yu, Hacia asistentes inteligentes de próxima generación

de 2016 sobre traducción automática, en: Actas de

que aprovechan las técnicas de llm, en: Actas de la 29ª Conferencia ACM SIGKDD sobre

la Primera Conferencia sobre Traducción Automática: Volumen 2, Tarea Compartida

descubrimiento de conocimientos

Artículos, 2016, págs. 131­198. 29

y Minería de datos, 2023, págs. 5792–5793. 31

[406] B. Loïc, B. Magdalena, B. Ondˇrej, F. Christian, G. Yvette, G. Ro­man, H. Barry, H. Matthias,

[424] K. Pandya, M. Holia, Automatización del servicio al cliente utilizando langchain:

J. Eric, K. Tom, et al., Hallazgos del

Creación de un chatbot gpt de código abierto personalizado para organizaciones, arXiv

Conferencia de 2020 sobre traducción automática (wmt20), en: Actas de

preimpresión arXiv:2310.05421 (2023). 31

Quinta Conferencia sobre Traducción Automática, Asociación de Lingüística Computacional

[425] J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao,

„ 2020, págs. 1–55. 29

R. Geng, et al., ¿Puede llm servir ya como interfaz de base de datos? a

[407] W. Li, F. Qi, M. Sun, X. Yi, J. Zhang, Ccpm: Una poesía clásica china

Gran banco para texto a SQL basado en bases de datos a gran escala, preimpresión de arXiv

arXiv:2305.03111 (2023). 31

conjunto de datos coincidente, preimpresión de arXiv arXiv:2106.01979 (2021). 29

[408] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, J. Weston, Mago de

[426] A. Rao, J. Kim, M. Kamineni, M. Pang, W. Lie, MD Succi, Evaluación

wikipedia: agentes conversacionales basados en el conocimiento, preimpresión de arXiv

chatgpt como complemento para la toma de decisiones radiológicas, medRxiv (2023)
2023–02. 31

arXiv:1811.01241 (2018). 29
[409] H. Rashkin, EM Smith, M. Li, Y.­L. Boureau, hacia la empatía

[427] M. Benary, XD Wang, M. Schmidt, D. Soll, G. Hilfenhaus, M. Nas­sir, C. Sigler, M. Knödler, U.

Modelos de conversación de dominio abierto: un nuevo punto de referencia y conjunto de datos, arXiv

Keller, D. Beule, et al., Aprovechamiento de grandes

preimpresión arXiv:1811.00207 (2018). 29

Modelos de lenguaje para el apoyo a la toma de decisiones en oncología personalizada, JAMA.

Red abierta 6 (11) (2023) e2343689–e2343689. 31

[410] E. Dinan, V. Logacheva, V. Malykh, A. Miller, K. Shuster, J. Urbanek,
D. Kiela, A. Szlam, I. Serban, R. Lowe, et al., El segundo desafío de inteligencia

[428] CM Chiesa­Estomba, JR Lechien, LA Vaira, A. Brunet, G. Cam­maroto, M. Mayo­Yanez, A.

conversacional (convai2), en: La competencia NeurIPS'18: del aprendizaje automático a

Sanchez­Barrueco, C. Saga­Gutiérrez, Explorando el potencial del chat­ gpt como

las conversaciones inteligentes, Springer,

herramienta de apoyo para la sialendoscopia

2020, págs. 187­208. 29

toma de decisiones clínicas y apoyo a la información del paciente, europeo

[411] H. Zhou, C. Zheng, K. Huang, M. Huang, X. Zhu, Kdconv: Un chino

Archivos de otorrinolaringología (2023) 1–6. 31
[429] S. Montagna, S. Ferretti, LC Klopfenstein, A. Florio, MF Pengo,

conjunto de datos de diálogo multidominio hacia múltiples turnos impulsados por el conocimiento

conversación, preimpresión de arXiv arXiv:2004.04100 (2020). 29

Descentralización de datos de sistemas de chatbot basados en llm en enfermedades crónicas

[412] L. CO, Iflytek: un clasificador de texto chino de múltiples categorías. competencia

autogestión, en: Actas de la Conferencia ACM 2023 sobre tecnología de la información

sitio web oficial (2019). 29

para el bien social, 2023, págs. 31

[413] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, J. Blackburn, The

[430] D. Bill, T. Eriksson, Ajuste de una película utilizando el aprendizaje por refuerzo a partir de

conjunto de datos pushshift reddit, en: Actas de la conferencia internacional AAAI sobre

Comentarios humanos para una aplicación de chatbot de terapia (2023). 31
[431] M. Abbasian, I. Azimi, AM Rahmani, R. Jain, Salud conversacional

web y redes sociales, vol. 14, 2020, págs. 830–839. 30

44

Machine Translated by Google

Agentes: un marco de agentes personalizado basado en llm, preimpresión de arXiv

[453] K. Yang, AM Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil, R. Prenger, A.

arXiv:2310.02374 (2023). 31

Anandkumar, Leandojo: demostración de teoremas con modelos de lenguaje de

[432] KV Lemley, ¿Chatgpt nos ayuda a comprender la literatura médica?, Revista de la Sociedad

recuperación aumentada, arXiv preimpresión arXiv:2306.15626 (2023). 32

Estadounidense de Nefrología (2023) 10–1681. 31 [433] S. Pal, M. Bhattacharya, S.­
S. Lee, C. Chakraborty, Se requiere un modelo de lenguaje grande (llm) o chatgpt de próxima

[454] KM Collins, AQ Jiang, S. Frieder, L. Wong, M. Zilka, U. Bhatt, T. Lukasiewicz, Y. Wu, JB

generación de dominio específico para la investigación y la ingeniería biomédica, Annals

Tenenbaum, W. Hart, et al., Evaluación de modelos de lenguaje para matemáticas a

of Biomedical Engineering (2023) 1–4. 31

través de interacciones, preimpresión de arXiv arXiv:2306.01694 (2023). 32

[434] Y. Du, S. Zhao, Y. Chen, R. Bai, J. Liu, H. Wu, H. Wang, B. Qin, The calla dataset: Sondeo

[455] Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He, Z. Liu, et al., Resumen

de la adquisición interactiva de conocimientos de las películas de médicos chinos

del chatgpt Investigación relacionada y perspectiva hacia el futuro de los grandes modelos

literatura, preimpresión de arXiv arXiv:2309.04198 (2023). 31 [435] A. Abd­Alrazaq,

de lenguaje, Meta­Radiology (2023) 100017. 32

R. AlSaad, D. Alhuwail, A. Ahmed, PM Healy, S. Latifi, S. Aziz, R. Damseh, SA Alrazak, J. Sheikh,
et al., Lenguaje grande Modelos en educación médica: oportunidades, desafíos y

[456] J. Drápal, H. Westermann, J. Savelka, Uso de modelos de lenguaje grandes para respaldar

direcciones futuras, JMIR Medical Education 9 (1) (2023) e48291. 31 [436] AB Mbakwe,

el análisis temático en estudios jurídicos empíricos, preimpresión de arXiv arXiv:2310.18729

I. Lourentzou, LA Celi, OJ Mechanic, A. Dagan, Chatgpt passing usmle arroja luz

(2023). 32

sobre los defectos de la educación médica (2023). 31

[457] J. Savelka, KD Ashley, MA Gray, H. Westermann, H. Xu, Explicación de conceptos legales
con modelos de lenguaje grandes aumentados (gpt­4), preimpresión de arXiv
arXiv:2306.09525 (2023). 32

[437] S. Ahn, Los impactos inminentes de los grandes modelos lingüísticos en la educación

[458] N. Guha, J. Nyarko, DE Ho, C. Ré, A. Chilton, A. Narayana, A. Chohlas­Wood, A. Peters, B.

médica, Korean Journal of Medical Education 35 (1) (2023) 103. 31 [438] E. Waisberg,

Waldon, DN Rockmore y otros, Legal­bench : Un punto de referencia construido en

J. Ong, M. Masalkhi, AG Lee , Chatbots basados en modelos de lenguaje grande (llm) para la

colaboración para medir el razonamiento jurídico en modelos de lenguaje grandes,

educación médica neurooftálmica, Eye (2023) 1–3. 31

preimpresión de arXiv arXiv:2308.11462 (2023). 32 [459] J. Cui, Z. Li, Y. Yan, B. Chen,
L. Yuan, Chatlaw: modelo de lenguaje grande legal de código abierto con bases de conocimiento

[439] G. Deiana, M. Dettori, A. Arghittu, A. Azara, G. Gabutti, P. Castiglia, Inteligencia artificial y

externas integradas, preimpresión de arXiv arXiv:2306.16092 (2023). 32

salud pública: Evaluación de las respuestas chatgpt a los mitos y conceptos erróneos
sobre la vacunación, Vaccines 11 (7) (2023) 1217. 31 [440] L. De Angelis, F. Baglivo, G.

[460] H. Yang, X.­Y. Liu, CD Wang, Fingpt: Modelos de lenguaje grande financiero de código

Arzilli, GP Privitera, P. Ferragina, AE

abierto, preimpresión de arXiv arXiv:2306.06031 (2023). 32 [461] Y. Li, S.

Tozzi, C. Rizzo, Chatgpt y el auge de los grandes modelos lingüísticos: la nueva amenaza

Wang, H. Ding, H. Chen, Grandes modelos de lenguaje en finanzas: una encuesta, en: Actas de

infodémica impulsada por la IA en la salud pública, Frontiers in Public Health 11 (2023)

la Cuarta Conferencia Internacional ACM sobre IA en Finanzas, 2023, págs. . 33

1166120. 31
[441] NL Rane, A. Tawde, SP Choudhary, J. Rane, Contribución y desempeño de chatgpt y otros

[462] A. Lykov, D. Tsetserukou, Llm­brain: Generación rápida de árbol de comportamiento de

grandes modelos lingüísticos (llm) para avances científicos y de investigación: un arma

robot impulsada por IA basada en un modelo de lenguaje grande, preimpresión de arXiv
arXiv:2305.19352 (2023). 33

de doble filo, International Re­search Journal of Modernización en tecnología y ciencia
de la ingeniería 5 (10) (2023) 875–899. 31, 32

[463] E. Billing, J. Rosén, M. Lamb, Modelos de lenguaje para la interacción entre humanos y
robots, en: Conferencia internacional ACM/IEEE sobre interacción entre humanos y

´
[442] W. Dai, J. Lin, H. Jin, T. Li, Y.­S. Tsai, D. Gaševic, G. Chen, ¿Pueden los modelos de

robots, 13 al 16 de marzo de 2023, Estocolmo, Suecia , Biblioteca digital ACM, 2023,
págs. 33

lenguaje grandes proporcionar retroalimentación a los estudiantes? un estudio de caso

[464] Y. Ye, H. You, J. Du, Mejora de la confianza en la colaboración entre humanos y robots con

sobre chatgpt, en: Conferencia internacional IEEE de 2023 sobre tecnologías de
aprendizaje avanzadas (ICALT), IEEE, 2023, págs. 32 [443]

chatgpt, IEEE Access (2023). 33 [465]

E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva, F. Fischer, U. Gasser, G.

Y. Ding, X. Zhang, C. Paxton, S. Zhang, Aprovechamiento del conocimiento de sentido común de

Groh, S. Günnemann, E. Hüllermeier y otros, Chatgpt para ¿bien? sobre oportunidades

grandes modelos de lenguaje para la planificación de tareas y movimientos, en: Taller

y desafíos de los grandes modelos lingüísticos para la educación, el aprendizaje y las

RSS 2023 sobre aprendizaje para la planificación de tareas y movimientos,
2023. 33

diferencias individuales 103 (2023) 102274. 32

[466] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, T.
[444] N. Rane, Mejora de la calidad de la enseñanza y el aprendizaje a través de chat­gpt y

Funkhouser, Tidybot: asistencia robótica personalizada con grandes modelos de lenguaje,

modelos de lenguaje grandes similares: desafíos, perspectivas futuras y consideraciones

arXiv preimpresión arXiv:2305.05658 (2023). 33

éticas en la educación, perspectivas futuras y consideraciones éticas en la educación (15
de septiembre de 2023) ( 2023). 32

[467] E. Strubell, A. Ganesh, A. McCallum, Consideraciones energéticas y políticas para el

[445] JC Young, M. Shishido, Investigación de los potenciales de chatgpt de openai en la

aprendizaje profundo en PNL, preimpresión de arXiv arXiv:1906.02243 (2019). 33

generación de diálogos de chatbot para el aprendizaje de inglés como lengua extranjera,

[468] EM Bender, T. Gebru, A. McMillan­Major, S. Shmitchell, Sobre los peligros de los loros

Revista Internacional de Aplicaciones y Ciencias de la Computación Avanzadas 14 (6)

estocásticos: ¿Pueden los modelos lingüísticos ser demasiado grandes?, en: Actas de

(2023). 32 [446] J.

la conferencia ACM de 2021 sobre equidad , rendición de cuentas y transparencia, 2021,

Irons, C. Mason, P. Cooper, S. Sidra, A. Reeson, C. Paris, Explorando los impactos de chatgpt

págs. 610–623. 33

en el trabajo científico futuro, SocArXiv (2023). 32 [447] PG Schmidt, AJ Meir, Uso de

[469] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Comprender el aprendizaje profundo

la IA generativa para búsquedas bibliográficas y escritura académica: ¿Está en peligro la

(todavía) requiere repensar la generalización, Communications of the ACM 64 (3) (2021)
107–115. 33

integridad del discurso científico?, preimpresión de arXiv arXiv:2311.06981 (2023). 32
[448] Y. Zheng, HY Koh, J. Ju, AT Nguyen, LT May, GI

[470] M. Tänzer, S. Ruder, M. Rei, Memorización versus generalización en modelos de lenguaje

Webb, S. Pan, Grandes modelos de lenguaje para síntesis, inferencia y explicación científica,

previamente entrenados, preimpresión de arXiv arXiv:2105.00828 (2021). 33 [471] SM

preimpresión de arXiv arXiv:2310.07984 (2023). 32 [449] B. Aczel, E.­J. Wagenmakers,

West, M. Whittaker, K. Crawford, Sistemas de discriminación, IA
Ahora (2019) 1–33. 33

Guía de transparencia para el uso de chatgpt en redacción
científica, PsyArXiv (2023). 32 [450] S. Altmäe, A. Sola­Leyva, A. Salumets, La inteligencia

[472] K. Valmeekam, A. Olmo, S. Sreedharan, S. Kambhampati, Los modelos de lenguajes

artificial en la escritura científica: ¿un amigo o un

grandes todavía no pueden planificar (un punto de referencia para películas sobre

enemigo?, Reproductive BioMedicine Online (2023). 32

planificación y razonamiento sobre el cambio), preimpresión de arXiv arXiv:2206.10498
(2022) . 33 [473] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y.
Chen, et al., Canto de sirena en el océano de ai: una encuesta sobre alucinaciones en

[451] S. Imani, L. Du, H. Shrivastava, Mathprompter: Razonamiento matemático utilizando

modelos de lenguaje grandes, preimpresión de arXiv arXiv:2309.01219 (2023). 33

modelos de lenguaje grandes, preimpresión de arXiv arXiv:2303.05398 (2023). 32
[474] A. Webson, E. Pavlick, ¿Los modelos basados en indicaciones realmente entienden el
[452] Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, C. Zhou, Relación de escala en el aprendizaje del

significado de sus indicaciones?, preimpresión de arXiv arXiv:2109.01247 (2021). 33

razonamiento matemático con modelos de lenguaje grandes, preimpresión de arXiv

[475] O. Shaikh, H. Zhang, W. Held, M. Bernstein, D. Yang, Pensándolo bien, ¡no pensemos paso

arXiv:2308.01825 (2023). 32

a paso! sesgo y toxicidad en reacciones de disparo cero.

45

Machine Translated by Google

soning, preimpresión de arXiv arXiv:2212.08061 (2022). 33 [476]
X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, J. Gao, Entrenamiento adversario para
grandes modelos de lenguaje neuronal, ArXiv (abril de 2020).
URL https://www.microsoft.com/en­us/research/publication/adversarial­training­for­large­
neural­language­models/ 34
[477] E. Shayegani, MAA Mamun, Y. Fu, P. Zaree, Y. Dong, N. Abu­Ghazaleh, Estudio de
vulnerabilidades en modelos lingüísticos grandes revelados por ataques adversarios
(2023). arXiv:2310.10844. 34 [478] X. Xu, K. Kong, N. Liu, L. Cui, D.
Wang, J. Zhang, M. Kankanhalli, Una película puede engañarse a sí misma: un ataque adversarial
basado en avisos (2023). arXiv: 2310.13345. 34 [479] H. Zhao, H. Chen, F. Yang, N. Liu,
H. Deng, H. Cai, S.
Wang, D. Yin, M. Du, Explicabilidad de modelos de lenguaje grandes: una encuesta (2023 ).
arXiv:2309.01029. 34 [480] S.
Huang, S. Mamidanna, S. Jangam, Y. Zhou, LH Gilpin, ¿Pueden los modelos de lenguaje grandes
explicarse por sí mismos? un estudio de autoexplicaciones generadas por películas
(2023). arXiv:2310.11207. 34 [481] H. Brown, K. Lee, F.
Mireshghallah, R. Shokri, F. Tramèr, ¿Qué significa que un modelo lingüístico preserve la
privacidad?, en: Actas de la Conferencia ACM de 2022 sobre equidad, responsabilidad, y
Transparencia, 2022, págs. 2280–2292. 34 [482] R. Plant, V. Giuffrida, D. Gkatzia, Eres lo
que escribes: Preservando la
privacidad en la era de los grandes modelos lingüísticos, preimpresión de arXiv arXiv:2204.09391
(2022). 34
[483] W. Niu, Z. Kong, G. Yuan, W. Jiang, J. Guan, C. Ding, P. Zhao, S. Liu, B. Ren, Y. Wang,
Ejecución en tiempo real de grandes Escalar modelos de lenguaje en dispositivos móviles
(2020). arXiv:2009.06823. 34 [484] C. Guo, J. Tang, W.
Hu, J. Leng, C. Zhang, F. Yang, Y. Liu, M. Guo, Y. Zhu, Olive: Aceleración de modelos de lenguaje
grandes mediante hardware compatible Cuantización del par atípico­víctima, en: Actas
del 50º Simposio Internacional Anual sobre Arquitectura de Computadoras, 2023, págs. 34

[485] B. Meskó, EJ Topol, El imperativo de la supervisión regulatoria de los grandes modelos de
lenguaje (o IA generativa) en la atención sanitaria, npj Digital Medicine 6 (1) (2023) 120. 34
[486] J. Zhang, X. Ji, Z. Zhao, X. Hei, K.­KR Choo, Consideraciones éticas e implicaciones políticas
para modelos de lenguaje grandes: Guiando el desarrollo y la implementación responsables,
preimpresión de arXiv arXiv:2308.02678 (2023). 34
[487] J. Mökander, J. Schuett, HR Kirk, L. Floridi, Auditoría de grandes modelos lingüísticos: un
enfoque de tres capas, IA y ética (2023) 1–31. 34

46

